{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” **1. Introduction to Feature Engineering**\n",
    "\n",
    "* ğŸ“Œ What is Feature Engineering?\n",
    "* ğŸ§  Why is it important in ML?\n",
    "* ğŸ§ª Role in model performance\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§¹ **2. Data Cleaning**\n",
    "\n",
    "* ğŸ§¼ Handling Missing Values\n",
    "\n",
    "  * Mean/Median/Mode Imputation\n",
    "  * Forward/Backward Fill\n",
    "  * Using Predictive Models\n",
    "* ğŸ§ª Outlier Detection and Treatment\n",
    "\n",
    "  * Z-score, IQR, Isolation Forest\n",
    "* ğŸ”„ Fixing Data Inconsistencies\n",
    "* ğŸ” Removing Duplicates\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¡ **3. Encoding Categorical Variables**\n",
    "\n",
    "* ğŸ”¢ Label Encoding\n",
    "* ğŸ§® One-Hot Encoding\n",
    "* ğŸ§² Ordinal Encoding\n",
    "* ğŸ“Š Binary Encoding\n",
    "* ğŸ§© Target/Mean Encoding\n",
    "* ğŸ² Frequency Encoding\n",
    "* ğŸŒ Hash Encoding\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§® **4. Numerical Feature Transformation**\n",
    "\n",
    "* ğŸ”„ Scaling & Normalization\n",
    "\n",
    "  * Min-Max Scaling\n",
    "  * Standardization (Z-score)\n",
    "  * Robust Scaler\n",
    "  * MaxAbs Scaler\n",
    "* ğŸŒ Log, Power, Box-Cox Transformations\n",
    "* ğŸ¯ Discretization (Binning)\n",
    "\n",
    "---\n",
    "\n",
    "### â³ **5. Temporal Feature Engineering**\n",
    "\n",
    "* ğŸ“… Extracting Time Components\n",
    "\n",
    "  * Day, Month, Year, Hour\n",
    "* ğŸ” Time Differences (Durations)\n",
    "* â± Lag Features\n",
    "* ğŸ”„ Rolling/Moving Averages\n",
    "* ğŸ” Cyclical Time Encoding (sin/cos)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ **6. Geospatial Feature Engineering**\n",
    "\n",
    "* ğŸŒ Latitude & Longitude Features\n",
    "* ğŸ“ Distance Calculations (Haversine)\n",
    "* ğŸ—º Region/Area Mapping\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ— **7. Feature Construction / Extraction**\n",
    "\n",
    "* ğŸ§® Mathematical Combinations of Features\n",
    "* ğŸ’¡ Domain-Specific Features\n",
    "* ğŸ“ˆ Polynomial Features\n",
    "* ğŸ”£ Text Lengths, Word Counts (for NLP)\n",
    "* ğŸ² Cross Features (Interaction Terms)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§¼ **8. Feature Selection**\n",
    "\n",
    "* ğŸ§ª Statistical Methods (Chi-square, ANOVA)\n",
    "* ğŸ“‰ Correlation Analysis\n",
    "* ğŸ§  Feature Importance from Models (RF, XGBoost)\n",
    "* ğŸš¦ Recursive Feature Elimination (RFE)\n",
    "* ğŸ§Š L1 Regularization (Lasso)\n",
    "* ğŸª Variance Thresholding\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š **9. Dimensionality Reduction**\n",
    "\n",
    "* ğŸ“‰ PCA (Principal Component Analysis)\n",
    "* ğŸ§® LDA (Linear Discriminant Analysis)\n",
    "* ğŸ§© t-SNE, UMAP\n",
    "* ğŸ”¢ Autoencoders (for Deep Learning)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¤ **10. Text Feature Engineering (NLP)**\n",
    "\n",
    "* ğŸ“ Bag-of-Words (BoW)\n",
    "* ğŸ¯ TF-IDF\n",
    "* ğŸ§  Word Embeddings (Word2Vec, GloVe)\n",
    "* ğŸ”£ N-grams, Character-level Features\n",
    "* ğŸ“ Readability Scores\n",
    "* ğŸ”¢ Sentiment Scores\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¥ **11. Image Feature Engineering (CV)**\n",
    "\n",
    "* ğŸ–¼ Pixel Intensity Stats\n",
    "* ğŸ§  Pre-trained CNN Feature Extraction\n",
    "* ğŸŒ€ HOG, SIFT, SURF Descriptors\n",
    "* ğŸ§¬ Color Histograms\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¤– **12. Feature Engineering in Deep Learning**\n",
    "\n",
    "* ğŸ“ Embedding Layers (for categorical features)\n",
    "* ğŸ“ Custom Feature Layers\n",
    "* ğŸ”€ Feature Fusion and Concatenation\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§° **13. Tools & Libraries for Feature Engineering**\n",
    "\n",
    "* ğŸ Python Libraries: `pandas`, `sklearn`, `feature-engine`, `category_encoders`, `tsfresh`\n",
    "* âš™ Automation: FeatureTools, AutoFeat, PyCaret\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” **1. What is Feature Engineering?**\n",
    "\n",
    "#### ğŸ“˜ Definition:\n",
    "\n",
    "* ğŸ› ï¸ Feature engineering is the process of **creating, transforming, selecting, or extracting** relevant variables (features) from raw data to improve the performance of machine learning models.\n",
    "* ğŸ¯ It helps make hidden patterns visible to models, increasing accuracy and interpretability.\n",
    "\n",
    "#### ğŸ§  Real-World Use Case (When to Use):\n",
    "\n",
    "* Anytime you have **raw, unstructured, or semi-structured data** that needs to be converted into a format usable by ML models.\n",
    "* Example: Converting date of birth to age, converting text reviews into sentiment scores, combining features like â€œprice per square foot.â€\n",
    "\n",
    "#### ğŸš« When Not to Use:\n",
    "\n",
    "* When using **end-to-end deep learning models** that can learn feature representations automatically (e.g., image classification with CNNs), but even then, feature engineering can help.\n",
    "* When data is already **pre-processed and well-represented**.\n",
    "\n",
    "#### ğŸ’» Code Implementation:\n",
    "\n",
    "Not a specific functionâ€”it's a concept, but hereâ€™s a simple example:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Raw data\n",
    "df = pd.DataFrame({\n",
    "    'dob': ['2000-01-01', '1995-06-15', '1990-09-10']\n",
    "})\n",
    "\n",
    "# Convert to datetime\n",
    "df['dob'] = pd.to_datetime(df['dob'])\n",
    "\n",
    "# Feature Engineering: Age\n",
    "df['age'] = 2025 - df['dob'].dt.year\n",
    "print(df)\n",
    "```\n",
    "\n",
    "#### ğŸŒŸ Advantages:\n",
    "\n",
    "* ğŸ§  Improves model performance significantly\n",
    "* ğŸ” Reveals patterns that models canâ€™t detect automatically\n",
    "* ğŸ§± Adds domain knowledge into ML process\n",
    "\n",
    "#### âš ï¸ Disadvantages:\n",
    "\n",
    "* â±ï¸ Time-consuming and requires domain expertise\n",
    "* âš–ï¸ May lead to **overfitting** if too many irrelevant features are created\n",
    "* ğŸ”„ Can be hard to automate in complex pipelines\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### ğŸ§¹ **2. Data Cleaning in Feature Engineering**\n",
    "\n",
    "#### ğŸ“˜ Definition:\n",
    "\n",
    "* ğŸ§¼ Data Cleaning is the process of **identifying and correcting (or removing)** inaccurate, incomplete, or irrelevant data to ensure the dataset is clean, consistent, and ready for modeling.\n",
    "* It includes handling missing values, outliers, duplicates, and inconsistent formats.\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ§  Real-World Use Case (When to Use):\n",
    "\n",
    "* When your dataset has:\n",
    "\n",
    "  * â“Missing entries (e.g., user age is blank)\n",
    "  * ğŸ˜µâ€ğŸ’« Outlier values (e.g., salary = 9999999)\n",
    "  * ğŸ” Duplicated records\n",
    "  * ğŸ”¤ Inconsistent formatting (e.g., â€œYesâ€, â€œyesâ€, â€œYâ€)\n",
    "\n",
    "Example: In retail data, if `price` is missing or extremely high, cleaning ensures valid model learning.\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸš« When Not to Use:\n",
    "\n",
    "* If data is already clean and validated (rare!)\n",
    "* During **model testing phase**, you shouldn't clean test data using methods fitted on test itself (causes leakage)\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ’» Code Implementation:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "df = pd.DataFrame({\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Alice'],\n",
    "    'age': [25, np.nan, 29, 25],\n",
    "    'income': [50000, 60000, None, 50000]\n",
    "})\n",
    "\n",
    "# ğŸ” Remove Duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# â“ Handle Missing Values\n",
    "df['age'].fillna(df['age'].mean(), inplace=True)\n",
    "df['income'].fillna(df['income'].median(), inplace=True)\n",
    "\n",
    "# ğŸ§¼ Final cleaned data\n",
    "print(df)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸŒŸ Advantages:\n",
    "\n",
    "* âœ… Improves **model accuracy and reliability**\n",
    "* ğŸ” Helps discover and fix **hidden data quality issues**\n",
    "* ğŸ§  Makes dataset interpretable for analysis\n",
    "\n",
    "---\n",
    "\n",
    "#### âš ï¸ Disadvantages:\n",
    "\n",
    "* â³ Can be **time-intensive** for large datasets\n",
    "* ğŸ§ª Wrong cleaning logic may lead to **loss of useful data**\n",
    "* ğŸ” Risk of **data leakage** if cleaning isnâ€™t done carefully (e.g., using test data statistics)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¡ **3. Encoding Categorical Variables**\n",
    "\n",
    "#### ğŸ“˜ Definition:\n",
    "\n",
    "* ğŸ”¤ Encoding converts **categorical (non-numeric)** values into a **numerical format** so that ML models can process them.\n",
    "* Different encoding techniques are used depending on whether the categorical variable is nominal (no order) or ordinal (has order).\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ§  Real-World Use Case (When to Use):\n",
    "\n",
    "* ğŸŒ When your dataset contains values like `\"Male/Female\"`, `\"Red/Blue/Green\"`, `\"Low/Medium/High\"` etc.\n",
    "* ğŸ¦ Example: In banking, customer â€œaccount\\_typeâ€ (`savings`, `current`, `loan`) must be converted into numbers before model training.\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸš« When Not to Use:\n",
    "\n",
    "* âŒ When using models that **natively handle categories**, like CatBoost or LightGBM (they accept categorical inputs directly).\n",
    "* âš ï¸ Donâ€™t apply **Label Encoding** to nominal features for tree-based models â€” it may imply a false order.\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ’» Code Implementation (Different Types):\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import category_encoders as ce\n",
    "\n",
    "# Sample data\n",
    "df = pd.DataFrame({'color': ['Red', 'Blue', 'Green', 'Blue']})\n",
    "\n",
    "# ğŸ”¢ Label Encoding\n",
    "le = LabelEncoder()\n",
    "df['label_encoded'] = le.fit_transform(df['color'])\n",
    "\n",
    "# ğŸ§® One-Hot Encoding\n",
    "df_onehot = pd.get_dummies(df['color'], prefix='color')\n",
    "\n",
    "# ğŸ¯ Target/Mean Encoding (for supervised tasks)\n",
    "# Note: Normally you use the target column here\n",
    "encoder = ce.TargetEncoder()\n",
    "df['target_encoded'] = encoder.fit_transform(df['color'], [1, 0, 1, 0])\n",
    "\n",
    "print(df)\n",
    "print(df_onehot)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸŒŸ Advantages:\n",
    "\n",
    "* ğŸ” Converts non-numeric data into numeric for ML models\n",
    "* ğŸ§  Methods like target encoding consider the relationship with the label\n",
    "* ğŸ”¢ One-hot encoding prevents false ordinal relationships\n",
    "\n",
    "---\n",
    "\n",
    "#### âš ï¸ Disadvantages:\n",
    "\n",
    "* ğŸ§¨ One-Hot Encoding can cause **dimensionality explosion** with high-cardinality features\n",
    "* ğŸ­ Label Encoding can **mislead** tree models if used wrongly on nominal features\n",
    "* ğŸ§® Target Encoding may lead to **data leakage** if not cross-validated\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### ğŸ§® **4. Numerical Feature Transformation**\n",
    "\n",
    "#### ğŸ“˜ Definition:\n",
    "\n",
    "* ğŸ”„ Numerical transformation is the process of **modifying numeric features** to improve model performance or meet algorithm assumptions.\n",
    "* It includes **scaling**, **normalization**, and **mathematical transformations** like log, square root, etc.\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ§  Real-World Use Case (When to Use):\n",
    "\n",
    "* ğŸ“ When your features have **different units or ranges** (e.g., income in lakhs vs. age in years)\n",
    "* ğŸ“ˆ When a feature is **skewed** (e.g., income, sales) and needs transformation for better model behavior\n",
    "* ğŸ§  Algorithms like **KNN, SVM, and logistic regression** are sensitive to feature scale.\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸš« When Not to Use:\n",
    "\n",
    "* ğŸŒ² Tree-based models (like Random Forest, XGBoost) usually **donâ€™t require scaling**\n",
    "* âŒ Don't use log or root on **zero or negative values** without fixing them first\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ’» Code Implementation:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "# Sample data\n",
    "df = pd.DataFrame({\n",
    "    'income': [30000, 50000, 70000, 100000, 1000000],\n",
    "    'age': [25, 30, 35, 40, 28]\n",
    "})\n",
    "\n",
    "# ğŸ”„ Min-Max Scaling (0 to 1)\n",
    "min_max = MinMaxScaler()\n",
    "df['income_minmax'] = min_max.fit_transform(df[['income']])\n",
    "\n",
    "# ğŸ§  Standardization (Z-score)\n",
    "standard = StandardScaler()\n",
    "df['income_standard'] = standard.fit_transform(df[['income']])\n",
    "\n",
    "# ğŸ›¡ Robust Scaling (ignores outliers)\n",
    "robust = RobustScaler()\n",
    "df['income_robust'] = robust.fit_transform(df[['income']])\n",
    "\n",
    "# ğŸ” Log Transformation (for skewed data)\n",
    "df['income_log'] = np.log1p(df['income'])  # log1p = log(1 + x)\n",
    "\n",
    "print(df)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸŒŸ Advantages:\n",
    "\n",
    "* ğŸ¯ Makes models converge faster (especially gradient-based models)\n",
    "* ğŸ“‰ Handles skewed distributions and outliers\n",
    "* ğŸ¤– Prepares data for distance-based models (like KNN)\n",
    "\n",
    "---\n",
    "\n",
    "#### âš ï¸ Disadvantages:\n",
    "\n",
    "* âš– May distort feature meaning or interpretability\n",
    "* ğŸ§® Sensitive to **zero or negative values** (log, sqrt)\n",
    "* ğŸ§  Needs careful choice of method based on distribution\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### â³ **5. Temporal Feature Engineering (Time-based Features)**\n",
    "\n",
    "#### ğŸ“˜ Definition:\n",
    "\n",
    "* ğŸ“… Temporal feature engineering involves extracting and transforming **date and time-related data** into meaningful features that improve model insights.\n",
    "* Common operations include extracting **day, month, hour**, calculating **differences**, and creating **lag/rolling features**.\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ§  Real-World Use Case (When to Use):\n",
    "\n",
    "* ğŸ“Š In sales forecasting, you can extract:\n",
    "\n",
    "  * **Day of week** (to catch weekend effects)\n",
    "  * **Month** (seasonality)\n",
    "  * **Lag features** (previous day's sales)\n",
    "  * **Rolling mean** (7-day average sales)\n",
    "* ğŸšŒ In transportation, time-based features can detect rush hour patterns.\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸš« When Not to Use:\n",
    "\n",
    "* ğŸ§Š When timestamps are irrelevant to your prediction (e.g., predicting customer churn from one-time data)\n",
    "* âš ï¸ Avoid using **future timestamps** for predictions â€” leads to **data leakage**\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ’» Code Implementation:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Sample timestamped dataset\n",
    "df = pd.DataFrame({\n",
    "    'timestamp': pd.date_range(start='2025-01-01', periods=5, freq='D'),\n",
    "    'sales': [100, 150, 200, 180, 220]\n",
    "})\n",
    "\n",
    "# ğŸ“… Extract components\n",
    "df['day'] = df['timestamp'].dt.day\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "df['weekday'] = df['timestamp'].dt.weekday\n",
    "\n",
    "# â± Lag feature (previous dayâ€™s sales)\n",
    "df['sales_lag1'] = df['sales'].shift(1)\n",
    "\n",
    "# ğŸ“‰ Rolling mean (last 3 days)\n",
    "df['sales_roll3'] = df['sales'].rolling(window=3).mean()\n",
    "\n",
    "# â­• Cyclical Encoding (for weekday)\n",
    "import numpy as np\n",
    "df['weekday_sin'] = np.sin(2 * np.pi * df['weekday'] / 7)\n",
    "df['weekday_cos'] = np.cos(2 * np.pi * df['weekday'] / 7)\n",
    "\n",
    "print(df)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸŒŸ Advantages:\n",
    "\n",
    "* ğŸ” Captures **seasonality**, **trends**, and **cyclical behavior**\n",
    "* â± Enables **lag-based modeling** (e.g., time series forecasting)\n",
    "* ğŸ’¡ Turns datetime into **actionable features**\n",
    "\n",
    "---\n",
    "\n",
    "#### âš ï¸ Disadvantages:\n",
    "\n",
    "* ğŸ”„ Requires **chronological order** for accuracy\n",
    "* ğŸ§  Improper handling of lags/rolling windows can lead to **data leakage**\n",
    "* â³ May add **redundant features** if not selected carefully\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ **6. Geospatial Feature Engineering**\n",
    "\n",
    "#### ğŸ“˜ Definition:\n",
    "\n",
    "* ğŸŒ Geospatial feature engineering is the process of extracting and creating new features from **geographic data** like latitude, longitude, ZIP code, or coordinates.\n",
    "* It helps capture **location-based patterns**, distances, and regional trends in ML models.\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ§  Real-World Use Case (When to Use):\n",
    "\n",
    "* ğŸ›µ In delivery apps (like Zomato, Uber), you can:\n",
    "\n",
    "  * Calculate **distance** between customer and restaurant\n",
    "  * Use coordinates to cluster areas\n",
    "* ğŸ˜ In real estate: use **location data** to estimate property prices\n",
    "* ğŸ§‘â€ğŸŒ¾ Agriculture: analyze **soil, rainfall, or crop zones** based on region\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸš« When Not to Use:\n",
    "\n",
    "* ğŸ“¦ If location doesnâ€™t influence the target variable (e.g., predicting product color from stock location)\n",
    "* ğŸ§Š When only **textual location names** (e.g., city names without coordinates) are available and canâ€™t be encoded meaningfully\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ’» Code Implementation:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Sample coordinates\n",
    "df = pd.DataFrame({\n",
    "    'store_lat': [28.61, 19.07],\n",
    "    'store_lon': [77.23, 72.87],\n",
    "    'user_lat': [28.70, 18.90],\n",
    "    'user_lon': [77.10, 72.70]\n",
    "})\n",
    "\n",
    "# ğŸ“ Calculate geodesic distance in kilometers\n",
    "df['distance_km'] = df.apply(lambda row: geodesic(\n",
    "    (row['store_lat'], row['store_lon']),\n",
    "    (row['user_lat'], row['user_lon'])\n",
    ").km, axis=1)\n",
    "\n",
    "print(df)\n",
    "```\n",
    "\n",
    "Other possible features:\n",
    "\n",
    "* ğŸ—º Region or cluster (using ZIP code or KMeans on coordinates)\n",
    "* ğŸŒ¡ Climate zone or risk zone (based on lat/lon)\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸŒŸ Advantages:\n",
    "\n",
    "* ğŸ“Š Adds **contextual intelligence** to models (location matters!)\n",
    "* ğŸ“ Enables **distance-based filtering or optimization**\n",
    "* ğŸ§  Captures **spatial trends** for better predictions\n",
    "\n",
    "---\n",
    "\n",
    "#### âš ï¸ Disadvantages:\n",
    "\n",
    "* ğŸ§® Calculating distances can be **computationally expensive**\n",
    "* ğŸŒ Latitude/longitude alone may not hold meaning â€” need transformation\n",
    "* ğŸ”„ Static coordinates may not reflect **dynamic behavior** (e.g., travel time)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### ğŸ— **7. Feature Construction / Extraction**\n",
    "\n",
    "#### ğŸ“˜ Definition:\n",
    "\n",
    "* ğŸ§± Feature construction is the process of **creating new features** from existing ones by combining, aggregating, or transforming them to reveal hidden patterns or relationships.\n",
    "* Think of it as **\"building new building blocks\"** for your ML model from the existing data.\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ§  Real-World Use Case (When to Use):\n",
    "\n",
    "* ğŸ  Real Estate:\n",
    "\n",
    "  * Construct â€œ**price per square foot**â€ from total price and area.\n",
    "* ğŸ›’ E-commerce:\n",
    "\n",
    "  * Create â€œ**total spent**â€ from quantity Ã— unit price.\n",
    "* ğŸ“± App Usage:\n",
    "\n",
    "  * Combine â€œlogin timeâ€ and â€œlogout timeâ€ to compute â€œ**session duration**â€\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸš« When Not to Use:\n",
    "\n",
    "* âš–ï¸ When you already have **optimized domain-specific features**\n",
    "* âŒ Avoid if it leads to overfitting due to high number of **noisy or redundant features**\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ’» Code Implementation:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Original data\n",
    "df = pd.DataFrame({\n",
    "    'total_price': [1000, 2000, 1500],\n",
    "    'area_sqft': [500, 1000, 750],\n",
    "    'login_time': [8.5, 9.0, 7.75],   # In hours\n",
    "    'logout_time': [10.0, 10.5, 9.25]\n",
    "})\n",
    "\n",
    "# ğŸ§® Feature Construction\n",
    "df['price_per_sqft'] = df['total_price'] / df['area_sqft']\n",
    "df['session_duration'] = df['logout_time'] - df['login_time']\n",
    "\n",
    "print(df)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸŒŸ Advantages:\n",
    "\n",
    "* ğŸ” Reveals **hidden relationships**\n",
    "* ğŸ“ˆ Boosts model accuracy with meaningful context\n",
    "* ğŸ¯ Allows models to **focus on what really matters**\n",
    "\n",
    "---\n",
    "\n",
    "#### âš ï¸ Disadvantages:\n",
    "\n",
    "* ğŸ§  Requires **domain expertise** to know what to create\n",
    "* ğŸ“Š Too many engineered features can lead to **overfitting**\n",
    "* ğŸ§¹ Needs regularization or selection if overused\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### âœ‚ï¸ **8. Feature Selection**\n",
    "\n",
    "#### ğŸ“˜ Definition:\n",
    "\n",
    "* âœ¨ Feature selection is the process of **selecting the most relevant features** and removing **irrelevant, redundant, or noisy** ones to improve model performance and reduce complexity.\n",
    "* It helps in making the model more **interpretable, faster, and generalizable**.\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ§  Real-World Use Case (When to Use):\n",
    "\n",
    "* ğŸ§ª In medical diagnosis, selecting key symptoms/features avoids confusion and improves prediction quality.\n",
    "* ğŸ’¼ In finance, selecting only the impactful indicators (e.g., interest rate, inflation) helps models generalize better.\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸš« When Not to Use:\n",
    "\n",
    "* ğŸ¤– Deep learning models can often **automatically learn** feature representations.\n",
    "* âš ï¸ When you have **too little data**, aggressive selection may cause **underfitting**.\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ’» Code Implementation:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Sample dataset\n",
    "df = pd.DataFrame({\n",
    "    'feature1': [1, 2, 3, 4, 5],\n",
    "    'feature2': [10, 20, 30, 40, 50],\n",
    "    'feature3': [9, 8, 7, 6, 5],\n",
    "    'target': [0, 1, 0, 1, 0]\n",
    "})\n",
    "\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# ğŸ“Š Select top 2 features using ANOVA F-test\n",
    "selector = SelectKBest(score_func=f_classif, k=2)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "# Selected columns\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "print(\"Selected Features:\", selected_features.tolist())\n",
    "```\n",
    "\n",
    "Other methods:\n",
    "\n",
    "* âœ… `Recursive Feature Elimination (RFE)`\n",
    "* ğŸ§  `Tree-based feature importance`\n",
    "* ğŸ” `L1 regularization (Lasso)`\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸŒŸ Advantages:\n",
    "\n",
    "* âš¡ Improves **model speed and accuracy**\n",
    "* ğŸ§¼ Reduces **overfitting** and noise\n",
    "* ğŸ“‰ Decreases dimensionality and improves interpretability\n",
    "\n",
    "---\n",
    "\n",
    "#### âš ï¸ Disadvantages:\n",
    "\n",
    "* âŒ May drop **useful features** if not done carefully\n",
    "* ğŸ”„ Depends on the feature selection method used\n",
    "* ğŸ§ª Risk of **information loss** if selection is aggressive\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### ğŸ”„ **9. Dimensionality Reduction**\n",
    "\n",
    "#### ğŸ“˜ Definition:\n",
    "\n",
    "* ğŸ“‰ Dimensionality reduction is the process of **reducing the number of input variables** (features) in a dataset by **projecting them into a lower-dimensional space** while preserving most of the information.\n",
    "* Unlike feature selection (which removes features), dimensionality reduction **transforms** features into a smaller set.\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ§  Real-World Use Case (When to Use):\n",
    "\n",
    "* ğŸ§¬ In genomics, thousands of gene expression features are reduced to core patterns.\n",
    "* ğŸ–¼ In image processing, it reduces pixels/features for faster ML modeling.\n",
    "* ğŸ’¼ In customer data, you can reduce dozens of behavioral metrics into a few principal components.\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸš« When Not to Use:\n",
    "\n",
    "* âŒ When **feature interpretability is essential** (e.g., in regulated industries)\n",
    "* ğŸ“Š When all original features are already few and well-defined\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ’» Code Implementation:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample data\n",
    "df = pd.DataFrame({\n",
    "    'math': [90, 85, 80, 70, 60],\n",
    "    'science': [88, 84, 81, 69, 65],\n",
    "    'english': [78, 75, 73, 60, 58]\n",
    "})\n",
    "\n",
    "# ğŸ”„ Standardize data\n",
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(df)\n",
    "\n",
    "# ğŸ§ª Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced = pca.fit_transform(scaled)\n",
    "\n",
    "# ğŸ“Š Resulting components\n",
    "df_pca = pd.DataFrame(reduced, columns=['PC1', 'PC2'])\n",
    "print(df_pca)\n",
    "```\n",
    "\n",
    "Other techniques:\n",
    "\n",
    "* ğŸ§  **t-SNE** (for visualization)\n",
    "* ğŸŒ€ **UMAP**\n",
    "* ğŸ“ **Linear Discriminant Analysis (LDA)** for supervised dimensionality reduction\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸŒŸ Advantages:\n",
    "\n",
    "* ğŸ§  Reduces **complexity** and training time\n",
    "* ğŸ“ˆ Helps in **visualization** of high-dimensional data\n",
    "* ğŸ›¡ Handles **multicollinearity** well\n",
    "\n",
    "---\n",
    "\n",
    "#### âš ï¸ Disadvantages:\n",
    "\n",
    "* âš ï¸ **Loss of interpretability** â€” new features are combinations, not original ones\n",
    "* ğŸ§ª May **lose important variance** if too many components are dropped\n",
    "* ğŸ”„ Needs **standardization** before applying\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
