{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 **1. Introduction to Feature Engineering**\n",
    "\n",
    "* 📌 What is Feature Engineering?\n",
    "* 🧠 Why is it important in ML?\n",
    "* 🧪 Role in model performance\n",
    "\n",
    "---\n",
    "\n",
    "### 🧹 **2. Data Cleaning**\n",
    "\n",
    "* 🧼 Handling Missing Values\n",
    "\n",
    "  * Mean/Median/Mode Imputation\n",
    "  * Forward/Backward Fill\n",
    "  * Using Predictive Models\n",
    "* 🧪 Outlier Detection and Treatment\n",
    "\n",
    "  * Z-score, IQR, Isolation Forest\n",
    "* 🔄 Fixing Data Inconsistencies\n",
    "* 🔍 Removing Duplicates\n",
    "\n",
    "---\n",
    "\n",
    "### 🔡 **3. Encoding Categorical Variables**\n",
    "\n",
    "* 🔢 Label Encoding\n",
    "* 🧮 One-Hot Encoding\n",
    "* 🧲 Ordinal Encoding\n",
    "* 📊 Binary Encoding\n",
    "* 🧩 Target/Mean Encoding\n",
    "* 🎲 Frequency Encoding\n",
    "* 🌍 Hash Encoding\n",
    "\n",
    "---\n",
    "\n",
    "### 🧮 **4. Numerical Feature Transformation**\n",
    "\n",
    "* 🔄 Scaling & Normalization\n",
    "\n",
    "  * Min-Max Scaling\n",
    "  * Standardization (Z-score)\n",
    "  * Robust Scaler\n",
    "  * MaxAbs Scaler\n",
    "* 🌐 Log, Power, Box-Cox Transformations\n",
    "* 🎯 Discretization (Binning)\n",
    "\n",
    "---\n",
    "\n",
    "### ⏳ **5. Temporal Feature Engineering**\n",
    "\n",
    "* 📅 Extracting Time Components\n",
    "\n",
    "  * Day, Month, Year, Hour\n",
    "* 🔁 Time Differences (Durations)\n",
    "* ⏱ Lag Features\n",
    "* 🔄 Rolling/Moving Averages\n",
    "* 🔁 Cyclical Time Encoding (sin/cos)\n",
    "\n",
    "---\n",
    "\n",
    "### 📍 **6. Geospatial Feature Engineering**\n",
    "\n",
    "* 🌍 Latitude & Longitude Features\n",
    "* 📏 Distance Calculations (Haversine)\n",
    "* 🗺 Region/Area Mapping\n",
    "\n",
    "---\n",
    "\n",
    "### 🏗 **7. Feature Construction / Extraction**\n",
    "\n",
    "* 🧮 Mathematical Combinations of Features\n",
    "* 💡 Domain-Specific Features\n",
    "* 📈 Polynomial Features\n",
    "* 🔣 Text Lengths, Word Counts (for NLP)\n",
    "* 🎲 Cross Features (Interaction Terms)\n",
    "\n",
    "---\n",
    "\n",
    "### 🧼 **8. Feature Selection**\n",
    "\n",
    "* 🧪 Statistical Methods (Chi-square, ANOVA)\n",
    "* 📉 Correlation Analysis\n",
    "* 🧠 Feature Importance from Models (RF, XGBoost)\n",
    "* 🚦 Recursive Feature Elimination (RFE)\n",
    "* 🧊 L1 Regularization (Lasso)\n",
    "* 🪞 Variance Thresholding\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 **9. Dimensionality Reduction**\n",
    "\n",
    "* 📉 PCA (Principal Component Analysis)\n",
    "* 🧮 LDA (Linear Discriminant Analysis)\n",
    "* 🧩 t-SNE, UMAP\n",
    "* 🔢 Autoencoders (for Deep Learning)\n",
    "\n",
    "---\n",
    "\n",
    "### 🔤 **10. Text Feature Engineering (NLP)**\n",
    "\n",
    "* 📝 Bag-of-Words (BoW)\n",
    "* 🎯 TF-IDF\n",
    "* 🧠 Word Embeddings (Word2Vec, GloVe)\n",
    "* 🔣 N-grams, Character-level Features\n",
    "* 📏 Readability Scores\n",
    "* 🔢 Sentiment Scores\n",
    "\n",
    "---\n",
    "\n",
    "### 🎥 **11. Image Feature Engineering (CV)**\n",
    "\n",
    "* 🖼 Pixel Intensity Stats\n",
    "* 🧠 Pre-trained CNN Feature Extraction\n",
    "* 🌀 HOG, SIFT, SURF Descriptors\n",
    "* 🧬 Color Histograms\n",
    "\n",
    "---\n",
    "\n",
    "### 🤖 **12. Feature Engineering in Deep Learning**\n",
    "\n",
    "* 📐 Embedding Layers (for categorical features)\n",
    "* 📏 Custom Feature Layers\n",
    "* 🔀 Feature Fusion and Concatenation\n",
    "\n",
    "---\n",
    "\n",
    "### 🧰 **13. Tools & Libraries for Feature Engineering**\n",
    "\n",
    "* 🐍 Python Libraries: `pandas`, `sklearn`, `feature-engine`, `category_encoders`, `tsfresh`\n",
    "* ⚙ Automation: FeatureTools, AutoFeat, PyCaret\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 **1. What is Feature Engineering?**\n",
    "\n",
    "#### 📘 Definition:\n",
    "\n",
    "* 🛠️ Feature engineering is the process of **creating, transforming, selecting, or extracting** relevant variables (features) from raw data to improve the performance of machine learning models.\n",
    "* 🎯 It helps make hidden patterns visible to models, increasing accuracy and interpretability.\n",
    "\n",
    "#### 🧠 Real-World Use Case (When to Use):\n",
    "\n",
    "* Anytime you have **raw, unstructured, or semi-structured data** that needs to be converted into a format usable by ML models.\n",
    "* Example: Converting date of birth to age, converting text reviews into sentiment scores, combining features like “price per square foot.”\n",
    "\n",
    "#### 🚫 When Not to Use:\n",
    "\n",
    "* When using **end-to-end deep learning models** that can learn feature representations automatically (e.g., image classification with CNNs), but even then, feature engineering can help.\n",
    "* When data is already **pre-processed and well-represented**.\n",
    "\n",
    "#### 💻 Code Implementation:\n",
    "\n",
    "Not a specific function—it's a concept, but here’s a simple example:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Raw data\n",
    "df = pd.DataFrame({\n",
    "    'dob': ['2000-01-01', '1995-06-15', '1990-09-10']\n",
    "})\n",
    "\n",
    "# Convert to datetime\n",
    "df['dob'] = pd.to_datetime(df['dob'])\n",
    "\n",
    "# Feature Engineering: Age\n",
    "df['age'] = 2025 - df['dob'].dt.year\n",
    "print(df)\n",
    "```\n",
    "\n",
    "#### 🌟 Advantages:\n",
    "\n",
    "* 🧠 Improves model performance significantly\n",
    "* 🔍 Reveals patterns that models can’t detect automatically\n",
    "* 🧱 Adds domain knowledge into ML process\n",
    "\n",
    "#### ⚠️ Disadvantages:\n",
    "\n",
    "* ⏱️ Time-consuming and requires domain expertise\n",
    "* ⚖️ May lead to **overfitting** if too many irrelevant features are created\n",
    "* 🔄 Can be hard to automate in complex pipelines\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 🧹 **2. Data Cleaning in Feature Engineering**\n",
    "\n",
    "#### 📘 Definition:\n",
    "\n",
    "* 🧼 Data Cleaning is the process of **identifying and correcting (or removing)** inaccurate, incomplete, or irrelevant data to ensure the dataset is clean, consistent, and ready for modeling.\n",
    "* It includes handling missing values, outliers, duplicates, and inconsistent formats.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧠 Real-World Use Case (When to Use):\n",
    "\n",
    "* When your dataset has:\n",
    "\n",
    "  * ❓Missing entries (e.g., user age is blank)\n",
    "  * 😵‍💫 Outlier values (e.g., salary = 9999999)\n",
    "  * 🔁 Duplicated records\n",
    "  * 🔤 Inconsistent formatting (e.g., “Yes”, “yes”, “Y”)\n",
    "\n",
    "Example: In retail data, if `price` is missing or extremely high, cleaning ensures valid model learning.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🚫 When Not to Use:\n",
    "\n",
    "* If data is already clean and validated (rare!)\n",
    "* During **model testing phase**, you shouldn't clean test data using methods fitted on test itself (causes leakage)\n",
    "\n",
    "---\n",
    "\n",
    "#### 💻 Code Implementation:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "df = pd.DataFrame({\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Alice'],\n",
    "    'age': [25, np.nan, 29, 25],\n",
    "    'income': [50000, 60000, None, 50000]\n",
    "})\n",
    "\n",
    "# 🔁 Remove Duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# ❓ Handle Missing Values\n",
    "df['age'].fillna(df['age'].mean(), inplace=True)\n",
    "df['income'].fillna(df['income'].median(), inplace=True)\n",
    "\n",
    "# 🧼 Final cleaned data\n",
    "print(df)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 🌟 Advantages:\n",
    "\n",
    "* ✅ Improves **model accuracy and reliability**\n",
    "* 🔍 Helps discover and fix **hidden data quality issues**\n",
    "* 🧠 Makes dataset interpretable for analysis\n",
    "\n",
    "---\n",
    "\n",
    "#### ⚠️ Disadvantages:\n",
    "\n",
    "* ⏳ Can be **time-intensive** for large datasets\n",
    "* 🧪 Wrong cleaning logic may lead to **loss of useful data**\n",
    "* 🔁 Risk of **data leakage** if cleaning isn’t done carefully (e.g., using test data statistics)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 🔡 **3. Encoding Categorical Variables**\n",
    "\n",
    "#### 📘 Definition:\n",
    "\n",
    "* 🔤 Encoding converts **categorical (non-numeric)** values into a **numerical format** so that ML models can process them.\n",
    "* Different encoding techniques are used depending on whether the categorical variable is nominal (no order) or ordinal (has order).\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧠 Real-World Use Case (When to Use):\n",
    "\n",
    "* 🌍 When your dataset contains values like `\"Male/Female\"`, `\"Red/Blue/Green\"`, `\"Low/Medium/High\"` etc.\n",
    "* 🏦 Example: In banking, customer “account\\_type” (`savings`, `current`, `loan`) must be converted into numbers before model training.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🚫 When Not to Use:\n",
    "\n",
    "* ❌ When using models that **natively handle categories**, like CatBoost or LightGBM (they accept categorical inputs directly).\n",
    "* ⚠️ Don’t apply **Label Encoding** to nominal features for tree-based models — it may imply a false order.\n",
    "\n",
    "---\n",
    "\n",
    "#### 💻 Code Implementation (Different Types):\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import category_encoders as ce\n",
    "\n",
    "# Sample data\n",
    "df = pd.DataFrame({'color': ['Red', 'Blue', 'Green', 'Blue']})\n",
    "\n",
    "# 🔢 Label Encoding\n",
    "le = LabelEncoder()\n",
    "df['label_encoded'] = le.fit_transform(df['color'])\n",
    "\n",
    "# 🧮 One-Hot Encoding\n",
    "df_onehot = pd.get_dummies(df['color'], prefix='color')\n",
    "\n",
    "# 🎯 Target/Mean Encoding (for supervised tasks)\n",
    "# Note: Normally you use the target column here\n",
    "encoder = ce.TargetEncoder()\n",
    "df['target_encoded'] = encoder.fit_transform(df['color'], [1, 0, 1, 0])\n",
    "\n",
    "print(df)\n",
    "print(df_onehot)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 🌟 Advantages:\n",
    "\n",
    "* 🔁 Converts non-numeric data into numeric for ML models\n",
    "* 🧠 Methods like target encoding consider the relationship with the label\n",
    "* 🔢 One-hot encoding prevents false ordinal relationships\n",
    "\n",
    "---\n",
    "\n",
    "#### ⚠️ Disadvantages:\n",
    "\n",
    "* 🧨 One-Hot Encoding can cause **dimensionality explosion** with high-cardinality features\n",
    "* 🎭 Label Encoding can **mislead** tree models if used wrongly on nominal features\n",
    "* 🧮 Target Encoding may lead to **data leakage** if not cross-validated\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 🧮 **4. Numerical Feature Transformation**\n",
    "\n",
    "#### 📘 Definition:\n",
    "\n",
    "* 🔄 Numerical transformation is the process of **modifying numeric features** to improve model performance or meet algorithm assumptions.\n",
    "* It includes **scaling**, **normalization**, and **mathematical transformations** like log, square root, etc.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧠 Real-World Use Case (When to Use):\n",
    "\n",
    "* 📏 When your features have **different units or ranges** (e.g., income in lakhs vs. age in years)\n",
    "* 📈 When a feature is **skewed** (e.g., income, sales) and needs transformation for better model behavior\n",
    "* 🧠 Algorithms like **KNN, SVM, and logistic regression** are sensitive to feature scale.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🚫 When Not to Use:\n",
    "\n",
    "* 🌲 Tree-based models (like Random Forest, XGBoost) usually **don’t require scaling**\n",
    "* ❌ Don't use log or root on **zero or negative values** without fixing them first\n",
    "\n",
    "---\n",
    "\n",
    "#### 💻 Code Implementation:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "# Sample data\n",
    "df = pd.DataFrame({\n",
    "    'income': [30000, 50000, 70000, 100000, 1000000],\n",
    "    'age': [25, 30, 35, 40, 28]\n",
    "})\n",
    "\n",
    "# 🔄 Min-Max Scaling (0 to 1)\n",
    "min_max = MinMaxScaler()\n",
    "df['income_minmax'] = min_max.fit_transform(df[['income']])\n",
    "\n",
    "# 🧠 Standardization (Z-score)\n",
    "standard = StandardScaler()\n",
    "df['income_standard'] = standard.fit_transform(df[['income']])\n",
    "\n",
    "# 🛡 Robust Scaling (ignores outliers)\n",
    "robust = RobustScaler()\n",
    "df['income_robust'] = robust.fit_transform(df[['income']])\n",
    "\n",
    "# 🔍 Log Transformation (for skewed data)\n",
    "df['income_log'] = np.log1p(df['income'])  # log1p = log(1 + x)\n",
    "\n",
    "print(df)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 🌟 Advantages:\n",
    "\n",
    "* 🎯 Makes models converge faster (especially gradient-based models)\n",
    "* 📉 Handles skewed distributions and outliers\n",
    "* 🤖 Prepares data for distance-based models (like KNN)\n",
    "\n",
    "---\n",
    "\n",
    "#### ⚠️ Disadvantages:\n",
    "\n",
    "* ⚖ May distort feature meaning or interpretability\n",
    "* 🧮 Sensitive to **zero or negative values** (log, sqrt)\n",
    "* 🧠 Needs careful choice of method based on distribution\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### ⏳ **5. Temporal Feature Engineering (Time-based Features)**\n",
    "\n",
    "#### 📘 Definition:\n",
    "\n",
    "* 📅 Temporal feature engineering involves extracting and transforming **date and time-related data** into meaningful features that improve model insights.\n",
    "* Common operations include extracting **day, month, hour**, calculating **differences**, and creating **lag/rolling features**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧠 Real-World Use Case (When to Use):\n",
    "\n",
    "* 📊 In sales forecasting, you can extract:\n",
    "\n",
    "  * **Day of week** (to catch weekend effects)\n",
    "  * **Month** (seasonality)\n",
    "  * **Lag features** (previous day's sales)\n",
    "  * **Rolling mean** (7-day average sales)\n",
    "* 🚌 In transportation, time-based features can detect rush hour patterns.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🚫 When Not to Use:\n",
    "\n",
    "* 🧊 When timestamps are irrelevant to your prediction (e.g., predicting customer churn from one-time data)\n",
    "* ⚠️ Avoid using **future timestamps** for predictions — leads to **data leakage**\n",
    "\n",
    "---\n",
    "\n",
    "#### 💻 Code Implementation:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Sample timestamped dataset\n",
    "df = pd.DataFrame({\n",
    "    'timestamp': pd.date_range(start='2025-01-01', periods=5, freq='D'),\n",
    "    'sales': [100, 150, 200, 180, 220]\n",
    "})\n",
    "\n",
    "# 📅 Extract components\n",
    "df['day'] = df['timestamp'].dt.day\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "df['weekday'] = df['timestamp'].dt.weekday\n",
    "\n",
    "# ⏱ Lag feature (previous day’s sales)\n",
    "df['sales_lag1'] = df['sales'].shift(1)\n",
    "\n",
    "# 📉 Rolling mean (last 3 days)\n",
    "df['sales_roll3'] = df['sales'].rolling(window=3).mean()\n",
    "\n",
    "# ⭕ Cyclical Encoding (for weekday)\n",
    "import numpy as np\n",
    "df['weekday_sin'] = np.sin(2 * np.pi * df['weekday'] / 7)\n",
    "df['weekday_cos'] = np.cos(2 * np.pi * df['weekday'] / 7)\n",
    "\n",
    "print(df)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 🌟 Advantages:\n",
    "\n",
    "* 🔍 Captures **seasonality**, **trends**, and **cyclical behavior**\n",
    "* ⏱ Enables **lag-based modeling** (e.g., time series forecasting)\n",
    "* 💡 Turns datetime into **actionable features**\n",
    "\n",
    "---\n",
    "\n",
    "#### ⚠️ Disadvantages:\n",
    "\n",
    "* 🔄 Requires **chronological order** for accuracy\n",
    "* 🧠 Improper handling of lags/rolling windows can lead to **data leakage**\n",
    "* ⏳ May add **redundant features** if not selected carefully\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 📍 **6. Geospatial Feature Engineering**\n",
    "\n",
    "#### 📘 Definition:\n",
    "\n",
    "* 🌍 Geospatial feature engineering is the process of extracting and creating new features from **geographic data** like latitude, longitude, ZIP code, or coordinates.\n",
    "* It helps capture **location-based patterns**, distances, and regional trends in ML models.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧠 Real-World Use Case (When to Use):\n",
    "\n",
    "* 🛵 In delivery apps (like Zomato, Uber), you can:\n",
    "\n",
    "  * Calculate **distance** between customer and restaurant\n",
    "  * Use coordinates to cluster areas\n",
    "* 🏘 In real estate: use **location data** to estimate property prices\n",
    "* 🧑‍🌾 Agriculture: analyze **soil, rainfall, or crop zones** based on region\n",
    "\n",
    "---\n",
    "\n",
    "#### 🚫 When Not to Use:\n",
    "\n",
    "* 📦 If location doesn’t influence the target variable (e.g., predicting product color from stock location)\n",
    "* 🧊 When only **textual location names** (e.g., city names without coordinates) are available and can’t be encoded meaningfully\n",
    "\n",
    "---\n",
    "\n",
    "#### 💻 Code Implementation:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Sample coordinates\n",
    "df = pd.DataFrame({\n",
    "    'store_lat': [28.61, 19.07],\n",
    "    'store_lon': [77.23, 72.87],\n",
    "    'user_lat': [28.70, 18.90],\n",
    "    'user_lon': [77.10, 72.70]\n",
    "})\n",
    "\n",
    "# 📏 Calculate geodesic distance in kilometers\n",
    "df['distance_km'] = df.apply(lambda row: geodesic(\n",
    "    (row['store_lat'], row['store_lon']),\n",
    "    (row['user_lat'], row['user_lon'])\n",
    ").km, axis=1)\n",
    "\n",
    "print(df)\n",
    "```\n",
    "\n",
    "Other possible features:\n",
    "\n",
    "* 🗺 Region or cluster (using ZIP code or KMeans on coordinates)\n",
    "* 🌡 Climate zone or risk zone (based on lat/lon)\n",
    "\n",
    "---\n",
    "\n",
    "#### 🌟 Advantages:\n",
    "\n",
    "* 📊 Adds **contextual intelligence** to models (location matters!)\n",
    "* 📍 Enables **distance-based filtering or optimization**\n",
    "* 🧠 Captures **spatial trends** for better predictions\n",
    "\n",
    "---\n",
    "\n",
    "#### ⚠️ Disadvantages:\n",
    "\n",
    "* 🧮 Calculating distances can be **computationally expensive**\n",
    "* 🌍 Latitude/longitude alone may not hold meaning — need transformation\n",
    "* 🔄 Static coordinates may not reflect **dynamic behavior** (e.g., travel time)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 🏗 **7. Feature Construction / Extraction**\n",
    "\n",
    "#### 📘 Definition:\n",
    "\n",
    "* 🧱 Feature construction is the process of **creating new features** from existing ones by combining, aggregating, or transforming them to reveal hidden patterns or relationships.\n",
    "* Think of it as **\"building new building blocks\"** for your ML model from the existing data.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧠 Real-World Use Case (When to Use):\n",
    "\n",
    "* 🏠 Real Estate:\n",
    "\n",
    "  * Construct “**price per square foot**” from total price and area.\n",
    "* 🛒 E-commerce:\n",
    "\n",
    "  * Create “**total spent**” from quantity × unit price.\n",
    "* 📱 App Usage:\n",
    "\n",
    "  * Combine “login time” and “logout time” to compute “**session duration**”\n",
    "\n",
    "---\n",
    "\n",
    "#### 🚫 When Not to Use:\n",
    "\n",
    "* ⚖️ When you already have **optimized domain-specific features**\n",
    "* ❌ Avoid if it leads to overfitting due to high number of **noisy or redundant features**\n",
    "\n",
    "---\n",
    "\n",
    "#### 💻 Code Implementation:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Original data\n",
    "df = pd.DataFrame({\n",
    "    'total_price': [1000, 2000, 1500],\n",
    "    'area_sqft': [500, 1000, 750],\n",
    "    'login_time': [8.5, 9.0, 7.75],   # In hours\n",
    "    'logout_time': [10.0, 10.5, 9.25]\n",
    "})\n",
    "\n",
    "# 🧮 Feature Construction\n",
    "df['price_per_sqft'] = df['total_price'] / df['area_sqft']\n",
    "df['session_duration'] = df['logout_time'] - df['login_time']\n",
    "\n",
    "print(df)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 🌟 Advantages:\n",
    "\n",
    "* 🔍 Reveals **hidden relationships**\n",
    "* 📈 Boosts model accuracy with meaningful context\n",
    "* 🎯 Allows models to **focus on what really matters**\n",
    "\n",
    "---\n",
    "\n",
    "#### ⚠️ Disadvantages:\n",
    "\n",
    "* 🧠 Requires **domain expertise** to know what to create\n",
    "* 📊 Too many engineered features can lead to **overfitting**\n",
    "* 🧹 Needs regularization or selection if overused\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### ✂️ **8. Feature Selection**\n",
    "\n",
    "#### 📘 Definition:\n",
    "\n",
    "* ✨ Feature selection is the process of **selecting the most relevant features** and removing **irrelevant, redundant, or noisy** ones to improve model performance and reduce complexity.\n",
    "* It helps in making the model more **interpretable, faster, and generalizable**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧠 Real-World Use Case (When to Use):\n",
    "\n",
    "* 🧪 In medical diagnosis, selecting key symptoms/features avoids confusion and improves prediction quality.\n",
    "* 💼 In finance, selecting only the impactful indicators (e.g., interest rate, inflation) helps models generalize better.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🚫 When Not to Use:\n",
    "\n",
    "* 🤖 Deep learning models can often **automatically learn** feature representations.\n",
    "* ⚠️ When you have **too little data**, aggressive selection may cause **underfitting**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 💻 Code Implementation:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Sample dataset\n",
    "df = pd.DataFrame({\n",
    "    'feature1': [1, 2, 3, 4, 5],\n",
    "    'feature2': [10, 20, 30, 40, 50],\n",
    "    'feature3': [9, 8, 7, 6, 5],\n",
    "    'target': [0, 1, 0, 1, 0]\n",
    "})\n",
    "\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# 📊 Select top 2 features using ANOVA F-test\n",
    "selector = SelectKBest(score_func=f_classif, k=2)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "# Selected columns\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "print(\"Selected Features:\", selected_features.tolist())\n",
    "```\n",
    "\n",
    "Other methods:\n",
    "\n",
    "* ✅ `Recursive Feature Elimination (RFE)`\n",
    "* 🧠 `Tree-based feature importance`\n",
    "* 🔍 `L1 regularization (Lasso)`\n",
    "\n",
    "---\n",
    "\n",
    "#### 🌟 Advantages:\n",
    "\n",
    "* ⚡ Improves **model speed and accuracy**\n",
    "* 🧼 Reduces **overfitting** and noise\n",
    "* 📉 Decreases dimensionality and improves interpretability\n",
    "\n",
    "---\n",
    "\n",
    "#### ⚠️ Disadvantages:\n",
    "\n",
    "* ❌ May drop **useful features** if not done carefully\n",
    "* 🔄 Depends on the feature selection method used\n",
    "* 🧪 Risk of **information loss** if selection is aggressive\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 🔄 **9. Dimensionality Reduction**\n",
    "\n",
    "#### 📘 Definition:\n",
    "\n",
    "* 📉 Dimensionality reduction is the process of **reducing the number of input variables** (features) in a dataset by **projecting them into a lower-dimensional space** while preserving most of the information.\n",
    "* Unlike feature selection (which removes features), dimensionality reduction **transforms** features into a smaller set.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧠 Real-World Use Case (When to Use):\n",
    "\n",
    "* 🧬 In genomics, thousands of gene expression features are reduced to core patterns.\n",
    "* 🖼 In image processing, it reduces pixels/features for faster ML modeling.\n",
    "* 💼 In customer data, you can reduce dozens of behavioral metrics into a few principal components.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🚫 When Not to Use:\n",
    "\n",
    "* ❌ When **feature interpretability is essential** (e.g., in regulated industries)\n",
    "* 📊 When all original features are already few and well-defined\n",
    "\n",
    "---\n",
    "\n",
    "#### 💻 Code Implementation:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample data\n",
    "df = pd.DataFrame({\n",
    "    'math': [90, 85, 80, 70, 60],\n",
    "    'science': [88, 84, 81, 69, 65],\n",
    "    'english': [78, 75, 73, 60, 58]\n",
    "})\n",
    "\n",
    "# 🔄 Standardize data\n",
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(df)\n",
    "\n",
    "# 🧪 Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced = pca.fit_transform(scaled)\n",
    "\n",
    "# 📊 Resulting components\n",
    "df_pca = pd.DataFrame(reduced, columns=['PC1', 'PC2'])\n",
    "print(df_pca)\n",
    "```\n",
    "\n",
    "Other techniques:\n",
    "\n",
    "* 🧠 **t-SNE** (for visualization)\n",
    "* 🌀 **UMAP**\n",
    "* 📐 **Linear Discriminant Analysis (LDA)** for supervised dimensionality reduction\n",
    "\n",
    "---\n",
    "\n",
    "#### 🌟 Advantages:\n",
    "\n",
    "* 🧠 Reduces **complexity** and training time\n",
    "* 📈 Helps in **visualization** of high-dimensional data\n",
    "* 🛡 Handles **multicollinearity** well\n",
    "\n",
    "---\n",
    "\n",
    "#### ⚠️ Disadvantages:\n",
    "\n",
    "* ⚠️ **Loss of interpretability** — new features are combinations, not original ones\n",
    "* 🧪 May **lose important variance** if too many components are dropped\n",
    "* 🔄 Needs **standardization** before applying\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
