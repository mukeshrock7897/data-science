{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **🐼Pandas Cheatsheet**\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 1. DataFrame & Series Creation  \n",
    "📌 Create structured data containers  \n",
    "🔧  \n",
    "- `pd.Series(data)` 📍 – 1D labeled array  \n",
    "- `pd.DataFrame(data)` 📋 – 2D labeled table  \n",
    "- `pd.read_csv()` 📄 – Read CSV  \n",
    "- `pd.read_excel()` 📊 – Read Excel  \n",
    "- `pd.read_json()` 📦 – Read JSON  \n",
    "- `df.to_csv()` 💾 – Save to CSV  \n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 2. Inspection & Info  \n",
    "📌 Explore structure, types, and data summary  \n",
    "🔧  \n",
    "- `df.head(n)` 👀 – First n rows  \n",
    "- `df.tail(n)` 🔚 – Last n rows  \n",
    "- `df.info()` ℹ️ – Summary info  \n",
    "- `df.describe()` 📐 – Statistical summary  \n",
    "- `df.shape / df.columns / df.index` 📏 – Structure details  \n",
    "- `df.dtypes` 🧬 – Data types  \n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 3. Selection & Filtering  \n",
    "📌 Get specific rows/columns/conditions  \n",
    "🔧  \n",
    "- `df['col']` 📌 – Access column  \n",
    "- `df[['col1', 'col2']]` 🧲 – Multiple columns  \n",
    "- `df.iloc[rows, cols]` 🔢 – Integer-location access  \n",
    "- `df.loc[rows, cols]` 🔤 – Label-based access  \n",
    "- `df[df['col'] > 5]` 🔍 – Conditional filter  \n",
    "- `df.query('col > 5')` 🧠 – SQL-style filtering  \n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 4. Data Cleaning  \n",
    "📌 Handle missing, duplicates, and type fixes  \n",
    "🔧  \n",
    "- `df.isnull()` ❓ – Check NaNs  \n",
    "- `df.dropna()` 🗑 – Remove NaNs  \n",
    "- `df.fillna(value)` 💧 – Fill NaNs  \n",
    "- `df.duplicated()` 🧬 – Find duplicates  \n",
    "- `df.drop_duplicates()` 🚮 – Remove duplicates  \n",
    "- `df.astype(type)` 🔁 – Convert types  \n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 5. Sorting & Ranking  \n",
    "📌 Organize and rank data  \n",
    "🔧  \n",
    "- `df.sort_values(by='col')` 🔃 – Sort by column  \n",
    "- `df.sort_index()` 🔢 – Sort by index  \n",
    "- `df.rank()` 🏅 – Ranking values  \n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 6. Aggregation & Grouping  \n",
    "📌 Summarize and analyze grouped data  \n",
    "🔧  \n",
    "- `df.groupby('col')` 🧩 – Group by  \n",
    "- `df.groupby('col').agg(['mean', 'sum'])` 📊 – Aggregate  \n",
    "- `df.pivot_table()` 🔄 – Pivot summary  \n",
    "- `df.value_counts()` 🔢 – Count unique  \n",
    "- `df.crosstab()` 🔀 – Cross tabulation  \n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 7. Merging & Joining  \n",
    "📌 Combine datasets  \n",
    "🔧  \n",
    "- `pd.concat([df1, df2])` ➕ – Stack vertically/horizontally  \n",
    "- `pd.merge(df1, df2, on='key')` 🔗 – SQL-style join  \n",
    "- `df.join(other_df)` 🤝 – Join by index  \n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 8. Apply & Mapping  \n",
    "📌 Element-wise transformations  \n",
    "🔧  \n",
    "- `df['col'].map(func)` 🧠 – Map values  \n",
    "- `df.apply(func)` ⚙️ – Apply function to rows/cols  \n",
    "- `df.applymap(func)` 🔁 – Apply to every cell (element-wise)  \n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 9. Time Series  \n",
    "📌 Time-based indexing & resampling  \n",
    "🔧  \n",
    "- `pd.to_datetime()` 🕒 – Convert to datetime  \n",
    "- `df.resample('M')` 📆 – Resample by time  \n",
    "- `df['date'].dt.year` 📅 – Extract year  \n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 10. Exporting & I/O  \n",
    "📌 Save and load from files  \n",
    "🔧  \n",
    "- `df.to_csv('file.csv')` 💾  \n",
    "- `df.to_excel('file.xlsx')` 📤  \n",
    "- `df.to_json('file.json')` 📦  \n",
    "- `pd.read_sql(query, conn)` 🛢 – SQL to DataFrame  \n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ Bonus Utilities  \n",
    "📌 Handy tricks & performance  \n",
    "🔧  \n",
    "- `pd.set_option('display.max_columns', None)` 🖥 – Show all columns  \n",
    "- `df.memory_usage()` 📊 – Check memory  \n",
    "- `df.sample(n)` 🎯 – Random sample  \n",
    "- `df.nunique()` 🧮 – Count unique per column  \n",
    "- `df.corr()` 📈 – Correlation matrix  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# **🤖Pandas for Machine Learning – Advanced Cheatsheet**\n",
    "\n",
    "---\n",
    "\n",
    "### 📥 1. Data Loading & Preparation  \n",
    "📌 Get and prepare data for ML models  \n",
    "🔧  \n",
    "- `pd.read_csv('data.csv')` 📄 – Load dataset  \n",
    "- `df.sample(frac=0.1)` 🎯 – Random sample  \n",
    "- `df.drop(columns=['col'])` 🗑 – Drop unwanted columns  \n",
    "- `df.rename(columns={'old': 'new'})` 🏷 – Rename columns  \n",
    "- `df.reset_index(drop=True)` 🔄 – Reset index  \n",
    "\n",
    "---\n",
    "\n",
    "### 🧹 2. Data Cleaning  \n",
    "📌 Clean up messy real-world data  \n",
    "🔧  \n",
    "- `df.isnull().sum()` ❓ – Count NaNs  \n",
    "- `df.dropna()` 🚮 – Drop rows with NaNs  \n",
    "- `df.fillna(value)` 💧 – Fill missing  \n",
    "- `df.duplicated()` 🧬 – Detect duplicates  \n",
    "- `df.drop_duplicates()` 🧹 – Drop duplicates  \n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 3. Feature Engineering  \n",
    "📌 Create, modify, or encode features  \n",
    "🔧  \n",
    "- `df['new'] = df['col1'] + df['col2']` ➕ – Create new feature  \n",
    "- `pd.get_dummies(df, drop_first=True)` 🧯 – One-hot encoding  \n",
    "- `df['col'].map({'A': 0, 'B': 1})` 🔁 – Label encoding  \n",
    "- `df['text'].str.extract(r'regex')` 🧠 – Text feature extraction  \n",
    "- `df['col'].apply(lambda x: x**2)` 🧪 – Feature transformation  \n",
    "\n",
    "---\n",
    "\n",
    "### 🏗 4. Data Splitting  \n",
    "📌 Prepare train/test/validation sets  \n",
    "🔧  \n",
    "- `from sklearn.model_selection import train_test_split` ✂️  \n",
    "- `X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)` 🔍 – Train-test split  \n",
    "\n",
    "---\n",
    "\n",
    "### 🧮 5. Scaling & Normalization  \n",
    "📌 Preprocess features before training  \n",
    "🔧  \n",
    "- `from sklearn.preprocessing import StandardScaler` 📊  \n",
    "- `scaler = StandardScaler()`  \n",
    "- `X_scaled = scaler.fit_transform(X)` 🧼 – Z-score scaling  \n",
    "\n",
    "Other options:  \n",
    "- `MinMaxScaler()` 📏 – Normalize to [0, 1]  \n",
    "- `RobustScaler()` 🧱 – Handle outliers  \n",
    "\n",
    "---\n",
    "\n",
    "### 📊 6. Correlation & Stats  \n",
    "📌 Explore data relationships  \n",
    "🔧  \n",
    "- `df.corr()` 🔗 – Correlation matrix  \n",
    "- `df['target'].value_counts()` 🧮 – Class balance check  \n",
    "- `df.groupby('label').mean()` 📐 – Stats by group  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 7. Model Evaluation Helpers  \n",
    "📌 Analyze results after predictions  \n",
    "🔧  \n",
    "- `df['pred_error'] = df['y_true'] - df['y_pred']` 🧾 – Error column  \n",
    "- `df['correct'] = df['y_true'] == df['y_pred']` ✅ – Boolean match  \n",
    "- `df['prob'] = model.predict_proba(X)[:,1]` 📈 – Probabilities  \n",
    "- `df['conf'] = np.max(model.predict_proba(X), axis=1)` 🧪 – Confidence scores  \n",
    "\n",
    "---\n",
    "\n",
    "### 📦 8. Export for Modeling  \n",
    "📌 Save cleaned/preprocessed data  \n",
    "🔧  \n",
    "- `df.to_csv('cleaned_data.csv', index=False)` 💾  \n",
    "- `df.to_pickle('df.pkl')` 🧺 – Save with types  \n",
    "- `df.to_parquet('df.parquet')` 🚀 – For big data ML  \n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ Extra Tricks for ML Pipelines  \n",
    "📌 Helpful utilities used in real ML workflows  \n",
    "🔧  \n",
    "- `df.select_dtypes(include='number')` 🔢 – Numeric columns only  \n",
    "- `df.columns[df.isnull().any()]` 🚨 – Columns with NaNs  \n",
    "- `df['cat'] = df['cat'].astype('category')` 🧬 – Convert to categorical  \n",
    "- `df.sort_values(by='importance', ascending=False)` 📌 – Feature importance  \n",
    "\n",
    "---\n",
    "\n",
    "# **🤖Scikit-learn + Pandas Integration Cheatsheet**\n",
    "\n",
    "---\n",
    "\n",
    "### 📋 1. Prepare Features & Target  \n",
    "📌 Slice your DataFrame into X (features) and y (target)  \n",
    "```python\n",
    "X = df.drop('target', axis=1)  # 🎯 Features\n",
    "y = df['target']               # 🏷 Target\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✂️ 2. Train-Test Split  \n",
    "📌 Split DataFrame for training and testing  \n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🧼 3. Use Pandas with Scalers  \n",
    "📌 Keep column names even after scaling  \n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X),\n",
    "    columns=X.columns\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🏗 4. Build Pipelines with Pandas  \n",
    "📌 Combine preprocessing + modeling (safe with DataFrames!)  \n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🧱 5. ColumnTransformer with Pandas  \n",
    "📌 Apply different transforms to numeric vs categorical columns  \n",
    "```python\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "numeric_cols = X.select_dtypes(include='number').columns\n",
    "cat_cols = X.select_dtypes(include='object').columns\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numeric_cols),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
    "])\n",
    "\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('model', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "full_pipeline.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 📈 6. Predictions with Pandas Index  \n",
    "📌 Return predictions with original row index  \n",
    "```python\n",
    "y_pred = pd.Series(pipeline.predict(X_test), index=X_test.index)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 7. Cross-Validation with DataFrame  \n",
    "📌 No need to convert to arrays – it just works  \n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(pipeline, X, y, cv=5)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🧾 8. Feature Importances with Columns  \n",
    "📌 Get feature importance + original column names  \n",
    "```python\n",
    "model = pipeline.named_steps['model']\n",
    "features = preprocessor.get_feature_names_out()\n",
    "pd.Series(model.feature_importances_, index=features).sort_values(ascending=False)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 9. Export Model & DataFrame Together  \n",
    "📌 Save full pipeline (with preprocessing)  \n",
    "```python\n",
    "import joblib\n",
    "joblib.dump(full_pipeline, 'model_pipeline.pkl')  # 💾 Save full model\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ 10. Clean Predictions with Original Data  \n",
    "📌 Combine predictions with the original DataFrame  \n",
    "```python\n",
    "df_result = df.copy()\n",
    "df_result['pred'] = pipeline.predict(X)\n",
    "```\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
