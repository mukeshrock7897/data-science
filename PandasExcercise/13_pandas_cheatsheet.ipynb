{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **ğŸ¼Pandas Cheatsheet**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  1. DataFrame & Series Creation  \n",
    "ğŸ“Œ Create structured data containers  \n",
    "ğŸ”§  \n",
    "- `pd.Series(data)` ğŸ“ â€“ 1D labeled array  \n",
    "- `pd.DataFrame(data)` ğŸ“‹ â€“ 2D labeled table  \n",
    "- `pd.read_csv()` ğŸ“„ â€“ Read CSV  \n",
    "- `pd.read_excel()` ğŸ“Š â€“ Read Excel  \n",
    "- `pd.read_json()` ğŸ“¦ â€“ Read JSON  \n",
    "- `df.to_csv()` ğŸ’¾ â€“ Save to CSV  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  2. Inspection & Info  \n",
    "ğŸ“Œ Explore structure, types, and data summary  \n",
    "ğŸ”§  \n",
    "- `df.head(n)` ğŸ‘€ â€“ First n rows  \n",
    "- `df.tail(n)` ğŸ”š â€“ Last n rows  \n",
    "- `df.info()` â„¹ï¸ â€“ Summary info  \n",
    "- `df.describe()` ğŸ“ â€“ Statistical summary  \n",
    "- `df.shape / df.columns / df.index` ğŸ“ â€“ Structure details  \n",
    "- `df.dtypes` ğŸ§¬ â€“ Data types  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  3. Selection & Filtering  \n",
    "ğŸ“Œ Get specific rows/columns/conditions  \n",
    "ğŸ”§  \n",
    "- `df['col']` ğŸ“Œ â€“ Access column  \n",
    "- `df[['col1', 'col2']]` ğŸ§² â€“ Multiple columns  \n",
    "- `df.iloc[rows, cols]` ğŸ”¢ â€“ Integer-location access  \n",
    "- `df.loc[rows, cols]` ğŸ”¤ â€“ Label-based access  \n",
    "- `df[df['col'] > 5]` ğŸ” â€“ Conditional filter  \n",
    "- `df.query('col > 5')` ğŸ§  â€“ SQL-style filtering  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  4. Data Cleaning  \n",
    "ğŸ“Œ Handle missing, duplicates, and type fixes  \n",
    "ğŸ”§  \n",
    "- `df.isnull()` â“ â€“ Check NaNs  \n",
    "- `df.dropna()` ğŸ—‘ â€“ Remove NaNs  \n",
    "- `df.fillna(value)` ğŸ’§ â€“ Fill NaNs  \n",
    "- `df.duplicated()` ğŸ§¬ â€“ Find duplicates  \n",
    "- `df.drop_duplicates()` ğŸš® â€“ Remove duplicates  \n",
    "- `df.astype(type)` ğŸ” â€“ Convert types  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  5. Sorting & Ranking  \n",
    "ğŸ“Œ Organize and rank data  \n",
    "ğŸ”§  \n",
    "- `df.sort_values(by='col')` ğŸ”ƒ â€“ Sort by column  \n",
    "- `df.sort_index()` ğŸ”¢ â€“ Sort by index  \n",
    "- `df.rank()` ğŸ… â€“ Ranking values  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  6. Aggregation & Grouping  \n",
    "ğŸ“Œ Summarize and analyze grouped data  \n",
    "ğŸ”§  \n",
    "- `df.groupby('col')` ğŸ§© â€“ Group by  \n",
    "- `df.groupby('col').agg(['mean', 'sum'])` ğŸ“Š â€“ Aggregate  \n",
    "- `df.pivot_table()` ğŸ”„ â€“ Pivot summary  \n",
    "- `df.value_counts()` ğŸ”¢ â€“ Count unique  \n",
    "- `df.crosstab()` ğŸ”€ â€“ Cross tabulation  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  7. Merging & Joining  \n",
    "ğŸ“Œ Combine datasets  \n",
    "ğŸ”§  \n",
    "- `pd.concat([df1, df2])` â• â€“ Stack vertically/horizontally  \n",
    "- `pd.merge(df1, df2, on='key')` ğŸ”— â€“ SQL-style join  \n",
    "- `df.join(other_df)` ğŸ¤ â€“ Join by index  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  8. Apply & Mapping  \n",
    "ğŸ“Œ Element-wise transformations  \n",
    "ğŸ”§  \n",
    "- `df['col'].map(func)` ğŸ§  â€“ Map values  \n",
    "- `df.apply(func)` âš™ï¸ â€“ Apply function to rows/cols  \n",
    "- `df.applymap(func)` ğŸ” â€“ Apply to every cell (element-wise)  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  9. Time Series  \n",
    "ğŸ“Œ Time-based indexing & resampling  \n",
    "ğŸ”§  \n",
    "- `pd.to_datetime()` ğŸ•’ â€“ Convert to datetime  \n",
    "- `df.resample('M')` ğŸ“† â€“ Resample by time  \n",
    "- `df['date'].dt.year` ğŸ“… â€“ Extract year  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  10. Exporting & I/O  \n",
    "ğŸ“Œ Save and load from files  \n",
    "ğŸ”§  \n",
    "- `df.to_csv('file.csv')` ğŸ’¾  \n",
    "- `df.to_excel('file.xlsx')` ğŸ“¤  \n",
    "- `df.to_json('file.json')` ğŸ“¦  \n",
    "- `pd.read_sql(query, conn)` ğŸ›¢ â€“ SQL to DataFrame  \n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ Bonus Utilities  \n",
    "ğŸ“Œ Handy tricks & performance  \n",
    "ğŸ”§  \n",
    "- `pd.set_option('display.max_columns', None)` ğŸ–¥ â€“ Show all columns  \n",
    "- `df.memory_usage()` ğŸ“Š â€“ Check memory  \n",
    "- `df.sample(n)` ğŸ¯ â€“ Random sample  \n",
    "- `df.nunique()` ğŸ§® â€“ Count unique per column  \n",
    "- `df.corr()` ğŸ“ˆ â€“ Correlation matrix  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# **ğŸ¤–Pandas for Machine Learning â€“ Advanced Cheatsheet**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“¥ 1. Data Loading & Preparation  \n",
    "ğŸ“Œ Get and prepare data for ML models  \n",
    "ğŸ”§  \n",
    "- `pd.read_csv('data.csv')` ğŸ“„ â€“ Load dataset  \n",
    "- `df.sample(frac=0.1)` ğŸ¯ â€“ Random sample  \n",
    "- `df.drop(columns=['col'])` ğŸ—‘ â€“ Drop unwanted columns  \n",
    "- `df.rename(columns={'old': 'new'})` ğŸ· â€“ Rename columns  \n",
    "- `df.reset_index(drop=True)` ğŸ”„ â€“ Reset index  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§¹ 2. Data Cleaning  \n",
    "ğŸ“Œ Clean up messy real-world data  \n",
    "ğŸ”§  \n",
    "- `df.isnull().sum()` â“ â€“ Count NaNs  \n",
    "- `df.dropna()` ğŸš® â€“ Drop rows with NaNs  \n",
    "- `df.fillna(value)` ğŸ’§ â€“ Fill missing  \n",
    "- `df.duplicated()` ğŸ§¬ â€“ Detect duplicates  \n",
    "- `df.drop_duplicates()` ğŸ§¹ â€“ Drop duplicates  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§ª 3. Feature Engineering  \n",
    "ğŸ“Œ Create, modify, or encode features  \n",
    "ğŸ”§  \n",
    "- `df['new'] = df['col1'] + df['col2']` â• â€“ Create new feature  \n",
    "- `pd.get_dummies(df, drop_first=True)` ğŸ§¯ â€“ One-hot encoding  \n",
    "- `df['col'].map({'A': 0, 'B': 1})` ğŸ” â€“ Label encoding  \n",
    "- `df['text'].str.extract(r'regex')` ğŸ§  â€“ Text feature extraction  \n",
    "- `df['col'].apply(lambda x: x**2)` ğŸ§ª â€“ Feature transformation  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ— 4. Data Splitting  \n",
    "ğŸ“Œ Prepare train/test/validation sets  \n",
    "ğŸ”§  \n",
    "- `from sklearn.model_selection import train_test_split` âœ‚ï¸  \n",
    "- `X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)` ğŸ” â€“ Train-test split  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§® 5. Scaling & Normalization  \n",
    "ğŸ“Œ Preprocess features before training  \n",
    "ğŸ”§  \n",
    "- `from sklearn.preprocessing import StandardScaler` ğŸ“Š  \n",
    "- `scaler = StandardScaler()`  \n",
    "- `X_scaled = scaler.fit_transform(X)` ğŸ§¼ â€“ Z-score scaling  \n",
    "\n",
    "Other options:  \n",
    "- `MinMaxScaler()` ğŸ“ â€“ Normalize to [0, 1]  \n",
    "- `RobustScaler()` ğŸ§± â€“ Handle outliers  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š 6. Correlation & Stats  \n",
    "ğŸ“Œ Explore data relationships  \n",
    "ğŸ”§  \n",
    "- `df.corr()` ğŸ”— â€“ Correlation matrix  \n",
    "- `df['target'].value_counts()` ğŸ§® â€“ Class balance check  \n",
    "- `df.groupby('label').mean()` ğŸ“ â€“ Stats by group  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” 7. Model Evaluation Helpers  \n",
    "ğŸ“Œ Analyze results after predictions  \n",
    "ğŸ”§  \n",
    "- `df['pred_error'] = df['y_true'] - df['y_pred']` ğŸ§¾ â€“ Error column  \n",
    "- `df['correct'] = df['y_true'] == df['y_pred']` âœ… â€“ Boolean match  \n",
    "- `df['prob'] = model.predict_proba(X)[:,1]` ğŸ“ˆ â€“ Probabilities  \n",
    "- `df['conf'] = np.max(model.predict_proba(X), axis=1)` ğŸ§ª â€“ Confidence scores  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“¦ 8. Export for Modeling  \n",
    "ğŸ“Œ Save cleaned/preprocessed data  \n",
    "ğŸ”§  \n",
    "- `df.to_csv('cleaned_data.csv', index=False)` ğŸ’¾  \n",
    "- `df.to_pickle('df.pkl')` ğŸ§º â€“ Save with types  \n",
    "- `df.to_parquet('df.parquet')` ğŸš€ â€“ For big data ML  \n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ Extra Tricks for ML Pipelines  \n",
    "ğŸ“Œ Helpful utilities used in real ML workflows  \n",
    "ğŸ”§  \n",
    "- `df.select_dtypes(include='number')` ğŸ”¢ â€“ Numeric columns only  \n",
    "- `df.columns[df.isnull().any()]` ğŸš¨ â€“ Columns with NaNs  \n",
    "- `df['cat'] = df['cat'].astype('category')` ğŸ§¬ â€“ Convert to categorical  \n",
    "- `df.sort_values(by='importance', ascending=False)` ğŸ“Œ â€“ Feature importance  \n",
    "\n",
    "---\n",
    "\n",
    "# **ğŸ¤–Scikit-learn + Pandas Integration Cheatsheet**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“‹ 1. Prepare Features & Target  \n",
    "ğŸ“Œ Slice your DataFrame into X (features) and y (target)  \n",
    "```python\n",
    "X = df.drop('target', axis=1)  # ğŸ¯ Features\n",
    "y = df['target']               # ğŸ· Target\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### âœ‚ï¸ 2. Train-Test Split  \n",
    "ğŸ“Œ Split DataFrame for training and testing  \n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§¼ 3. Use Pandas with Scalers  \n",
    "ğŸ“Œ Keep column names even after scaling  \n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X),\n",
    "    columns=X.columns\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ— 4. Build Pipelines with Pandas  \n",
    "ğŸ“Œ Combine preprocessing + modeling (safe with DataFrames!)  \n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§± 5. ColumnTransformer with Pandas  \n",
    "ğŸ“Œ Apply different transforms to numeric vs categorical columns  \n",
    "```python\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "numeric_cols = X.select_dtypes(include='number').columns\n",
    "cat_cols = X.select_dtypes(include='object').columns\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numeric_cols),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
    "])\n",
    "\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('model', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "full_pipeline.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ˆ 6. Predictions with Pandas Index  \n",
    "ğŸ“Œ Return predictions with original row index  \n",
    "```python\n",
    "y_pred = pd.Series(pipeline.predict(X_test), index=X_test.index)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§ª 7. Cross-Validation with DataFrame  \n",
    "ğŸ“Œ No need to convert to arrays â€“ it just works  \n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(pipeline, X, y, cv=5)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§¾ 8. Feature Importances with Columns  \n",
    "ğŸ“Œ Get feature importance + original column names  \n",
    "```python\n",
    "model = pipeline.named_steps['model']\n",
    "features = preprocessor.get_feature_names_out()\n",
    "pd.Series(model.feature_importances_, index=features).sort_values(ascending=False)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  9. Export Model & DataFrame Together  \n",
    "ğŸ“Œ Save full pipeline (with preprocessing)  \n",
    "```python\n",
    "import joblib\n",
    "joblib.dump(full_pipeline, 'model_pipeline.pkl')  # ğŸ’¾ Save full model\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… 10. Clean Predictions with Original Data  \n",
    "ğŸ“Œ Combine predictions with the original DataFrame  \n",
    "```python\n",
    "df_result = df.copy()\n",
    "df_result['pred'] = pipeline.predict(X)\n",
    "```\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
