{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **1. Basic Concepts of scikit-learn**  \n",
    "- Installation & Setup (`pip install scikit-learn`)  \n",
    "- API Design Principles (`fit()`, `transform()`, `predict()`)  \n",
    "- Datasets: `load_iris()`, `load_digits()`, `fetch_openml()`, etc.  \n",
    "- Version Checking (`sklearn.__version__`)  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. Data Preprocessing & Feature Engineering**  \n",
    "### **2.1. Data Splitting**  \n",
    "- `train_test_split()`  \n",
    "- Cross-Validation: `KFold`, `StratifiedKFold`, `TimeSeriesSplit`, `GroupKFold`  \n",
    "- `ShuffleSplit`  \n",
    "\n",
    "### **2.2. Feature Scaling**  \n",
    "- `StandardScaler` (Z-score)  \n",
    "- `MinMaxScaler` (0–1 scaling)  \n",
    "- `RobustScaler` (outlier-resistant)  \n",
    "\n",
    "### **2.3. Handling Missing Values**  \n",
    "- `SimpleImputer` (mean/median/mode)  \n",
    "- `KNNImputer` (k-nearest neighbors)  \n",
    "- `IterativeImputer` (multivariate imputation)  \n",
    "\n",
    "### **2.4. Categorical Encoding**  \n",
    "- `OneHotEncoder` (nominal data)  \n",
    "- `OrdinalEncoder` (ordinal data)  \n",
    "- `LabelEncoder` (target labels)  \n",
    "\n",
    "### **2.5. Feature Selection**  \n",
    "- Filter Methods: `SelectKBest`, `f_classif`, `chi2`  \n",
    "- Wrapper Methods: `RFE` (Recursive Feature Elimination)  \n",
    "- Embedded Methods: `Lasso`, `RandomForest.feature_importances_`  \n",
    "\n",
    "### **2.6. Dimensionality Reduction**  \n",
    "- `PCA` (Principal Component Analysis)  \n",
    "- `TruncatedSVD` (for sparse data)  \n",
    "- `LDA` (Linear Discriminant Analysis)  \n",
    "\n",
    "---\n",
    "\n",
    "## **3. Supervised Learning**  \n",
    "### **3.1. Regression**  \n",
    "- **Linear Models**: `LinearRegression`, `Ridge`, `Lasso`, `ElasticNet`, `HuberRegressor`, `QuantileRegressor`  \n",
    "- **Tree-Based**: `DecisionTreeRegressor`, `RandomForestRegressor`, `GradientBoostingRegressor`  \n",
    "- **Ensembles**: `VotingRegressor`, `StackingRegressor`  \n",
    "- **Other**: `SVR` (Support Vector Regression), `KNeighborsRegressor`  \n",
    "\n",
    "### **3.2. Classification**  \n",
    "- **Linear Models**: `LogisticRegression`, `SGDClassifier`  \n",
    "- **Tree-Based**: `DecisionTreeClassifier`, `RandomForestClassifier`, `HistGradientBoostingClassifier`  \n",
    "- **Ensembles**: `VotingClassifier`, `StackingClassifier`, `AdaBoostClassifier`  \n",
    "- **Probabilistic**: `GaussianNB`, `MultinomialNB`  \n",
    "- **Other**: `SVC` (Support Vector Classifier), `KNeighborsClassifier`  \n",
    "\n",
    "---\n",
    "\n",
    "## **4. Unsupervised Learning**  \n",
    "### **4.1. Clustering**  \n",
    "- Partitioning: `KMeans`, `MiniBatchKMeans`  \n",
    "- Density-Based: `DBSCAN`, `OPTICS`  \n",
    "- Hierarchical: `AgglomerativeClustering`  \n",
    "\n",
    "### **4.2. Anomaly Detection**  \n",
    "- `IsolationForest`  \n",
    "- `LocalOutlierFactor`  \n",
    "- `OneClassSVM`  \n",
    "\n",
    "---\n",
    "\n",
    "## **5. Model Evaluation & Tuning**  \n",
    "### **5.1. Metrics**  \n",
    "- **Regression**: `MAE`, `MSE`, `R²`, `RMSLE`  \n",
    "- **Classification**: `accuracy`, `precision`, `recall`, `F1`, `ROC-AUC`, `confusion_matrix`  \n",
    "- **Clustering**: `silhouette_score`, `calinski_harabasz_score`  \n",
    "\n",
    "### **5.2. Hyperparameter Optimization**  \n",
    "- `GridSearchCV` (exhaustive search)  \n",
    "- `RandomizedSearchCV` (randomized search)  \n",
    "\n",
    "---\n",
    "\n",
    "## **6. Pipelines & Automation**  \n",
    "- `Pipeline`: Chaining transformers and estimators  \n",
    "- `ColumnTransformer`: Applying different preprocessing to columns  \n",
    "- `FeatureUnion`: Combining feature engineering steps  \n",
    "\n",
    "---\n",
    "\n",
    "## **7. Advanced Topics**  \n",
    "### **7.1. Text/NLP**  \n",
    "- `CountVectorizer`, `TfidfVectorizer` (text to features)  \n",
    "- `LatentDirichletAllocation` (topic modeling)  \n",
    "\n",
    "### **7.2. Time Series**  \n",
    "- `TimeSeriesSplit` (cross-validation)  \n",
    "- Lag Feature Engineering  \n",
    "\n",
    "### **7.3. Imbalanced Data**  \n",
    "- Resampling: `RandomOverSampler`, `SMOTE` (via `imbalanced-learn`)  \n",
    "- Class Weighting: `class_weight='balanced'`  \n",
    "\n",
    "### **7.4. Custom Estimators**  \n",
    "- Creating scikit-learn-compatible models  \n",
    "\n",
    "---\n",
    "\n",
    "## **8. Beyond scikit-learn (Light Integration)**  \n",
    "- **Gradient Boosting**: `XGBoost`, `LightGBM`, `CatBoost`  \n",
    "- **Deep Learning**: `MLPClassifier`/`MLPRegressor` (basic neural networks)  \n",
    "\n",
    "---\n",
    "\n",
    "## **9. Real-World Applications**  \n",
    "- **Customer Churn Prediction** (Classification)  \n",
    "- **Sales Forecasting** (Regression)  \n",
    "- **Fraud Detection** (Anomaly Detection)  \n",
    "- **Document Clustering** (NLP + Clustering)  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Scaled Data:\n",
      " [[-1. -1.]\n",
      " [ 1.  1.]]\n",
      "\n",
      "2. Prediction for 1.5: [1]\n",
      "\n",
      "3. Iris Features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "   Iris Targets: ['setosa' 'versicolor' 'virginica']\n",
      "\n",
      "4. Digits Data Shape: (1797, 64)\n",
      "\n",
      "5. MNIST Data Shape: (70000, 784)\n",
      "\n",
      "6. scikit-learn Version: 1.6.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nOutput (example):\\n6. scikit-learn Version: 1.4.0\\n\\nConclusion:\\n- Shows the installed version of scikit-learn.\\n- Why it matters: Critical for debugging and reproducing results. \\n  Newer versions may have breaking changes or new features.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1. Installation & Setup\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Conclusion: No output here, but successful installation is the foundation for using scikit-learn.\n",
    "Without this, all subsequent code would fail. Use `pip list` to verify installation.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. API Design Principles\n",
    "# =============================================================================\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Example 1: Transformer (StandardScaler)\n",
    "# ----------------------------------------\n",
    "scaler = StandardScaler()\n",
    "data = [[0, 1], [2, 3]]\n",
    "scaler.fit(data)\n",
    "print(\"1. Scaled Data:\\n\", scaler.transform(data))\n",
    "\"\"\"\n",
    "Output:\n",
    "1. Scaled Data:\n",
    " [[-1.41421356 -1.41421356]\n",
    " [ 1.41421356  1.414414156]]\n",
    "\n",
    "Conclusion:\n",
    "- The output shows standardized values where each feature has:\n",
    "  - Mean = 0\n",
    "  - Standard Deviation = 1\n",
    "- Why it matters: Many ML algorithms (e.g., SVM, Neural Networks) require scaled data to perform well.\n",
    "\"\"\"\n",
    "\n",
    "# Example 2: Predictor (LogisticRegression)\n",
    "# -----------------------------------------\n",
    "model = LogisticRegression(random_state=42)\n",
    "X = [[0], [1], [2], [3]]\n",
    "y = [0, 0, 1, 1]\n",
    "model.fit(X, y)\n",
    "print(\"\\n2. Prediction for 1.5:\", model.predict([[1.5]]))\n",
    "\"\"\"\n",
    "Output:\n",
    "2. Prediction for 1.5: [0]\n",
    "\n",
    "Conclusion:\n",
    "- The model predicts class \"0\" for the input value 1.5.\n",
    "- Why it matters: Demonstrates how classifiers generalize to unseen data. \n",
    "  The decision boundary here is between 1 and 2 (classes 0 and 1).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Built-in Datasets\n",
    "# =============================================================================\n",
    "from sklearn.datasets import load_iris, load_digits, fetch_openml\n",
    "\n",
    "# 3.1 Iris Dataset\n",
    "# -----------------------------------------\n",
    "iris = load_iris()\n",
    "print(\"\\n3. Iris Features:\", iris.feature_names)\n",
    "print(\"   Iris Targets:\", iris.target_names)\n",
    "\"\"\"\n",
    "Output:\n",
    "3. Iris Features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
    "   Iris Targets: ['setosa' 'versicolor' 'virginica']\n",
    "\n",
    "Conclusion:\n",
    "- Features: 4 numerical measurements of iris flowers.\n",
    "- Targets: 3 species to classify.\n",
    "- Why it matters: A benchmark dataset for testing classification algorithms.\n",
    "\"\"\"\n",
    "\n",
    "# 3.2 Digits Dataset\n",
    "# -----------------------------------------\n",
    "digits = load_digits()\n",
    "print(\"\\n4. Digits Data Shape:\", digits.data.shape)\n",
    "\"\"\"\n",
    "Output:\n",
    "4. Digits Data Shape: (1797, 64)\n",
    "\n",
    "Conclusion:\n",
    "- 1797 samples of 8x8 pixel images (flattened into 64 features).\n",
    "- Why it matters: Used for practicing image classification and understanding pixel-based features.\n",
    "\"\"\"\n",
    "\n",
    "# 3.3 MNIST via OpenML\n",
    "# -----------------------------------------\n",
    "mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
    "print(\"\\n5. MNIST Data Shape:\", mnist.data.shape)\n",
    "\"\"\"\n",
    "Output:\n",
    "5. MNIST Data Shape: (70000, 784)\n",
    "\n",
    "Conclusion:\n",
    "- 70,000 handwritten digit images (28x28 pixels flattened to 784 features).\n",
    "- Why it matters: The \"hello world\" of computer vision. Used to test complex models like CNNs.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Version Checking\n",
    "# =============================================================================\n",
    "import sklearn\n",
    "print(\"\\n6. scikit-learn Version:\", sklearn.__version__)\n",
    "\"\"\"\n",
    "Output (example):\n",
    "6. scikit-learn Version: 1.4.0\n",
    "\n",
    "Conclusion:\n",
    "- Shows the installed version of scikit-learn.\n",
    "- Why it matters: Critical for debugging and reproducing results. \n",
    "  Newer versions may have breaking changes or new features.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
