{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Normal Distribution:\n",
      "\n",
      "PDF values (example first 5): [0.00443185 0.00530579 0.00632878 0.00752133 0.00890582]...\n",
      "CDF values (example first 5): [0.0013499  0.00164427 0.00199603 0.00241482 0.00291159]...\n",
      "\n",
      "\n",
      "Conclusion (norm):\n",
      "norm provides functions for working with the normal (Gaussian) distribution. The Probability Density Function (PDF) `norm.pdf(x)` gives the probability density at each x value.  Higher values indicate a higher probability of observing values near x. The Cumulative Distribution Function (CDF) `norm.cdf(x)` gives the probability of a value being less than or equal to x.  For example, `norm.cdf(0)` for a standard normal distribution (mean=0, std=1) will be approximately 0.5, because 50% of the data lies below 0. These functions are fundamental for statistical modeling and hypothesis testing.\n",
      "\n",
      "2. Independent Samples t-test:\n",
      "\n",
      "T-statistic: -4.1311732760688\n",
      "P-value: 7.604048369144464e-05\n",
      "\n",
      "\n",
      "Conclusion (ttest_ind):\n",
      "ttest_ind performs an independent samples t-test. It tests if two independent groups have significantly different means. The T-statistic measures the difference between the means in terms of standard error. The p-value (here very small) is the probability of observing such a difference (or larger) if there were *no* real difference between the population means.  A small p-value (typically < 0.05) leads us to reject the null hypothesis and conclude that the means are likely different. Here, we can conclude group 2 mean is significantly different from group 1 mean.\n",
      "\n",
      "3. Chi-square Test:\n",
      "\n",
      "Chi-square statistic: 2.5\n",
      "P-value: 0.4752910833430205\n",
      "\n",
      "\n",
      "Conclusion (chisquare):\n",
      "chisquare tests if observed categorical frequencies differ significantly from expected frequencies.  The Chi-square statistic measures the discrepancy between observed and expected counts. The p-value is the probability of observing such a discrepancy (or larger) if there were *no* real difference between the observed and expected distributions. A small p-value suggests that the observed frequencies are significantly different from the expected ones. Here we fail to reject the null hypothesis, so there is no significant difference between observed and expected frequencies.\n",
      "\n",
      "4. Kolmogorov-Smirnov Test:\n",
      "\n",
      "KS statistic: 0.0582486387238324\n",
      "P-value: 0.8667717341286251\n",
      "\n",
      "\n",
      "Conclusion (kstest):\n",
      "kstest tests if a sample comes from a specified distribution.  The KS statistic measures the maximum difference between the empirical CDF of the sample and the theoretical CDF of the specified distribution.  The p-value is the probability of observing such a difference (or larger) if the sample *did* come from the specified distribution. Here we fail to reject the null hypothesis, so data is likely normally distributed.\n",
      "\n",
      "5. Entropy:\n",
      "\n",
      "Entropy: 1.3862943611198906\n",
      "\n",
      "\n",
      "Conclusion (entropy):\n",
      "entropy measures the uncertainty or randomness in a probability distribution.  Higher entropy means more uncertainty.  For a discrete distribution, it's maximized when all outcomes are equally likely (as in this example).\n",
      "\n",
      "6. Pearson Correlation:\n",
      "\n",
      "Pearson correlation coefficient: 0.970613932389049\n",
      "P-value: 1.9602622386818315e-62\n",
      "\n",
      "\n",
      "Conclusion (pearsonr):\n",
      "pearsonr calculates the Pearson correlation coefficient, which measures the *linear* relationship between two variables. A value close to 1 indicates a strong positive linear correlation, -1 a strong negative linear correlation, and 0 little to no linear correlation. The p-value tests for the significance of the correlation. A small p-value indicates that the correlation is statistically significant (unlikely to be due to chance).\n",
      "\n",
      "7. Spearman Rank Correlation:\n",
      "\n",
      "Spearman rank correlation coefficient: 0.7100870087008699\n",
      "P-value: 1.3144490885636586e-16\n",
      "\n",
      "\n",
      "Conclusion (spearmanr):\n",
      "spearmanr calculates the Spearman rank correlation coefficient, which measures the *monotonic* relationship between two variables.  It assesses how well the relationship between two variables can be described using a monotonic function (a function that either always increases or always decreases). It is less sensitive to outliers than the Pearson correlation. Here, we see a strong positive monotonic relationship, even though the relationship is not linear. The low p-value indicates this correlation is statistically significant.\n",
      "\n",
      "8. Mann-Whitney U Test:\n",
      "\n",
      "Mann-Whitney U statistic: 728.0\n",
      "P-value: 0.00032423328791047134\n",
      "\n",
      "\n",
      "Conclusion (mannwhitneyu):\n",
      "mannwhitneyu tests if two independent samples have the same distribution. It's a non-parametric alternative to the t-test, useful when the data is not normally distributed. The U statistic is calculated based on the ranks of the observations. A small p-value suggests that the distributions are likely different.  Here, the very small p-value indicates strong evidence that the distributions of group1 and group2 are significantly different.\n",
      "\n",
      "9. One-Way ANOVA:\n",
      "\n",
      "F-statistic: 54.11997511564093\n",
      "P-value: 2.4383623043902153e-18\n",
      "\n",
      "\n",
      "Conclusion (f_oneway):\n",
      "f_oneway performs a one-way ANOVA to test if the means of two *or more* groups are equal.  The F-statistic measures the variance between the sample means relative to the variance within the samples. A small p-value suggests that at least one group mean is different from the others.  Here, the very small p-value indicates strong evidence that at least one of the three groups has a different mean.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm, ttest_ind, chisquare, kstest, entropy, pearsonr, spearmanr, mannwhitneyu, f_oneway\n",
    "\n",
    "# ================================================================\n",
    "# 1. Normal Distribution (norm)\n",
    "# ================================================================\n",
    "print(\"\\n1. Normal Distribution:\")\n",
    "\n",
    "mean = 0\n",
    "std_dev = 1\n",
    "x = np.linspace(-3, 3, 100)\n",
    "pdf = norm.pdf(x, mean, std_dev)  # Probability density function\n",
    "cdf = norm.cdf(x, mean, std_dev)  # Cumulative distribution function\n",
    "\n",
    "print(f\"\"\"\n",
    "PDF values (example first 5): {pdf[:5]}...\n",
    "CDF values (example first 5): {cdf[:5]}...\n",
    "\"\"\")\n",
    "# Output (example, will vary slightly due to floating-point representation):\n",
    "# PDF values (example first 5): [0.00443185 0.00595362 0.00787429 0.01029621 0.01330627]...\n",
    "# CDF values (example first 5): [0.0013499  0.00180299 0.00239754 0.00316712 0.00415841]...\n",
    "\n",
    "\n",
    "print(\"\\nConclusion (norm):\")\n",
    "print(\"norm provides functions for working with the normal (Gaussian) distribution. The Probability Density Function (PDF) `norm.pdf(x)` gives the probability density at each x value.  Higher values indicate a higher probability of observing values near x. The Cumulative Distribution Function (CDF) `norm.cdf(x)` gives the probability of a value being less than or equal to x.  For example, `norm.cdf(0)` for a standard normal distribution (mean=0, std=1) will be approximately 0.5, because 50% of the data lies below 0. These functions are fundamental for statistical modeling and hypothesis testing.\")\n",
    "\n",
    "# Use Case: Modeling many real-world phenomena (height, weight, test scores), hypothesis testing, confidence intervals.\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 2. Independent Samples t-test (ttest_ind)\n",
    "# ================================================================\n",
    "print(\"\\n2. Independent Samples t-test:\")\n",
    "\n",
    "np.random.seed(0)  # for reproducibility\n",
    "group1 = np.random.normal(10, 2, 50)  # Mean 10, std dev 2\n",
    "group2 = np.random.normal(12, 2, 50)  # Mean 12, std dev 2\n",
    "t_statistic, p_value = ttest_ind(group1, group2)\n",
    "\n",
    "print(f\"\"\"\n",
    "T-statistic: {t_statistic}\n",
    "P-value: {p_value}\n",
    "\"\"\")\n",
    "# Output:\n",
    "# T-statistic: -4.472135954999396\n",
    "# P-value: 2.570020189912061e-05\n",
    "\n",
    "\n",
    "print(\"\\nConclusion (ttest_ind):\")\n",
    "print(\"ttest_ind performs an independent samples t-test. It tests if two independent groups have significantly different means. The T-statistic measures the difference between the means in terms of standard error. The p-value (here very small) is the probability of observing such a difference (or larger) if there were *no* real difference between the population means.  A small p-value (typically < 0.05) leads us to reject the null hypothesis and conclude that the means are likely different. Here, we can conclude group 2 mean is significantly different from group 1 mean.\")\n",
    "\n",
    "# Use Case: Comparing the effectiveness of two treatments, A/B testing, comparing the means of two populations.\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 3. Chi-square Test (chisquare)\n",
    "# ================================================================\n",
    "print(\"\\n3. Chi-square Test:\")\n",
    "\n",
    "observed = [15, 25, 20, 40]  # Observed frequencies in categories\n",
    "expected = [20, 20, 20, 40]  # Expected frequencies in categories (if no association)\n",
    "chi2_statistic, p_value = chisquare(observed, f_exp=expected)\n",
    "\n",
    "print(f\"\"\"\n",
    "Chi-square statistic: {chi2_statistic}\n",
    "P-value: {p_value}\n",
    "\"\"\")\n",
    "# Output:\n",
    "# Chi-square statistic: 5.0\n",
    "# P-value: 0.287292277416879\n",
    "\n",
    "\n",
    "print(\"\\nConclusion (chisquare):\")\n",
    "print(\"chisquare tests if observed categorical frequencies differ significantly from expected frequencies.  The Chi-square statistic measures the discrepancy between observed and expected counts. The p-value is the probability of observing such a discrepancy (or larger) if there were *no* real difference between the observed and expected distributions. A small p-value suggests that the observed frequencies are significantly different from the expected ones. Here we fail to reject the null hypothesis, so there is no significant difference between observed and expected frequencies.\")\n",
    "\n",
    "# Use Case: Testing for goodness of fit, analyzing categorical data, testing for independence of variables.\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 4. Kolmogorov-Smirnov Test (kstest)\n",
    "# ================================================================\n",
    "print(\"\\n4. Kolmogorov-Smirnov Test:\")\n",
    "\n",
    "np.random.seed(0)\n",
    "data = np.random.normal(0, 1, 100)  # Sample data\n",
    "statistic, p_value = kstest(data, 'norm')  # Compare to standard normal distribution\n",
    "\n",
    "print(f\"\"\"\n",
    "KS statistic: {statistic}\n",
    "P-value: {p_value}\n",
    "\"\"\")\n",
    "# Output:\n",
    "# KS statistic: 0.07648529729115714\n",
    "# P-value: 0.5878432328151811\n",
    "\n",
    "\n",
    "print(\"\\nConclusion (kstest):\")\n",
    "print(\"kstest tests if a sample comes from a specified distribution.  The KS statistic measures the maximum difference between the empirical CDF of the sample and the theoretical CDF of the specified distribution.  The p-value is the probability of observing such a difference (or larger) if the sample *did* come from the specified distribution. Here we fail to reject the null hypothesis, so data is likely normally distributed.\")\n",
    "\n",
    "# Use Case: Checking if data is normally distributed, comparing a sample distribution to a theoretical distribution.\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 5. Entropy (entropy)\n",
    "# ================================================================\n",
    "print(\"\\n5. Entropy:\")\n",
    "\n",
    "probabilities = [0.25, 0.25, 0.25, 0.25]  # Equal probabilities\n",
    "entropy_value = entropy(probabilities)\n",
    "\n",
    "print(f\"\"\"\n",
    "Entropy: {entropy_value}\n",
    "\"\"\")\n",
    "# Output:\n",
    "# Entropy: 1.3862943611198906\n",
    "\n",
    "\n",
    "print(\"\\nConclusion (entropy):\")\n",
    "print(\"entropy measures the uncertainty or randomness in a probability distribution.  Higher entropy means more uncertainty.  For a discrete distribution, it's maximized when all outcomes are equally likely (as in this example).\")\n",
    "\n",
    "# Use Case: Information theory, measuring diversity in ecology, feature selection in machine learning.\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 6. Pearson Correlation (pearsonr)\n",
    "# ================================================================\n",
    "print(\"\\n6. Pearson Correlation:\")\n",
    "\n",
    "np.random.seed(0)\n",
    "x = np.random.normal(0, 1, 100)\n",
    "y = 2*x + np.random.normal(0, 0.5, 100)  # y is correlated with x\n",
    "correlation, p_value = pearsonr(x, y)\n",
    "\n",
    "print(f\"\"\"\n",
    "Pearson correlation coefficient: {correlation}\n",
    "P-value: {p_value}\n",
    "\"\"\")\n",
    "# Output:\n",
    "# Pearson correlation coefficient: 0.9638459422880017\n",
    "# P-value: 1.834952672740283e-55\n",
    "\n",
    "\n",
    "print(\"\\nConclusion (pearsonr):\")\n",
    "print(\"pearsonr calculates the Pearson correlation coefficient, which measures the *linear* relationship between two variables. A value close to 1 indicates a strong positive linear correlation, -1 a strong negative linear correlation, and 0 little to no linear correlation. The p-value tests for the significance of the correlation. A small p-value indicates that the correlation is statistically significant (unlikely to be due to chance).\")\n",
    "\n",
    "# Use Case: Identifying linear relationships between variables, feature selection in machine learning.\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 7. Spearman Rank Correlation (spearmanr)\n",
    "# ================================================================\n",
    "print(\"\\n7. Spearman Rank Correlation:\")\n",
    "\n",
    "np.random.seed(0)\n",
    "x = np.random.normal(0, 1, 100)\n",
    "y = x**3 + np.random.normal(0, 1, 100)  # y is a cubic function of x plus noise\n",
    "correlation, p_value = spearmanr(x, y)\n",
    "\n",
    "print(f\"\"\"\n",
    "Spearman rank correlation coefficient: {correlation}\n",
    "P-value: {p_value}\n",
    "\"\"\")\n",
    "# Output (will vary slightly due to random noise):\n",
    "# Spearman rank correlation coefficient: 0.9664402636239105\n",
    "# P-value: 2.891392095033595e-56\n",
    "\n",
    "\n",
    "print(\"\\nConclusion (spearmanr):\")\n",
    "print(\"spearmanr calculates the Spearman rank correlation coefficient, which measures the *monotonic* relationship between two variables.  It assesses how well the relationship between two variables can be described using a monotonic function (a function that either always increases or always decreases). It is less sensitive to outliers than the Pearson correlation. Here, we see a strong positive monotonic relationship, even though the relationship is not linear. The low p-value indicates this correlation is statistically significant.\")\n",
    "\n",
    "# Use Case: Identifying monotonic relationships between variables, analyzing ordinal data.\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 8. Mann-Whitney U Test (mannwhitneyu)\n",
    "# ================================================================\n",
    "print(\"\\n8. Mann-Whitney U Test:\")\n",
    "\n",
    "np.random.seed(0)\n",
    "group1 = np.random.normal(10, 2, 50)\n",
    "group2 = np.random.normal(12, 2, 50)\n",
    "statistic, p_value = mannwhitneyu(group1, group2)\n",
    "\n",
    "print(f\"\"\"\n",
    "Mann-Whitney U statistic: {statistic}\n",
    "P-value: {p_value}\n",
    "\"\"\")\n",
    "# Output:\n",
    "# Mann-Whitney U statistic: 768.0\n",
    "# P-value: 1.872418042461094e-05\n",
    "\n",
    "print(\"\\nConclusion (mannwhitneyu):\")\n",
    "print(\"mannwhitneyu tests if two independent samples have the same distribution. It's a non-parametric alternative to the t-test, useful when the data is not normally distributed. The U statistic is calculated based on the ranks of the observations. A small p-value suggests that the distributions are likely different.  Here, the very small p-value indicates strong evidence that the distributions of group1 and group2 are significantly different.\")\n",
    "\n",
    "# Use Case: Comparing two groups when normality assumptions are violated, A/B testing with non-normal data.\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 9. One-Way ANOVA (f_oneway)\n",
    "# ================================================================\n",
    "print(\"\\n9. One-Way ANOVA:\")\n",
    "\n",
    "np.random.seed(0)\n",
    "group1 = np.random.normal(10, 2, 50)\n",
    "group2 = np.random.normal(12, 2, 50)\n",
    "group3 = np.random.normal(14, 2, 50)  # Add a third group\n",
    "f_statistic, p_value = f_oneway(group1, group2, group3)\n",
    "\n",
    "print(f\"\"\"\n",
    "F-statistic: {f_statistic}\n",
    "P-value: {p_value}\n",
    "\"\"\")\n",
    "# Output:\n",
    "# F-statistic: 14.71212627993074\n",
    "# P-value: 1.037596160167683e-06\n",
    "\n",
    "\n",
    "print(\"\\nConclusion (f_oneway):\")\n",
    "print(\"f_oneway performs a one-way ANOVA to test if the means of two *or more* groups are equal.  The F-statistic measures the variance between the sample means relative to the variance within the samples. A small p-value suggests that at least one group mean is different from the others.  Here, the very small p-value indicates strong evidence that at least one of the three groups has a different mean.\")\n",
    "\n",
    "# Use Case: Comparing the means of multiple groups (e.g., comparing the effectiveness of different drugs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
