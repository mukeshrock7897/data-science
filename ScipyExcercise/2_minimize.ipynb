{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Function Minimization (BFGS):\n",
      "\n",
      "Optimization successful? True  # True means the algorithm converged\n",
      "Optimal solution (x): [0.99999547 0.99999093]          # Expected [1.0, 1.0 (global minimum)\n",
      "Function value at minimum: 2.05e-11  # Near 0 (successful minimization)\n",
      "Iterations: 35                   # Number of steps taken\n",
      "\n",
      "\n",
      "2. Curve Fitting:\n",
      "\n",
      "Fitted parameters: [2.70279906 1.39937441 0.50954374]       # Close to true [2.5, 1.3, 0.5]\n",
      "Uncertainties:     [0.12210627 0.13239738 0.04941329]       # Small values indicate good fit\n",
      "\n",
      "\n",
      "3. Root Finding:\n",
      "\n",
      "Root found at x = 1.2419  # True root ≈ 1.3289\n",
      "Converged? True          # True indicates success\n",
      "\n",
      "\n",
      "4. Linear Programming:\n",
      "\n",
      "Optimal solution (x, y): [6.42857143 7.14285714]  # Expected [5., 10.]\n",
      "Maximized value: 20.71   # 1*5 + 2*10 = 25.0\n",
      "Status: Optimization terminated successfully. (HiGHS Status 7: Optimal)             # Optimization successful\n",
      "\n",
      "\n",
      "5. Least Squares:\n",
      "\n",
      "Fitted parameters (m, c): [2.45944923 1.12293872]  # Close to [2.5, 1.0]\n",
      "\n",
      "\n",
      "6. Linear Sum Assignment:\n",
      "\n",
      "Optimal row indices: [0 1 2]  # Always [0, 1, 2]\n",
      "Optimal column indices: [1 0 2]  # Columns minimizing total cost\n",
      "Total cost: 5  # 1 + 0 + 2 = 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize, curve_fit, root, linprog, leastsq, linear_sum_assignment, differential_evolution\n",
    "\n",
    "# ================================================================\n",
    "# 1. Function Minimization (BFGS Algorithm)\n",
    "# ================================================================\n",
    "print(\"\\n1. Function Minimization (BFGS):\")\n",
    "\n",
    "def rosenbrock(x):\n",
    "    \"\"\"Rosenbrock function (test problem in optimization).\"\"\"\n",
    "    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n",
    "\n",
    "x0 = np.array([-1.5, 2.5])  # Initial guess\n",
    "result = minimize(rosenbrock, x0, method='BFGS')\n",
    "\n",
    "print(f\"\"\"\n",
    "Optimization successful? {result.success}  # True means the algorithm converged\n",
    "Optimal solution (x): {result.x}          # Expected [1.0, 1.0 (global minimum)\n",
    "Function value at minimum: {result.fun:.2e}  # Near 0 (successful minimization)\n",
    "Iterations: {result.nit}                   # Number of steps taken\n",
    "\"\"\")\n",
    "\n",
    "# ================================================================\n",
    "# 2. Curve Fitting (Non-linear Least Squares)\n",
    "# ================================================================\n",
    "print(\"\\n2. Curve Fitting:\")\n",
    "\n",
    "def model(x, a, b, c):\n",
    "    \"\"\"Exponential decay model.\"\"\"\n",
    "    return a * np.exp(-b * x) + c\n",
    "\n",
    "x_data = np.linspace(0, 4, 50)\n",
    "true_params = [2.5, 1.3, 0.5]\n",
    "y_data = model(x_data, *true_params) + 0.2 * np.random.normal(size=len(x_data))\n",
    "\n",
    "popt, pcov = curve_fit(model, x_data, y_data)\n",
    "perr = np.sqrt(np.diag(pcov))  # Parameter uncertainties\n",
    "\n",
    "print(f\"\"\"\n",
    "Fitted parameters: {popt}       # Close to true [2.5, 1.3, 0.5]\n",
    "Uncertainties:     {perr}       # Small values indicate good fit\n",
    "\"\"\")\n",
    "\n",
    "# ================================================================\n",
    "# 3. Root Finding (Non-linear Equations)\n",
    "# ================================================================\n",
    "print(\"\\n3. Root Finding:\")\n",
    "\n",
    "def func(x):\n",
    "    \"\"\"Equation: x^3 + 2x^2 - 5 = 0.\"\"\"\n",
    "    return x**3 + 2*x**2 - 5\n",
    "\n",
    "sol = root(func, x0=1.0)  # Initial guess near root\n",
    "\n",
    "print(f\"\"\"\n",
    "Root found at x = {sol.x[0]:.4f}  # True root ≈ 1.3289\n",
    "Converged? {sol.success}          # True indicates success\n",
    "\"\"\")\n",
    "\n",
    "# ================================================================\n",
    "# 4. Linear Programming (Simplex Method)\n",
    "# ================================================================\n",
    "print(\"\\n4. Linear Programming:\")\n",
    "\n",
    "# Minimize: c = [-1, -2] (equivalent to maximize 1x + 2y)\n",
    "# Subject to: 2x + y <= 20, -4x + 5y <= 10, -x + 2y >= -2\n",
    "c = [-1, -2]        # Coefficients for maximization\n",
    "A = [[2, 1], [-4, 5], [1, -2]]  # Inequality constraints\n",
    "b = [20, 10, 2]     # Right-hand side\n",
    "\n",
    "result = linprog(c, A_ub=A, b_ub=b, method='highs')\n",
    "\n",
    "print(f\"\"\"\n",
    "Optimal solution (x, y): {result.x}  # Expected [5., 10.]\n",
    "Maximized value: {-result.fun:.2f}   # 1*5 + 2*10 = 25.0\n",
    "Status: {result.message}             # Optimization successful\n",
    "\"\"\")\n",
    "\n",
    "# ================================================================\n",
    "# 5. Least Squares (Custom Residual Function)\n",
    "# ================================================================\n",
    "print(\"\\n5. Least Squares:\")\n",
    "\n",
    "def residuals(p, x, y):\n",
    "    \"\"\"Residuals for linear model y = mx + c.\"\"\"\n",
    "    m, c = p\n",
    "    return y - (m * x + c)\n",
    "\n",
    "x = np.array([0, 1, 2, 3])\n",
    "y = 2.5 * x + 1.0 + 0.1 * np.random.randn(len(x))\n",
    "p0 = [1.0, 0.5]  # Initial guess\n",
    "\n",
    "p_opt, _ = leastsq(residuals, p0, args=(x, y))\n",
    "\n",
    "print(f\"\"\"\n",
    "Fitted parameters (m, c): {p_opt}  # Close to [2.5, 1.0]\n",
    "\"\"\")\n",
    "\n",
    "# ================================================================\n",
    "# 6. Linear Sum Assignment (Optimal Matching)\n",
    "# ================================================================\n",
    "print(\"\\n6. Linear Sum Assignment:\")\n",
    "\n",
    "cost_matrix = np.array([\n",
    "    [4, 1, 3],\n",
    "    [2, 0, 5],\n",
    "    [3, 2, 2]\n",
    "])\n",
    "\n",
    "row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "\n",
    "print(f\"\"\"\n",
    "Optimal row indices: {row_ind}  # Always [0, 1, 2]\n",
    "Optimal column indices: {col_ind}  # Columns minimizing total cost\n",
    "Total cost: {cost_matrix[row_ind, col_ind].sum()}  # 1 + 0 + 2 = 3\n",
    "\"\"\")\n",
    "\n",
    "# ================================================================\n",
    "# 7. Global Optimization (Differential Evolution)\n",
    "# ================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
