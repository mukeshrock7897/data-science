{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### ‚úÖ **Complete List of EDA Topics for Machine Learning**\n",
    "\n",
    "*(Organized step-by-step, from raw data to insights)*\n",
    "\n",
    "---\n",
    "\n",
    "#### üßπ 1. **Data Cleaning**\n",
    "\n",
    "* Handling missing values (imputation, removal)\n",
    "* Handling duplicates\n",
    "* Handling outliers\n",
    "* Type conversions (e.g., object to float)\n",
    "* Dealing with inconsistent formats (dates, currencies, etc.)\n",
    "* String trimming and standardization\n",
    "\n",
    "---\n",
    "\n",
    "#### üìè 2. **Data Type Identification & Conversion**\n",
    "\n",
    "* Numerical vs Categorical\n",
    "* Ordinal vs Nominal\n",
    "* Datetime parsing\n",
    "* Encoding (Label, One-hot, Ordinal)\n",
    "\n",
    "---\n",
    "\n",
    "#### üìä 3. **Univariate Analysis**\n",
    "\n",
    "* Summary statistics (mean, median, mode, std, IQR)\n",
    "* Frequency distribution\n",
    "* Value counts\n",
    "* Distribution plots (histogram, KDE, boxplot)\n",
    "* Detecting skewness and kurtosis\n",
    "\n",
    "---\n",
    "\n",
    "#### üìà 4. **Bivariate Analysis**\n",
    "\n",
    "* Correlation matrix (Pearson, Spearman)\n",
    "* Scatter plots\n",
    "* Heatmaps\n",
    "* Pair plots\n",
    "* Groupby analysis\n",
    "* Cross-tabulation\n",
    "* Boxplots/grouped boxplots\n",
    "\n",
    "---\n",
    "\n",
    "#### üîÅ 5. **Multivariate Analysis**\n",
    "\n",
    "* Multivariate correlation\n",
    "* Pairplots (Seaborn)\n",
    "* FacetGrid analysis\n",
    "* PCA for visualization\n",
    "* Bubble charts\n",
    "\n",
    "---\n",
    "\n",
    "#### üìâ 6. **Outlier Detection**\n",
    "\n",
    "* Z-score\n",
    "* IQR method\n",
    "* Boxplot visual method\n",
    "* Isolation Forest (optional ML method)\n",
    "* Mahalanobis distance\n",
    "\n",
    "---\n",
    "\n",
    "#### üß¨ 7. **Feature Distribution Analysis**\n",
    "\n",
    "* Normal vs non-normal distribution\n",
    "* Skewness correction (log, sqrt, Box-Cox)\n",
    "* Visualization: histogram, distplot, violin plot\n",
    "\n",
    "---\n",
    "\n",
    "#### üìÜ 8. **Time Series EDA**\n",
    "\n",
    "* Time-based decomposition\n",
    "* Rolling statistics\n",
    "* Seasonal trends\n",
    "* Lag analysis\n",
    "* Autocorrelation/Partial Autocorrelation\n",
    "\n",
    "---\n",
    "\n",
    "#### üìÇ 9. **Categorical Variable Analysis**\n",
    "\n",
    "* Frequency tables\n",
    "* Bar plots / Count plots\n",
    "* Pie charts (use sparingly)\n",
    "* Stacked bar charts\n",
    "* Chi-square test (for association)\n",
    "\n",
    "---\n",
    "\n",
    "#### üìä 10. **Numerical Variable Analysis**\n",
    "\n",
    "* Distribution shape\n",
    "* Mean/median comparison\n",
    "* Boxplots by category\n",
    "* Violin plots\n",
    "* ANOVA or t-tests\n",
    "\n",
    "---\n",
    "\n",
    "#### üîÄ 11. **Encoding Categorical Data**\n",
    "\n",
    "* Label Encoding\n",
    "* One-Hot Encoding\n",
    "* Target/Mean Encoding\n",
    "* Frequency Encoding\n",
    "\n",
    "---\n",
    "\n",
    "#### üìâ 12. **Correlation Analysis**\n",
    "\n",
    "* Pearson, Spearman, Kendall coefficients\n",
    "* Heatmaps\n",
    "* Variance Inflation Factor (VIF) for multicollinearity\n",
    "\n",
    "---\n",
    "\n",
    "#### üß™ 13. **Missing Value Treatment**\n",
    "\n",
    "* Count and percentage of missing values\n",
    "* Missingness pattern visualization\n",
    "* Imputation techniques:\n",
    "\n",
    "  * Mean/median/mode\n",
    "  * Forward/backward fill\n",
    "  * KNN imputation\n",
    "  * Regression imputation\n",
    "\n",
    "---\n",
    "\n",
    "#### üßÆ 14. **Feature Engineering (EDA-Aided)**\n",
    "\n",
    "* Polynomial features\n",
    "* Interaction terms\n",
    "* Date parts (year, month, weekday, etc.)\n",
    "* Binning (equal-width, equal-frequency, quantile-based)\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚öñÔ∏è 15. **Target Variable Analysis**\n",
    "\n",
    "* Class imbalance (binary/multiclass)\n",
    "* Distribution of target vs features\n",
    "* Use of stratification for classification\n",
    "* SMOTE or undersampling techniques (if applied)\n",
    "\n",
    "---\n",
    "\n",
    "#### üìà 16. **Visualization Techniques**\n",
    "\n",
    "* Seaborn, Matplotlib, Plotly\n",
    "* Histograms, KDE, Boxplots\n",
    "* Count plots, Pie charts, Bar charts\n",
    "* Heatmaps, Correlation plots\n",
    "* Joint plots, Pair plots, Violin plots\n",
    "* Time series plots\n",
    "* Missing value matrix (e.g., `msno.matrix()`)\n",
    "\n",
    "---\n",
    "\n",
    "#### üìã 17. **EDA Reporting**\n",
    "\n",
    "* Pandas Profiling\n",
    "* Sweetviz\n",
    "* D-Tale\n",
    "* Autoviz\n",
    "* Lux\n",
    "\n",
    "---\n",
    "\n",
    "#### üõë 18. **EDA Red Flags**\n",
    "\n",
    "* Data leakage detection\n",
    "* Target leakage in features\n",
    "* High multicollinearity\n",
    "* Dominant class in target\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### üî∑ **1. Data Cleaning** üßπ\n",
    "\n",
    "---\n",
    "\n",
    "#### üìñ 1. Definition:\n",
    "\n",
    "* **Data cleaning** refers to identifying and correcting errors or inconsistencies in data to improve its quality and reliability.\n",
    "* It includes removing duplicates, fixing missing or inconsistent data, and formatting values appropriately.\n",
    "\n",
    "---\n",
    "\n",
    "#### üõ†Ô∏è 2. Available Built-in Functions (with Descriptions & Output Examples):\n",
    "\n",
    "| üîß Function | üí° Description | üß™ Example & Output |\n",
    "| ----------- | -------------- | ------------------- |\n",
    "\n",
    "**‚û§ `df.isnull()` / `df.isna()`**\n",
    "\n",
    "* Detects missing (`NaN`) values.\n",
    "\n",
    "```python\n",
    "df.isnull()\n",
    "# Output:\n",
    "#     Name    Age  Gender\n",
    "# 0  False  False   False\n",
    "# 1  False  False   False\n",
    "# 2  False   True   False\n",
    "# 3  False  False   False\n",
    "# 4   True  False   False\n",
    "```\n",
    "\n",
    "**‚û§ `df.dropna()`**\n",
    "\n",
    "* Removes rows (or columns) with missing values.\n",
    "\n",
    "```python\n",
    "df.dropna()\n",
    "# Output:\n",
    "#     Name   Age Gender\n",
    "# 0  Alice  25.0      F\n",
    "# 1    Bob  30.0      M\n",
    "# 3    Bob  30.0      M\n",
    "```\n",
    "\n",
    "**‚û§ `df.fillna(value)`**\n",
    "\n",
    "* Replaces missing values with a specific value.\n",
    "\n",
    "```python\n",
    "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
    "# Output: [25.0, 30.0, 25.0, 30.0, 22.0]\n",
    "```\n",
    "\n",
    "**‚û§ `df.duplicated()`**\n",
    "\n",
    "* Returns a Boolean Series indicating duplicate rows.\n",
    "\n",
    "```python\n",
    "df.duplicated()\n",
    "# Output:\n",
    "# 0    False\n",
    "# 1    False\n",
    "# 2    False\n",
    "# 3     True\n",
    "# 4    False\n",
    "```\n",
    "\n",
    "**‚û§ `df.drop_duplicates()`**\n",
    "\n",
    "* Drops duplicate rows from DataFrame.\n",
    "\n",
    "```python\n",
    "df.drop_duplicates()\n",
    "# Output:\n",
    "#     Name   Age Gender\n",
    "# 0  Alice  25.0      F\n",
    "# 1    Bob  30.0      M\n",
    "# 2 Charlie   NaN      M\n",
    "# 4   None  22.0      F\n",
    "```\n",
    "\n",
    "**‚û§ `df.astype(type)`**\n",
    "\n",
    "* Converts column to a different data type.\n",
    "\n",
    "```python\n",
    "df['Age'] = df['Age'].astype(int)\n",
    "# Output: [25, 30, 25, 30, 22]\n",
    "```\n",
    "\n",
    "**‚û§ `df.replace()`**\n",
    "\n",
    "* Replaces specified values with new ones.\n",
    "\n",
    "```python\n",
    "df['Gender'].replace({'M': 'Male', 'F': 'Female'}, inplace=True)\n",
    "# Output: ['Female', 'Male', 'Male', 'Male', 'Female']\n",
    "```\n",
    "\n",
    "**‚û§ `str.strip()` / `str.lower()` / `str.upper()`**\n",
    "\n",
    "* Trims strings and adjusts case.\n",
    "\n",
    "```python\n",
    "df['Name'] = df['Name'].str.strip()\n",
    "# Output: ['Alice', 'Bob', 'Charlie', 'Bob', None]\n",
    "\n",
    "df['Name'] = df['Name'].str.upper()\n",
    "# Output: ['ALICE', 'BOB', 'CHARLIE', 'BOB', None]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå 3. Code Example:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Raw data with issues\n",
    "df = pd.DataFrame({\n",
    "    'Name': [' Alice ', 'Bob', 'Charlie', 'Bob', None],\n",
    "    'Age': [25, 30, None, 30, 22],\n",
    "    'Gender': ['F', 'M', 'M', 'M', 'F']\n",
    "})\n",
    "\n",
    "# 1. Strip whitespace\n",
    "df['Name'] = df['Name'].str.strip()\n",
    "# df['Name']: ['Alice', 'Bob', 'Charlie', 'Bob', None]\n",
    "\n",
    "# 2. Fill missing 'Age' with median\n",
    "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "# df['Age']: [25.0, 30.0, 25.0, 30.0, 22.0]\n",
    "\n",
    "# 3. Drop rows with missing 'Name'\n",
    "df = df.dropna(subset=['Name'])\n",
    "# Rows with None in 'Name' are removed\n",
    "\n",
    "# 4. Remove duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "# Keeps only unique rows\n",
    "\n",
    "# 5. Replace 'M'/'F' with full gender labels\n",
    "df['Gender'] = df['Gender'].replace({'M': 'Male', 'F': 'Female'})\n",
    "# df['Gender']: ['Female', 'Male', 'Male']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚è±Ô∏è 4. When to Use:\n",
    "\n",
    "* üïê Immediately after importing a dataset\n",
    "* üß™ Before visualizations, analysis, or machine learning modeling\n",
    "* ‚öôÔ∏è When encountering missing, duplicate, or inconsistent values\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ö†Ô∏è 5. Limitations and Challenges:\n",
    "\n",
    "* üßØ Risk of removing important data when dropping\n",
    "* üéØ Imputation might distort statistical integrity\n",
    "* üß† Requires domain knowledge for accurate cleaning\n",
    "* üß™ Edge cases and text data often require custom logic\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üî∑ **2. Data Type Identification & Conversion** üß¨\n",
    "\n",
    "---\n",
    "\n",
    "#### üìñ 1. Definition:\n",
    "\n",
    "* This step involves **understanding and converting** the data types of columns in a dataset.\n",
    "* Correct data types ensure efficient memory usage, accurate calculations, and proper function behavior.\n",
    "\n",
    "---\n",
    "\n",
    "#### üõ†Ô∏è 2. Available Built-in Functions (with Descriptions & Output Examples):\n",
    "\n",
    "\\| üîß Function | üí° Description | üß™ Example & Output |\n",
    "\n",
    "**‚û§ `df.dtypes`**\n",
    "\n",
    "* Returns data types of each column\n",
    "\n",
    "```python\n",
    "df.dtypes\n",
    "# Output:\n",
    "# Name       object\n",
    "# Age       float64\n",
    "# Gender     object\n",
    "# DOB        object\n",
    "```\n",
    "\n",
    "**‚û§ `df.info()`**\n",
    "\n",
    "* Gives a summary: column count, non-null count, and data types\n",
    "\n",
    "```python\n",
    "df.info()\n",
    "# Output:\n",
    "# <class 'pandas.core.frame.DataFrame'>\n",
    "# RangeIndex: 5 entries, 0 to 4\n",
    "# Data columns (total 4 columns):\n",
    "#  #   Column  Non-Null Count  Dtype  \n",
    "# ---  ------  --------------  -----  \n",
    "#  0   Name    5 non-null      object \n",
    "#  1   Age     5 non-null      float64\n",
    "#  2   Gender  5 non-null      object \n",
    "#  3   DOB     5 non-null      object\n",
    "```\n",
    "\n",
    "**‚û§ `df.astype(type)`**\n",
    "\n",
    "* Converts column to a different data type\n",
    "\n",
    "```python\n",
    "df['Age'] = df['Age'].astype(int)\n",
    "# Output: df['Age']: [25, 30, 25, 30, 22]\n",
    "```\n",
    "\n",
    "**‚û§ `pd.to_numeric()`**\n",
    "\n",
    "* Converts values to numeric type (int or float), with error handling\n",
    "\n",
    "```python\n",
    "df['Age'] = pd.to_numeric(df['Age'], errors='coerce')\n",
    "# Output: Converts valid values, replaces invalid ones with NaN\n",
    "```\n",
    "\n",
    "**‚û§ `pd.to_datetime()`**\n",
    "\n",
    "* Converts string column to datetime format\n",
    "\n",
    "```python\n",
    "df['DOB'] = pd.to_datetime(df['DOB'])\n",
    "# Output: datetime64[ns]\n",
    "```\n",
    "\n",
    "**‚û§ `df.select_dtypes(include=...)`**\n",
    "\n",
    "* Filters columns by their data type\n",
    "\n",
    "```python\n",
    "df.select_dtypes(include='object')\n",
    "# Output: Only columns with dtype=object\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå 3. Code Example:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Age': ['25', '30', '28'],     # String type\n",
    "    'DOB': ['2000-01-01', '1998-05-06', '1995-03-10']  # String dates\n",
    "})\n",
    "\n",
    "# Check current data types\n",
    "print(df.dtypes)\n",
    "# Output:\n",
    "# Name    object\n",
    "# Age     object\n",
    "# DOB     object\n",
    "\n",
    "# Convert Age to int\n",
    "df['Age'] = pd.to_numeric(df['Age'])\n",
    "# Output: [25, 30, 28]\n",
    "\n",
    "# Convert DOB to datetime\n",
    "df['DOB'] = pd.to_datetime(df['DOB'])\n",
    "# Output: datetime64[ns]\n",
    "\n",
    "# Confirm changes\n",
    "print(df.dtypes)\n",
    "# Output:\n",
    "# Name    object\n",
    "# Age      int64\n",
    "# DOB     datetime64[ns]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚è±Ô∏è 4. When to Use:\n",
    "\n",
    "* üíº When importing external datasets (CSV, Excel, SQL)\n",
    "* üß† Before applying functions that require specific types (e.g., math ops on numbers)\n",
    "* üìä When preparing features for machine learning models\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ö†Ô∏è 5. Limitations and Challenges:\n",
    "\n",
    "* üßØ Data with mixed types (e.g., '10', 'ten') may fail to convert\n",
    "* ‚õî `astype()` will throw an error on invalid conversion\n",
    "* üìÖ Date parsing can fail with inconsistent formats\n",
    "* üß† Requires manual inspection in ambiguous cases\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üî∑ **3. Univariate Analysis** üìä\n",
    "\n",
    "---\n",
    "\n",
    "#### üìñ 1. Definition:\n",
    "\n",
    "* **Univariate analysis** is the examination of **a single variable**.\n",
    "* It helps summarize and understand the distribution, central tendency, and spread of a variable.\n",
    "* Can be performed on both **numerical** and **categorical** data.\n",
    "\n",
    "---\n",
    "\n",
    "#### üõ†Ô∏è 2. Available Built-in Functions (with Descriptions & Output Examples):\n",
    "\n",
    "\\| üîß Function | üí° Description | üß™ Example & Output |\n",
    "\n",
    "**‚û§ `df['col'].value_counts()`**\n",
    "\n",
    "* Counts frequency of unique values (best for categorical)\n",
    "\n",
    "```python\n",
    "df['Gender'].value_counts()\n",
    "# Output:\n",
    "# Male      3\n",
    "# Female    2\n",
    "```\n",
    "\n",
    "**‚û§ `df['col'].unique()` / `df['col'].nunique()`**\n",
    "\n",
    "* Returns unique values and number of unique values\n",
    "\n",
    "```python\n",
    "df['Gender'].unique()\n",
    "# Output: ['Male', 'Female']\n",
    "\n",
    "df['Gender'].nunique()\n",
    "# Output: 2\n",
    "```\n",
    "\n",
    "**‚û§ `df['col'].describe()`**\n",
    "\n",
    "* Gives summary stats (count, mean, std, min, 25%, 50%, 75%, max)\n",
    "\n",
    "```python\n",
    "df['Age'].describe()\n",
    "# Output:\n",
    "# count     5.000000\n",
    "# mean     26.400000\n",
    "# std       3.209361\n",
    "# min      22.000000\n",
    "# 25%      25.000000\n",
    "# 50%      25.000000\n",
    "# 75%      30.000000\n",
    "# max      30.000000\n",
    "```\n",
    "\n",
    "**‚û§ `df['col'].plot(kind='hist')` / `df['col'].hist()`**\n",
    "\n",
    "* Plots histogram for numeric data\n",
    "\n",
    "```python\n",
    "df['Age'].plot(kind='hist')\n",
    "# Output: Histogram showing distribution of Age\n",
    "```\n",
    "\n",
    "**‚û§ `df['col'].plot(kind='box')` / `sns.boxplot()`**\n",
    "\n",
    "* Boxplot shows median, quartiles, and outliers\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "sns.boxplot(x=df['Age'])\n",
    "# Output: Boxplot for Age distribution\n",
    "```\n",
    "\n",
    "**‚û§ `sns.countplot(x='col', data=df)`**\n",
    "\n",
    "* Countplot for categorical features\n",
    "\n",
    "```python\n",
    "sns.countplot(x='Gender', data=df)\n",
    "# Output: Bar chart showing count of Male/Female\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå 3. Code Example:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Age': [22, 25, 25, 30, 30],\n",
    "    'Gender': ['Female', 'Male', 'Male', 'Male', 'Female']\n",
    "})\n",
    "\n",
    "# Summary statistics\n",
    "print(df['Age'].describe())\n",
    "# Output:\n",
    "# count     5.000000\n",
    "# mean     26.400000\n",
    "# std       3.209361\n",
    "# min      22.000000\n",
    "# 25%      25.000000\n",
    "# 50%      25.000000\n",
    "# 75%      30.000000\n",
    "# max      30.000000\n",
    "\n",
    "# Value counts for Gender\n",
    "print(df['Gender'].value_counts())\n",
    "# Output:\n",
    "# Male      3\n",
    "# Female    2\n",
    "\n",
    "# Countplot for Gender\n",
    "sns.countplot(x='Gender', data=df)\n",
    "plt.show()\n",
    "\n",
    "# Boxplot for Age\n",
    "sns.boxplot(x=df['Age'])\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚è±Ô∏è 4. When to Use:\n",
    "\n",
    "* üìä Early in EDA to understand each variable individually\n",
    "* üéØ To detect outliers, skewness, and distribution shape\n",
    "* ‚úÖ Useful in feature selection (e.g., if all values are same, it‚Äôs not useful)\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ö†Ô∏è 5. Limitations and Challenges:\n",
    "\n",
    "* üö´ Doesn‚Äôt reveal relationships between variables\n",
    "* üìâ Can miss hidden patterns unless visualized well\n",
    "* ‚ö†Ô∏è Sensitive to outliers (especially for numerical data)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üî∑ **4. Bivariate Analysis** üîó\n",
    "\n",
    "---\n",
    "\n",
    "#### üìñ 1. Definition:\n",
    "\n",
    "* **Bivariate analysis** explores the **relationship between two variables**.\n",
    "* It helps detect **correlation**, **association**, or **causal patterns** between features.\n",
    "* Variable combinations can be:\n",
    "\n",
    "  * üî¢ Numerical vs Numerical\n",
    "  * üß© Categorical vs Categorical\n",
    "  * üß† Categorical vs Numerical\n",
    "\n",
    "---\n",
    "\n",
    "#### üõ†Ô∏è 2. Available Built-in Functions (with Descriptions & Output Examples):\n",
    "\n",
    "\\| üîß Function | üí° Description | üß™ Example & Output |\n",
    "\n",
    "**‚û§ `df.corr()`**\n",
    "\n",
    "* Computes pairwise correlation between numeric columns\n",
    "\n",
    "```python\n",
    "df.corr()\n",
    "# Output:\n",
    "#         Age  Salary\n",
    "# Age     1.0    0.89\n",
    "# Salary  0.89   1.0\n",
    "```\n",
    "\n",
    "**‚û§ `sns.heatmap()`**\n",
    "\n",
    "* Visualizes the correlation matrix\n",
    "\n",
    "```python\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "# Output: Heatmap with correlation coefficients\n",
    "```\n",
    "\n",
    "**‚û§ `sns.scatterplot(x=..., y=..., data=...)`**\n",
    "\n",
    "* Scatter plot for numeric vs numeric\n",
    "\n",
    "```python\n",
    "sns.scatterplot(x='Age', y='Salary', data=df)\n",
    "# Output: Points showing relationship\n",
    "```\n",
    "\n",
    "**‚û§ `sns.boxplot(x=..., y=..., data=...)`**\n",
    "\n",
    "* Boxplot for numeric vs categorical\n",
    "\n",
    "```python\n",
    "sns.boxplot(x='Gender', y='Salary', data=df)\n",
    "# Output: Salary distribution across genders\n",
    "```\n",
    "\n",
    "**‚û§ `pd.crosstab()`**\n",
    "\n",
    "* Cross-tabulation (frequency table) of two categorical variables\n",
    "\n",
    "```python\n",
    "pd.crosstab(df['Gender'], df['Purchased'])\n",
    "# Output:\n",
    "# Purchased  No  Yes\n",
    "# Gender            \n",
    "# Female      1    1\n",
    "# Male        1    2\n",
    "```\n",
    "\n",
    "**‚û§ `sns.countplot(x=..., hue=..., data=...)`**\n",
    "\n",
    "* Countplot for comparing categories\n",
    "\n",
    "```python\n",
    "sns.countplot(x='Gender', hue='Purchased', data=df)\n",
    "# Output: Side-by-side bars grouped by gender\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå 3. Code Example:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Age': [22, 25, 30, 28, 26],\n",
    "    'Salary': [25000, 30000, 50000, 42000, 38000],\n",
    "    'Gender': ['Female', 'Male', 'Male', 'Male', 'Female'],\n",
    "    'Purchased': ['Yes', 'No', 'Yes', 'Yes', 'No']\n",
    "})\n",
    "\n",
    "# Correlation matrix\n",
    "print(df.corr())\n",
    "# Output:\n",
    "#              Age    Salary\n",
    "# Age     1.000000  0.899735\n",
    "# Salary  0.899735  1.000000\n",
    "\n",
    "# Heatmap\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot: Age vs Salary\n",
    "sns.scatterplot(x='Age', y='Salary', data=df)\n",
    "plt.show()\n",
    "\n",
    "# Box plot: Gender vs Salary\n",
    "sns.boxplot(x='Gender', y='Salary', data=df)\n",
    "plt.show()\n",
    "\n",
    "# Crosstab\n",
    "print(pd.crosstab(df['Gender'], df['Purchased']))\n",
    "# Output:\n",
    "# Purchased  No  Yes\n",
    "# Gender            \n",
    "# Female      1    1\n",
    "# Male        1    2\n",
    "\n",
    "# Countplot\n",
    "sns.countplot(x='Gender', hue='Purchased', data=df)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚è±Ô∏è 4. When to Use:\n",
    "\n",
    "* üîç To identify correlation, trends, or class separation\n",
    "* üß† To detect possible data leakage or multicollinearity\n",
    "* üéØ Before selecting features for models\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ö†Ô∏è 5. Limitations and Challenges:\n",
    "\n",
    "* ‚ùå Correlation ‚â† causation\n",
    "* üìâ Correlation applies only to numeric values\n",
    "* üß™ Categorical relations may require deeper statistical testing (e.g., chi-square)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üî∑ **5. Multivariate Analysis** üß™üìä\n",
    "\n",
    "---\n",
    "\n",
    "#### üìñ 1. Definition:\n",
    "\n",
    "* **Multivariate analysis** examines **three or more variables** simultaneously.\n",
    "* It helps uncover **complex relationships**, **interactions**, and **patterns** that aren‚Äôt visible in univariate or bivariate analysis.\n",
    "* Common in feature selection, hypothesis testing, and advanced visualization.\n",
    "\n",
    "---\n",
    "\n",
    "#### üõ†Ô∏è 2. Available Built-in Functions (with Descriptions & Output Examples):\n",
    "\n",
    "\\| üîß Function | üí° Description | üß™ Example & Output |\n",
    "\n",
    "**‚û§ `sns.pairplot(data, hue=...)`**\n",
    "\n",
    "* Plots pairwise relationships between numeric columns, color-coded by categorical column\n",
    "\n",
    "```python\n",
    "sns.pairplot(df, hue='Purchased')\n",
    "# Output: Matrix of scatter plots and histograms\n",
    "```\n",
    "\n",
    "**‚û§ `sns.heatmap(df.corr(), annot=True)`**\n",
    "\n",
    "* Heatmap for all variable correlations\n",
    "\n",
    "```python\n",
    "sns.heatmap(df.corr(), annot=True, cmap='YlGnBu')\n",
    "# Output: Correlation between all numeric features\n",
    "```\n",
    "\n",
    "**‚û§ `sns.scatterplot(x=..., y=..., hue=..., size=..., data=...)`**\n",
    "\n",
    "* Scatterplot with third or fourth variable shown via color and size\n",
    "\n",
    "```python\n",
    "sns.scatterplot(x='Age', y='Salary', hue='Purchased', size='Experience', data=df)\n",
    "# Output: Enhanced scatter plot\n",
    "```\n",
    "\n",
    "**‚û§ `pd.plotting.scatter_matrix()`**\n",
    "\n",
    "* Similar to pairplot, built into pandas\n",
    "\n",
    "```python\n",
    "pd.plotting.scatter_matrix(df[['Age', 'Salary', 'Experience']], figsize=(8, 6))\n",
    "# Output: Matrix of scatter plots\n",
    "```\n",
    "\n",
    "**‚û§ `sns.lmplot(x=..., y=..., hue=..., col=..., data=...)`**\n",
    "\n",
    "* Fits regression lines and shows separation across multiple dimensions\n",
    "\n",
    "```python\n",
    "sns.lmplot(x='Age', y='Salary', hue='Purchased', col='Gender', data=df)\n",
    "# Output: Multi-panel linear regression plots\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå 3. Code Example:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Age': [22, 25, 30, 28, 26],\n",
    "    'Salary': [25000, 30000, 50000, 42000, 38000],\n",
    "    'Experience': [1, 3, 7, 6, 4],\n",
    "    'Gender': ['Female', 'Male', 'Male', 'Male', 'Female'],\n",
    "    'Purchased': ['Yes', 'No', 'Yes', 'Yes', 'No']\n",
    "})\n",
    "\n",
    "# Pairplot\n",
    "sns.pairplot(df, hue='Purchased')\n",
    "plt.show()\n",
    "\n",
    "# Scatterplot with hue and size\n",
    "sns.scatterplot(x='Age', y='Salary', hue='Purchased', size='Experience', data=df)\n",
    "plt.show()\n",
    "\n",
    "# Heatmap\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "# lmplot (multi-panel regression lines)\n",
    "sns.lmplot(x='Age', y='Salary', hue='Purchased', col='Gender', data=df)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚è±Ô∏è 4. When to Use:\n",
    "\n",
    "* üß† For exploring complex relationships between multiple features\n",
    "* üß™ During feature selection and dimensionality reduction\n",
    "* üî¨ For hypothesis generation in multivariate modeling\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ö†Ô∏è 5. Limitations and Challenges:\n",
    "\n",
    "* ‚ö†Ô∏è High-dimensional plots can be hard to interpret\n",
    "* üñºÔ∏è Too many features = cluttered visuals\n",
    "* üîç May require advanced techniques (PCA, t-SNE) for visual clarity\n",
    "* üßÆ Computationally expensive for large datasets\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üî∑ **6. Outlier Detection** üö®üìâ\n",
    "\n",
    "---\n",
    "\n",
    "#### üìñ 1. Definition:\n",
    "\n",
    "* **Outliers** are data points that significantly differ from the rest of the dataset.\n",
    "* Outlier detection is essential in **data cleaning** and **model accuracy**.\n",
    "* Outliers can result from **errors**, **natural variance**, or **rare events**.\n",
    "\n",
    "---\n",
    "\n",
    "#### üõ†Ô∏è 2. Available Built-in Functions (with Descriptions & Output Examples):\n",
    "\n",
    "\\| üîß Function | üí° Description | üß™ Example & Output |\n",
    "\n",
    "**‚û§ `df.describe()`**\n",
    "\n",
    "* Shows summary statistics; useful for spotting large deviations\n",
    "\n",
    "```python\n",
    "df['Salary'].describe()\n",
    "# Output:\n",
    "# count        6.000000\n",
    "# mean     65000.000000\n",
    "# std      35000.000000\n",
    "# min      25000.000000\n",
    "# 25%      30000.000000\n",
    "# 50%      45000.000000\n",
    "# 75%      70000.000000\n",
    "# max     150000.000000  ‚Üê possible outlier\n",
    "```\n",
    "\n",
    "**‚û§ `sns.boxplot()`**\n",
    "\n",
    "* Visual tool to detect outliers using IQR (dots beyond whiskers = outliers)\n",
    "\n",
    "```python\n",
    "sns.boxplot(x=df['Salary'])\n",
    "plt.show()\n",
    "# Output: Boxplot with outliers shown as individual dots\n",
    "```\n",
    "\n",
    "**‚û§ `zscore` from `scipy.stats`**\n",
    "\n",
    "* Detects outliers using standard deviation (Z-score > 3 or < -3)\n",
    "\n",
    "```python\n",
    "from scipy.stats import zscore\n",
    "import numpy as np\n",
    "\n",
    "z_scores = zscore(df['Salary'])\n",
    "outliers = df[np.abs(z_scores) > 3]\n",
    "print(outliers)\n",
    "# Output: Rows with Z-score outliers\n",
    "```\n",
    "\n",
    "**‚û§ IQR method (manual)**\n",
    "\n",
    "* Filters data outside Q1‚Äì1.5*IQR or Q3+1.5*IQR\n",
    "\n",
    "```python\n",
    "Q1 = df['Salary'].quantile(0.25)\n",
    "Q3 = df['Salary'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = df[(df['Salary'] < Q1 - 1.5 * IQR) | (df['Salary'] > Q3 + 1.5 * IQR)]\n",
    "print(outliers)\n",
    "# Output: Data points outside the acceptable range\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå 3. Code Example:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Salary': [25000, 30000, 45000, 70000, 80000, 150000]  # 150000 may be an outlier\n",
    "})\n",
    "\n",
    "# Describe\n",
    "print(df['Salary'].describe())\n",
    "# Output:\n",
    "# count         6.000000\n",
    "# mean      67500.000000\n",
    "# std       43899.201438\n",
    "# min       25000.000000\n",
    "# 25%       33750.000000\n",
    "# 50%       57500.000000\n",
    "# 75%       77500.000000\n",
    "# max      150000.000000\n",
    "\n",
    "# Boxplot\n",
    "sns.boxplot(x=df['Salary'])\n",
    "plt.show()\n",
    "\n",
    "# Z-score method\n",
    "z_scores = zscore(df['Salary'])\n",
    "outliers_z = df[np.abs(z_scores) > 3]\n",
    "print(outliers_z)\n",
    "# Output:\n",
    "#     Salary\n",
    "# 5  150000.0\n",
    "\n",
    "# IQR method\n",
    "Q1 = df['Salary'].quantile(0.25)\n",
    "Q3 = df['Salary'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers_iqr = df[(df['Salary'] < Q1 - 1.5 * IQR) | (df['Salary'] > Q3 + 1.5 * IQR)]\n",
    "print(outliers_iqr)\n",
    "# Output:\n",
    "#     Salary\n",
    "# 5  150000\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚è±Ô∏è 4. When to Use:\n",
    "\n",
    "* üîç Before modeling or normalization steps\n",
    "* üí° In fraud detection, anomaly detection, or quality control\n",
    "* üìä In datasets where variance matters (e.g., salaries, prices)\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ö†Ô∏è 5. Limitations and Challenges:\n",
    "\n",
    "* ‚ùó May incorrectly remove **legitimate rare events**\n",
    "* ‚ö†Ô∏è Not all outliers are ‚Äúbad‚Äù (some may carry insights)\n",
    "* ‚öôÔ∏è Sensitive to method used (Z-score, IQR, etc.)\n",
    "* üß† Must consider domain knowledge before removing outliers\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üî∑ **7. Handling Missing Values** üß©üõ†Ô∏è\n",
    "\n",
    "---\n",
    "\n",
    "#### üìñ 1. Definition:\n",
    "\n",
    "* **Missing values** are blank, `NaN`, or `None` entries in the dataset.\n",
    "* They can arise due to data corruption, manual errors, or unrecorded info.\n",
    "* Handling them is essential to ensure clean, complete, and reliable models.\n",
    "\n",
    "---\n",
    "\n",
    "#### üõ†Ô∏è 2. Available Built-in Functions (with Descriptions & Output Examples):\n",
    "\n",
    "\\| üîß Function | üí° Description | üß™ Example & Output |\n",
    "\n",
    "**‚û§ `df.isnull()` / `df.isnull().sum()`**\n",
    "\n",
    "* Detects missing values and counts them\n",
    "\n",
    "```python\n",
    "print(df.isnull())\n",
    "# Output:\n",
    "#     Age  Salary  Gender\n",
    "# 0  False   False   False\n",
    "# 1  False   False   False\n",
    "# 2  False   False   False\n",
    "# 3  False    True   False\n",
    "# 4  False   False    True\n",
    "\n",
    "print(df.isnull().sum())\n",
    "# Output:\n",
    "# Age       0\n",
    "# Salary    1\n",
    "# Gender    1\n",
    "```\n",
    "\n",
    "**‚û§ `df.dropna()`**\n",
    "\n",
    "* Removes rows with any missing values\n",
    "\n",
    "```python\n",
    "df_clean = df.dropna()\n",
    "print(df_clean)\n",
    "# Output: Rows where all values are present\n",
    "```\n",
    "\n",
    "**‚û§ `df.fillna(value)`**\n",
    "\n",
    "* Fills missing values with a specified value\n",
    "\n",
    "```python\n",
    "df_filled = df.fillna(0)\n",
    "print(df_filled)\n",
    "# Output: Missing values replaced with 0\n",
    "```\n",
    "\n",
    "**‚û§ `df['col'].fillna(method='ffill') / 'bfill'`**\n",
    "\n",
    "* Forward or backward fill from adjacent rows\n",
    "\n",
    "```python\n",
    "df['Gender'] = df['Gender'].fillna(method='ffill')\n",
    "# Output: Fills with previous row value\n",
    "```\n",
    "\n",
    "**‚û§ `df['col'].fillna(df['col'].mean())`**\n",
    "\n",
    "* Fills numeric column missing values with mean\n",
    "\n",
    "```python\n",
    "df['Salary'] = df['Salary'].fillna(df['Salary'].mean())\n",
    "# Output: Fills with average salary\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå 3. Code Example:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Age': [22, 25, 30, 28, 26],\n",
    "    'Salary': [25000, 30000, 50000, None, 38000],\n",
    "    'Gender': ['Female', 'Male', 'Male', 'Male', None]\n",
    "})\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "# Output:\n",
    "# Age       0\n",
    "# Salary    1\n",
    "# Gender    1\n",
    "\n",
    "# Fill Salary with mean\n",
    "df['Salary'] = df['Salary'].fillna(df['Salary'].mean())\n",
    "print(df['Salary'])\n",
    "# Output:\n",
    "# 0    25000.0\n",
    "# 1    30000.0\n",
    "# 2    50000.0\n",
    "# 3    35750.0   ‚Üê mean of other 4 salaries\n",
    "# 4    38000.0\n",
    "\n",
    "# Forward fill Gender\n",
    "df['Gender'] = df['Gender'].fillna(method='ffill')\n",
    "print(df['Gender'])\n",
    "# Output:\n",
    "# 0    Female\n",
    "# 1      Male\n",
    "# 2      Male\n",
    "# 3      Male\n",
    "# 4      Male   ‚Üê copied from row 3\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚è±Ô∏è 4. When to Use:\n",
    "\n",
    "* üìã Always check for missing values during EDA\n",
    "* üõ†Ô∏è Use drop when only a small % of data is missing\n",
    "* üìä Use imputation (mean/median/mode) when data is numeric and missing randomly\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ö†Ô∏è 5. Limitations and Challenges:\n",
    "\n",
    "* ‚ùå Dropping rows may lose valuable information\n",
    "* üìâ Filling with averages may distort variance\n",
    "* üß† Requires domain knowledge to fill accurately\n",
    "* ‚ö†Ô∏è May bias the model if patterns in missingness are ignored\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üî∑ **8. Feature Engineering** üõ†Ô∏è‚ú®\n",
    "\n",
    "---\n",
    "\n",
    "#### üìñ 1. Definition:\n",
    "\n",
    "* **Feature Engineering** is the process of **creating, transforming, or selecting** variables (features) to improve the performance of machine learning models.\n",
    "* It involves techniques like encoding categorical data, scaling numerical features, creating interaction terms, and extracting new variables.\n",
    "\n",
    "---\n",
    "\n",
    "#### üõ†Ô∏è 2. Available Built-in Functions (with Descriptions & Output Examples):\n",
    "\n",
    "\\| üîß Function | üí° Description | üß™ Example & Output |\n",
    "\n",
    "**‚û§ `pd.get_dummies()`**\n",
    "\n",
    "* Converts categorical variables into dummy/indicator variables (one-hot encoding)\n",
    "\n",
    "```python\n",
    "pd.get_dummies(df['Gender'])\n",
    "# Output:\n",
    "#    Female  Male\n",
    "# 0       1     0\n",
    "# 1       0     1\n",
    "# 2       0     1\n",
    "```\n",
    "\n",
    "**‚û§ `sklearn.preprocessing.StandardScaler()`**\n",
    "\n",
    "* Standardizes features by removing the mean and scaling to unit variance\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(df[['Age', 'Salary']])\n",
    "print(scaled)\n",
    "# Output: 2D array with scaled values\n",
    "```\n",
    "\n",
    "**‚û§ `sklearn.preprocessing.MinMaxScaler()`**\n",
    "\n",
    "* Scales features to a given range (default 0 to 1)\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(df[['Age', 'Salary']])\n",
    "print(scaled)\n",
    "# Output: 2D array with scaled values between 0 and 1\n",
    "```\n",
    "\n",
    "**‚û§ `df['new_feature'] = df['Age'] * df['Salary']`**\n",
    "\n",
    "* Creating interaction terms or new features by combining existing ones\n",
    "\n",
    "```python\n",
    "df['Age_Salary'] = df['Age'] * df['Salary']\n",
    "print(df['Age_Salary'])\n",
    "# Output: Series with multiplied values\n",
    "```\n",
    "\n",
    "**‚û§ `df['col'].astype('category').cat.codes`**\n",
    "\n",
    "* Label encoding categorical columns\n",
    "\n",
    "```python\n",
    "df['Gender_code'] = df['Gender'].astype('category').cat.codes\n",
    "print(df['Gender_code'])\n",
    "# Output:\n",
    "# 0    0\n",
    "# 1    1\n",
    "# 2    1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå 3. Code Example:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Age': [22, 25, 30, 28, 26],\n",
    "    'Salary': [25000, 30000, 50000, 42000, 38000],\n",
    "    'Gender': ['Female', 'Male', 'Male', 'Male', 'Female']\n",
    "})\n",
    "\n",
    "# One-hot encoding\n",
    "gender_dummies = pd.get_dummies(df['Gender'])\n",
    "print(gender_dummies)\n",
    "# Output:\n",
    "#    Female  Male\n",
    "# 0       1     0\n",
    "# 1       0     1\n",
    "# 2       0     1\n",
    "# 3       0     1\n",
    "# 4       1     0\n",
    "\n",
    "# Standard scaling\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df[['Age', 'Salary']])\n",
    "print(scaled_features)\n",
    "# Output: e.g.\n",
    "# [[-1.414 -1.414]\n",
    "#  [-0.707 -1.131]\n",
    "#  [ 1.414  1.414]\n",
    "#  [ 0.707  0.566]\n",
    "#  [ 0.000 -0.434]]\n",
    "\n",
    "# Min-Max scaling\n",
    "minmax = MinMaxScaler()\n",
    "scaled_minmax = minmax.fit_transform(df[['Age', 'Salary']])\n",
    "print(scaled_minmax)\n",
    "# Output: e.g.\n",
    "# [[0.   0.  ]\n",
    "#  [0.15 0.14]\n",
    "#  [1.   1.  ]\n",
    "#  [0.75 0.7 ]\n",
    "#  [0.4  0.46]]\n",
    "\n",
    "# Interaction feature\n",
    "df['Age_Salary'] = df['Age'] * df['Salary']\n",
    "print(df['Age_Salary'])\n",
    "# Output:\n",
    "# 0     550000\n",
    "# 1     750000\n",
    "# 2    1500000\n",
    "# 3    1176000\n",
    "# 4     988000\n",
    "\n",
    "# Label encoding\n",
    "df['Gender_code'] = df['Gender'].astype('category').cat.codes\n",
    "print(df['Gender_code'])\n",
    "# Output:\n",
    "# 0    0\n",
    "# 1    1\n",
    "# 2    1\n",
    "# 3    1\n",
    "# 4    0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚è±Ô∏è 4. When to Use:\n",
    "\n",
    "* üîß Before training models to improve accuracy and performance\n",
    "* üß† When dealing with categorical variables\n",
    "* ‚öñÔ∏è To scale features for models sensitive to feature magnitude (e.g., SVM, KNN)\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ö†Ô∏è 5. Limitations and Challenges:\n",
    "\n",
    "* ‚ö†Ô∏è Over-engineering can cause overfitting\n",
    "* üïµÔ∏è‚Äç‚ôÇÔ∏è Requires domain knowledge for meaningful features\n",
    "* ‚è≥ Time-consuming and may increase model complexity\n",
    "* üîÑ Some transformations require reversing for model interpretability\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üî∑ **9. Data Transformation and Scaling** ‚öôÔ∏èüìè\n",
    "\n",
    "---\n",
    "\n",
    "#### üìñ 1. Definition:\n",
    "\n",
    "* **Data transformation and scaling** involve changing the range or distribution of data features.\n",
    "* It makes data suitable for machine learning algorithms by normalizing, standardizing, or applying mathematical transformations.\n",
    "* Essential for algorithms sensitive to feature scale (e.g., KNN, SVM, gradient descent).\n",
    "\n",
    "---\n",
    "\n",
    "#### üõ†Ô∏è 2. Available Built-in Functions (with Descriptions & Output Examples):\n",
    "\n",
    "\\| üîß Function | üí° Description | üß™ Example & Output |\n",
    "\n",
    "**‚û§ `sklearn.preprocessing.StandardScaler()`**\n",
    "\n",
    "* Standardizes features by removing mean and scaling to unit variance\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(df[['Age', 'Salary']])\n",
    "print(scaled)\n",
    "# Output: 2D numpy array of standardized values\n",
    "```\n",
    "\n",
    "**‚û§ `sklearn.preprocessing.MinMaxScaler()`**\n",
    "\n",
    "* Scales features to a fixed range, usually \\[0, 1]\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(df[['Age', 'Salary']])\n",
    "print(scaled)\n",
    "# Output: 2D numpy array with values between 0 and 1\n",
    "```\n",
    "\n",
    "**‚û§ `sklearn.preprocessing.MaxAbsScaler()`**\n",
    "\n",
    "* Scales each feature by its maximum absolute value (keeps sparse data intact)\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "scaler = MaxAbsScaler()\n",
    "scaled = scaler.fit_transform(df[['Age', 'Salary']])\n",
    "print(scaled)\n",
    "# Output: values between -1 and 1 scaled by max abs value\n",
    "```\n",
    "\n",
    "**‚û§ `sklearn.preprocessing.PowerTransformer()`**\n",
    "\n",
    "* Applies power transformations to make data more Gaussian-like (Box-Cox or Yeo-Johnson)\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "transformed = pt.fit_transform(df[['Salary']])\n",
    "print(transformed)\n",
    "# Output: transformed data array\n",
    "```\n",
    "\n",
    "**‚û§ `np.log1p()` and `np.sqrt()`**\n",
    "\n",
    "* Log and square root transformations for skewed data\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "df['Log_Salary'] = np.log1p(df['Salary'])\n",
    "print(df['Log_Salary'])\n",
    "# Output: log-transformed salary values\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå 3. Code Example:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Age': [22, 25, 30, 28, 26],\n",
    "    'Salary': [25000, 30000, 50000, 42000, 38000]\n",
    "})\n",
    "\n",
    "# Standard scaling\n",
    "scaler = StandardScaler()\n",
    "scaled_std = scaler.fit_transform(df[['Age', 'Salary']])\n",
    "print(scaled_std)\n",
    "# Output:\n",
    "# [[-1.414 -1.414]\n",
    "#  [-0.707 -1.131]\n",
    "#  [ 1.414  1.414]\n",
    "#  [ 0.707  0.566]\n",
    "#  [ 0.    -0.434]]\n",
    "\n",
    "# Min-Max scaling\n",
    "minmax = MinMaxScaler()\n",
    "scaled_minmax = minmax.fit_transform(df[['Age', 'Salary']])\n",
    "print(scaled_minmax)\n",
    "# Output:\n",
    "# [[0.   0.  ]\n",
    "#  [0.15 0.14]\n",
    "#  [1.   1.  ]\n",
    "#  [0.75 0.7 ]\n",
    "#  [0.4  0.46]]\n",
    "\n",
    "# Power transformation (Yeo-Johnson)\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "transformed = pt.fit_transform(df[['Salary']])\n",
    "print(transformed)\n",
    "# Output: array([[ -1.172], [-0.971], [1.555], [0.741], [-0.154]])\n",
    "\n",
    "# Log transformation for skewness reduction\n",
    "df['Log_Salary'] = np.log1p(df['Salary'])\n",
    "print(df['Log_Salary'])\n",
    "# Output:\n",
    "# 0    10.126631\n",
    "# 1    10.308953\n",
    "# 2    10.819778\n",
    "# 3    10.645941\n",
    "# 4    10.545163\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚è±Ô∏è 4. When to Use:\n",
    "\n",
    "* ‚öñÔ∏è When features have different units or scales\n",
    "* üß† Before applying distance-based ML algorithms (KNN, SVM)\n",
    "* üìâ To reduce skewness and improve normality assumptions\n",
    "* üöÄ To speed up convergence in gradient descent algorithms\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ö†Ô∏è 5. Limitations and Challenges:\n",
    "\n",
    "* üîÑ Transformation may make data less interpretable\n",
    "* ‚ùó Not all transformations fit every data distribution\n",
    "* ‚ö†Ô∏è Improper scaling may hurt model performance\n",
    "* üïµÔ∏è‚Äç‚ôÇÔ∏è Requires consistent scaling on train and test datasets\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üî∑ **10. Feature Selection** üéØüîç\n",
    "\n",
    "---\n",
    "\n",
    "#### üìñ 1. Definition:\n",
    "\n",
    "* **Feature Selection** is the process of selecting a subset of relevant features (variables) for model building.\n",
    "* It helps improve model performance, reduces overfitting, decreases training time, and enhances interpretability.\n",
    "* Methods include filter, wrapper, and embedded approaches.\n",
    "\n",
    "---\n",
    "\n",
    "#### üõ†Ô∏è 2. Available Built-in Functions (with Descriptions & Output Examples):\n",
    "\n",
    "\\| üîß Function | üí° Description | üß™ Example & Output |\n",
    "\n",
    "**‚û§ `sklearn.feature_selection.SelectKBest`**\n",
    "\n",
    "* Selects the top k features based on a scoring function (e.g., chi2, f\\_classif)\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "selector = SelectKBest(score_func=f_classif, k=2)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "print(X_new.shape)\n",
    "# Output: (n_samples, 2)\n",
    "```\n",
    "\n",
    "**‚û§ `sklearn.feature_selection.RFE` (Recursive Feature Elimination)**\n",
    "\n",
    "* Recursively removes features and builds model to select most important features\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, n_features_to_select=2)\n",
    "X_rfe = rfe.fit_transform(X, y)\n",
    "print(rfe.support_)\n",
    "# Output: array indicating selected features (True/False)\n",
    "```\n",
    "\n",
    "**‚û§ `sklearn.feature_selection.VarianceThreshold`**\n",
    "\n",
    "* Removes features with low variance (below threshold)\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=0.1)\n",
    "X_var = sel.fit_transform(X)\n",
    "print(X_var.shape)\n",
    "# Output: shape after removing low variance features\n",
    "```\n",
    "\n",
    "**‚û§ `feature_importances_` attribute (e.g., from RandomForestClassifier)**\n",
    "\n",
    "* Feature importance from tree-based models\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X, y)\n",
    "print(model.feature_importances_)\n",
    "# Output: array of feature importance scores\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå 3. Code Example:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE, VarianceThreshold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load example data\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = iris.target\n",
    "\n",
    "# SelectKBest example\n",
    "selector = SelectKBest(score_func=f_classif, k=2)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "print(\"SelectKBest shape:\", X_new.shape)\n",
    "# Output: SelectKBest shape: (150, 2)\n",
    "\n",
    "# Recursive Feature Elimination\n",
    "model = LogisticRegression(max_iter=200)\n",
    "rfe = RFE(model, n_features_to_select=2)\n",
    "X_rfe = rfe.fit_transform(X, y)\n",
    "print(\"RFE selected features:\", rfe.support_)\n",
    "# Output: RFE selected features: [ True False False  True]\n",
    "\n",
    "# Variance Threshold\n",
    "sel = VarianceThreshold(threshold=0.5)\n",
    "X_var = sel.fit_transform(X)\n",
    "print(\"VarianceThreshold shape:\", X_var.shape)\n",
    "# Output: VarianceThreshold shape: (150, 2)\n",
    "\n",
    "# Feature Importances from Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X, y)\n",
    "print(\"Feature importances:\", rf.feature_importances_)\n",
    "# Output: e.g. [0.1 0.4 0.3 0.2]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚è±Ô∏è 4. When to Use:\n",
    "\n",
    "* üöÄ To reduce dimensionality and improve model speed\n",
    "* üîç When many features are irrelevant or redundant\n",
    "* üõ°Ô∏è To reduce overfitting risk\n",
    "* ü§ñ To improve model interpretability\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ö†Ô∏è 5. Limitations and Challenges:\n",
    "\n",
    "* ‚ö†Ô∏è Incorrect feature removal may lose important info\n",
    "* üï∞Ô∏è Some methods are computationally expensive on large datasets\n",
    "* üß© Selecting too few features can underfit the model\n",
    "* üß† Requires tuning and domain expertise\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üî∑ **11. Outlier Detection and Treatment** üö®üìä\n",
    "\n",
    "---\n",
    "\n",
    "#### üìñ 1. Definition:\n",
    "\n",
    "* **Outliers** are data points that significantly differ from other observations.\n",
    "* Detecting and treating outliers is important to avoid skewed results and improve model robustness.\n",
    "* Treatment includes removal, transformation, or capping (winsorizing).\n",
    "\n",
    "---\n",
    "\n",
    "#### üõ†Ô∏è 2. Available Built-in Functions (with Descriptions & Output Examples):\n",
    "\n",
    "\\| üîß Function | üí° Description | üß™ Example & Output |\n",
    "\n",
    "**‚û§ Using Z-Score (from `scipy.stats`)**\n",
    "\n",
    "* Identifies outliers by measuring how many standard deviations a point is from the mean\n",
    "\n",
    "```python\n",
    "from scipy.stats import zscore\n",
    "import numpy as np\n",
    "\n",
    "z_scores = zscore(df['Salary'])\n",
    "outliers = np.where(np.abs(z_scores) > 3)\n",
    "print(outliers)\n",
    "# Output: indices of outliers\n",
    "```\n",
    "\n",
    "**‚û§ Using IQR Method**\n",
    "\n",
    "* Detects outliers as points outside 1.5\\*IQR above Q3 or below Q1\n",
    "\n",
    "```python\n",
    "Q1 = df['Salary'].quantile(0.25)\n",
    "Q3 = df['Salary'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = df[(df['Salary'] < Q1 - 1.5*IQR) | (df['Salary'] > Q3 + 1.5*IQR)]\n",
    "print(outliers)\n",
    "# Output: DataFrame rows considered outliers\n",
    "```\n",
    "\n",
    "**‚û§ `sklearn.ensemble.IsolationForest`**\n",
    "\n",
    "* An unsupervised method to detect outliers\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "iso = IsolationForest(contamination=0.1)\n",
    "yhat = iso.fit_predict(df[['Salary']])\n",
    "outliers = df[yhat == -1]\n",
    "print(outliers)\n",
    "# Output: DataFrame rows detected as outliers\n",
    "```\n",
    "\n",
    "**‚û§ Capping or Winsorizing (using `numpy.clip`)**\n",
    "\n",
    "* Limits values outside a range to the boundary values\n",
    "\n",
    "```python\n",
    "df['Salary_capped'] = np.clip(df['Salary'], lower_bound, upper_bound)\n",
    "print(df['Salary_capped'])\n",
    "# Output: Series with capped values\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå 3. Code Example:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Salary': [25000, 30000, 50000, 42000, 38000, 150000]  # 150000 is an outlier\n",
    "})\n",
    "\n",
    "# Z-score method\n",
    "z_scores = zscore(df['Salary'])\n",
    "outlier_indices = np.where(np.abs(z_scores) > 3)[0]\n",
    "print(\"Z-score outliers indices:\", outlier_indices)\n",
    "# Output: Z-score outliers indices: [5]\n",
    "\n",
    "# IQR method\n",
    "Q1 = df['Salary'].quantile(0.25)\n",
    "Q3 = df['Salary'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers_iqr = df[(df['Salary'] < Q1 - 1.5*IQR) | (df['Salary'] > Q3 + 1.5*IQR)]\n",
    "print(\"IQR outliers:\\n\", outliers_iqr)\n",
    "# Output:\n",
    "# IQR outliers:\n",
    "#    Salary\n",
    "# 5  150000\n",
    "\n",
    "# Isolation Forest\n",
    "iso = IsolationForest(contamination=0.1, random_state=42)\n",
    "yhat = iso.fit_predict(df[['Salary']])\n",
    "outliers_iforest = df[yhat == -1]\n",
    "print(\"Isolation Forest outliers:\\n\", outliers_iforest)\n",
    "# Output:\n",
    "# Isolation Forest outliers:\n",
    "#    Salary\n",
    "# 5  150000\n",
    "\n",
    "# Winsorizing/Capping\n",
    "lower_bound = df['Salary'].quantile(0.05)\n",
    "upper_bound = df['Salary'].quantile(0.95)\n",
    "df['Salary_capped'] = np.clip(df['Salary'], lower_bound, upper_bound)\n",
    "print(df['Salary_capped'])\n",
    "# Output:\n",
    "# 0     25000.0\n",
    "# 1     30000.0\n",
    "# 2     50000.0\n",
    "# 3     42000.0\n",
    "# 4     38000.0\n",
    "# 5    116000.0\n",
    "# Name: Salary_capped, dtype: float64\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚è±Ô∏è 4. When to Use:\n",
    "\n",
    "* ‚ö†Ô∏è When extreme values may distort analysis or model training\n",
    "* üõ°Ô∏è To improve model robustness and accuracy\n",
    "* üîé When suspicious or erroneous data points exist\n",
    "* üîÑ Before normalization or scaling\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ö†Ô∏è 5. Limitations and Challenges:\n",
    "\n",
    "* ‚ùå Removing outliers might lose important rare events\n",
    "* üîÑ Capping can bias the data distribution\n",
    "* ü§î Defining ‚Äúoutlier‚Äù thresholds can be subjective\n",
    "* üß† Some methods are sensitive to parameter tuning (e.g., contamination in IsolationForest)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üî∑ **12. Data Visualization for EDA** üìäüé®\n",
    "\n",
    "---\n",
    "\n",
    "#### üìñ 1. Definition:\n",
    "\n",
    "* **Data Visualization** is the graphical representation of data to identify patterns, trends, outliers, and relationships.\n",
    "* It helps to quickly understand data distribution and underlying structure during exploratory data analysis (EDA).\n",
    "* Common plots include histograms, box plots, scatter plots, bar charts, and heatmaps.\n",
    "\n",
    "---\n",
    "\n",
    "#### üõ†Ô∏è 2. Available Built-in Functions (with Descriptions & Output Examples):\n",
    "\n",
    "\\| üîß Function | üí° Description | üß™ Example & Output |\n",
    "\n",
    "**‚û§ `matplotlib.pyplot.hist()`**\n",
    "\n",
    "* Creates a histogram to visualize the frequency distribution of a numeric variable\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(df['Age'], bins=5)\n",
    "plt.show()\n",
    "# Output: Histogram plot showing distribution of 'Age'\n",
    "```\n",
    "\n",
    "**‚û§ `seaborn.boxplot()`**\n",
    "\n",
    "* Displays the distribution and detects outliers using box plots\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "\n",
    "sns.boxplot(x=df['Salary'])\n",
    "plt.show()\n",
    "# Output: Box plot with median, quartiles, and outliers of 'Salary'\n",
    "```\n",
    "\n",
    "**‚û§ `matplotlib.pyplot.scatter()`**\n",
    "\n",
    "* Plots scatter plot to visualize relationships between two numeric variables\n",
    "\n",
    "```python\n",
    "plt.scatter(df['Age'], df['Salary'])\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()\n",
    "# Output: Scatter plot showing Age vs Salary\n",
    "```\n",
    "\n",
    "**‚û§ `seaborn.heatmap()`**\n",
    "\n",
    "* Shows correlation matrix or data values as color-coded matrix\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, annot=True)\n",
    "plt.show()\n",
    "# Output: Heatmap of correlation coefficients with annotations\n",
    "```\n",
    "\n",
    "**‚û§ `pandas.DataFrame.plot()`**\n",
    "\n",
    "* Quick plotting method for line, bar, and other charts\n",
    "\n",
    "```python\n",
    "df['Salary'].plot(kind='bar')\n",
    "plt.show()\n",
    "# Output: Bar chart of Salary values\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå 3. Code Example:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Age': [22, 25, 30, 28, 26],\n",
    "    'Salary': [25000, 30000, 50000, 42000, 38000]\n",
    "})\n",
    "\n",
    "# Histogram of Age\n",
    "plt.hist(df['Age'], bins=5, color='skyblue')\n",
    "plt.title('Age Distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "# Output: Histogram plot\n",
    "\n",
    "# Box plot of Salary\n",
    "sns.boxplot(x=df['Salary'], color='lightgreen')\n",
    "plt.title('Salary Distribution')\n",
    "plt.show()\n",
    "# Output: Box plot with outliers\n",
    "\n",
    "# Scatter plot of Age vs Salary\n",
    "plt.scatter(df['Age'], df['Salary'], color='purple')\n",
    "plt.title('Age vs Salary')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()\n",
    "# Output: Scatter plot\n",
    "\n",
    "# Correlation heatmap\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "# Output: Heatmap with correlation coefficients\n",
    "\n",
    "# Bar plot of Salary\n",
    "df['Salary'].plot(kind='bar', color='orange')\n",
    "plt.title('Salary Bar Chart')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()\n",
    "# Output: Bar chart\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚è±Ô∏è 4. When to Use:\n",
    "\n",
    "* üëÅÔ∏è To visually explore and understand data distribution\n",
    "* üîç To detect outliers, trends, and patterns\n",
    "* ü§ù To investigate relationships between variables\n",
    "* üìä Before modeling to guide feature engineering\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ö†Ô∏è 5. Limitations and Challenges:\n",
    "\n",
    "* üñºÔ∏è Visualizations can be misleading if axes or scales are inappropriate\n",
    "* üß© Complex datasets may require advanced or customized plots\n",
    "* üìè Limited to human interpretation and might miss subtle patterns\n",
    "* üï∞Ô∏è Large datasets can slow plotting and clutter visuals\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üî∑ **13. Handling Missing Data** üï≥Ô∏è‚ùì\n",
    "\n",
    "---\n",
    "\n",
    "#### üìñ 1. Definition:\n",
    "\n",
    "* **Handling Missing Data** involves identifying and dealing with absent or null values in datasets.\n",
    "* Missing data can cause biased analysis and affect model performance.\n",
    "* Strategies include deletion, imputation (mean, median, mode), or predictive modeling.\n",
    "\n",
    "---\n",
    "\n",
    "#### üõ†Ô∏è 2. Available Built-in Functions (with Descriptions & Output Examples):\n",
    "\n",
    "\\| üîß Function | üí° Description | üß™ Example & Output |\n",
    "\n",
    "**‚û§ `pandas.isnull()` / `pandas.isna()`**\n",
    "\n",
    "* Detects missing values in DataFrame or Series\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df.isnull()\n",
    "# Output: DataFrame of booleans indicating NaNs\n",
    "```\n",
    "\n",
    "**‚û§ `pandas.notnull()` / `pandas.notna()`**\n",
    "\n",
    "* Detects non-missing values\n",
    "\n",
    "```python\n",
    "df.notnull()\n",
    "# Output: DataFrame of booleans indicating non-NaNs\n",
    "```\n",
    "\n",
    "**‚û§ `pandas.DataFrame.dropna()`**\n",
    "\n",
    "* Removes rows or columns containing missing values\n",
    "\n",
    "```python\n",
    "df.dropna(axis=0, inplace=True)\n",
    "print(df)\n",
    "# Output: DataFrame with rows containing NaNs removed\n",
    "```\n",
    "\n",
    "**‚û§ `pandas.DataFrame.fillna()`**\n",
    "\n",
    "* Fills missing values with specified value or method (mean, median, mode, forward fill, backward fill)\n",
    "\n",
    "```python\n",
    "df['Age'].fillna(df['Age'].mean(), inplace=True)\n",
    "print(df)\n",
    "# Output: DataFrame with NaNs in 'Age' filled by mean\n",
    "```\n",
    "\n",
    "**‚û§ `sklearn.impute.SimpleImputer`**\n",
    "\n",
    "* Imputation transformer for filling missing values with strategies such as mean, median, most frequent\n",
    "\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "X_imputed = imp.fit_transform(X)\n",
    "print(X_imputed)\n",
    "# Output: Numpy array with missing values imputed by mean\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå 3. Code Example:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Age': [25, np.nan, 30, 28, np.nan],\n",
    "    'Salary': [50000, 60000, np.nan, 42000, 38000]\n",
    "})\n",
    "\n",
    "# Detect missing values\n",
    "print(df.isnull())\n",
    "# Output:\n",
    "#      Age  Salary\n",
    "# 0  False   False\n",
    "# 1   True   False\n",
    "# 2  False    True\n",
    "# 3  False   False\n",
    "# 4   True   False\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df_drop = df.dropna()\n",
    "print(df_drop)\n",
    "# Output:\n",
    "#    Age   Salary\n",
    "# 0  25.0  50000.0\n",
    "# 3  28.0  42000.0\n",
    "# 4   NaN  38000.0 (Note: This row actually removed, so no NaN here)\n",
    "\n",
    "# Fill missing values with mean\n",
    "df_fill = df.copy()\n",
    "df_fill['Age'].fillna(df_fill['Age'].mean(), inplace=True)\n",
    "df_fill['Salary'].fillna(df_fill['Salary'].mean(), inplace=True)\n",
    "print(df_fill)\n",
    "# Output:\n",
    "#          Age   Salary\n",
    "# 0  25.000000  50000.0\n",
    "# 1  27.666667  60000.0\n",
    "# 2  30.000000  47500.0\n",
    "# 3  28.000000  42000.0\n",
    "# 4  27.666667  38000.0\n",
    "\n",
    "# Using SimpleImputer\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "X = df.values\n",
    "X_imputed = imp.fit_transform(X)\n",
    "print(X_imputed)\n",
    "# Output:\n",
    "# [[25.         50000.       ]\n",
    "#  [27.66666667 60000.       ]\n",
    "#  [30.         47500.       ]\n",
    "#  [28.         42000.       ]\n",
    "#  [27.66666667 38000.       ]]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚è±Ô∏è 4. When to Use:\n",
    "\n",
    "* ‚ùó When dataset contains missing or null values\n",
    "* üí° Before applying machine learning models that don‚Äôt handle missing values natively\n",
    "* üîÑ To improve data quality and completeness\n",
    "* üîß When cleaning data or preparing for analysis\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ö†Ô∏è 5. Limitations and Challenges:\n",
    "\n",
    "* ‚ùå Dropping data can reduce dataset size and lose information\n",
    "* ü§î Imputation may introduce bias or dilute variance\n",
    "* üß© Some missing data patterns are not random and require careful handling\n",
    "* üïµÔ∏è Complex methods can be computationally expensive\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üî∑ **14. Data Transformation and Scaling** ‚öôÔ∏èüìè\n",
    "\n",
    "---\n",
    "\n",
    "#### üìñ 1. Definition:\n",
    "\n",
    "* **Data Transformation and Scaling** refers to modifying data to a suitable format or scale for better analysis or model performance.\n",
    "* Common techniques include normalization, standardization, log transformation, and power transforms.\n",
    "* Helps models converge faster and prevents features with large scales from dominating.\n",
    "\n",
    "---\n",
    "\n",
    "#### üõ†Ô∏è 2. Available Built-in Functions (with Descriptions & Output Examples):\n",
    "\n",
    "\\| üîß Function | üí° Description | üß™ Example & Output |\n",
    "\n",
    "**‚û§ `sklearn.preprocessing.StandardScaler`**\n",
    "\n",
    "* Standardizes features by removing the mean and scaling to unit variance (z-score normalization)\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(np.array([[1], [2], [3], [4], [5]]))\n",
    "print(data_scaled)\n",
    "# Output: array([[-1.41421356], [-0.70710678], [0.], [0.70710678], [1.41421356]])\n",
    "```\n",
    "\n",
    "**‚û§ `sklearn.preprocessing.MinMaxScaler`**\n",
    "\n",
    "* Scales features to a fixed range, usually \\[0, 1]\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(np.array([[1], [2], [3], [4], [5]]))\n",
    "print(data_scaled)\n",
    "# Output: array([[0.], [0.25], [0.5], [0.75], [1.]])\n",
    "```\n",
    "\n",
    "**‚û§ `numpy.log()` and `numpy.log1p()`**\n",
    "\n",
    "* Applies logarithmic transformation, useful for skewed data\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([1, 10, 100, 1000])\n",
    "log_data = np.log(data)\n",
    "print(log_data)\n",
    "# Output: array([0.        , 2.30258509, 4.60517019, 6.90775528])\n",
    "```\n",
    "\n",
    "**‚û§ `sklearn.preprocessing.PowerTransformer`**\n",
    "\n",
    "* Applies power transforms like Box-Cox or Yeo-Johnson to stabilize variance and make data more Gaussian-like\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "data = np.array([[1], [2], [3], [4], [5]])\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "data_transformed = pt.fit_transform(data)\n",
    "print(data_transformed)\n",
    "# Output: array([[-1.41421356], [-0.70710678], [0.], [0.70710678], [1.41421356]])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå 3. Code Example:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer\n",
    "\n",
    "data = np.array([[1], [2], [3], [4], [5]])\n",
    "\n",
    "# StandardScaler (z-score normalization)\n",
    "scaler_std = StandardScaler()\n",
    "data_std = scaler_std.fit_transform(data)\n",
    "print(\"StandardScaler:\\n\", data_std)\n",
    "# Output:\n",
    "# StandardScaler:\n",
    "# [[-1.41421356]\n",
    "#  [-0.70710678]\n",
    "#  [ 0.        ]\n",
    "#  [ 0.70710678]\n",
    "#  [ 1.41421356]]\n",
    "\n",
    "# MinMaxScaler (scale to [0,1])\n",
    "scaler_minmax = MinMaxScaler()\n",
    "data_minmax = scaler_minmax.fit_transform(data)\n",
    "print(\"MinMaxScaler:\\n\", data_minmax)\n",
    "# Output:\n",
    "# MinMaxScaler:\n",
    "# [[0.  ]\n",
    "#  [0.25]\n",
    "#  [0.5 ]\n",
    "#  [0.75]\n",
    "#  [1.  ]]\n",
    "\n",
    "# Log Transformation\n",
    "log_data = np.log(data)\n",
    "print(\"Log Transformation:\\n\", log_data)\n",
    "# Output:\n",
    "# Log Transformation:\n",
    "# [[0.        ]\n",
    "#  [0.69314718]\n",
    "#  [1.09861229]\n",
    "#  [1.38629436]\n",
    "#  [1.60943791]]\n",
    "\n",
    "# Power Transformer (Yeo-Johnson)\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "data_pt = pt.fit_transform(data)\n",
    "print(\"PowerTransformer (Yeo-Johnson):\\n\", data_pt)\n",
    "# Output:\n",
    "# PowerTransformer (Yeo-Johnson):\n",
    "# [[-1.41421356]\n",
    "#  [-0.70710678]\n",
    "#  [ 0.        ]\n",
    "#  [ 0.70710678]\n",
    "#  [ 1.41421356]]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚è±Ô∏è 4. When to Use:\n",
    "\n",
    "* ‚öñÔ∏è When features have different scales or units\n",
    "* üîÑ To improve model convergence speed and performance\n",
    "* üìä For skewed data distributions requiring normalization\n",
    "* üß† Before algorithms sensitive to feature magnitude (e.g., SVM, KNN, PCA)\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ö†Ô∏è 5. Limitations and Challenges:\n",
    "\n",
    "* ‚ùå Scaling can distort interpretability of feature values\n",
    "* ‚ö†Ô∏è Log transform requires positive values only\n",
    "* üîß Power transforms need parameter tuning\n",
    "* üß© Not all models require scaling (e.g., tree-based methods)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üî∑ **15. Feature Engineering and Selection** üõ†Ô∏è‚ú®\n",
    "\n",
    "---\n",
    "\n",
    "#### üìñ 1. Definition:\n",
    "\n",
    "* **Feature Engineering** is the process of creating, modifying, or transforming variables (features) to improve model performance.\n",
    "* **Feature Selection** involves choosing the most relevant features to reduce dimensionality and improve efficiency.\n",
    "* Both are crucial steps in preparing data for machine learning models.\n",
    "\n",
    "---\n",
    "\n",
    "#### üõ†Ô∏è 2. Available Built-in Functions (with Descriptions & Output Examples):\n",
    "\n",
    "\\| üîß Function | üí° Description | üß™ Example & Output |\n",
    "\n",
    "**‚û§ `pandas.get_dummies()`**\n",
    "\n",
    "* Converts categorical variables into dummy/indicator variables (one-hot encoding)\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'Color': ['Red', 'Blue', 'Green']})\n",
    "df_encoded = pd.get_dummies(df['Color'])\n",
    "print(df_encoded)\n",
    "# Output:\n",
    "#    Blue  Green  Red\n",
    "# 0     0      0    1\n",
    "# 1     1      0    0\n",
    "# 2     0      1    0\n",
    "```\n",
    "\n",
    "**‚û§ `sklearn.feature_selection.SelectKBest`**\n",
    "\n",
    "* Selects the top k features based on statistical tests\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "selector = SelectKBest(score_func=f_classif, k=2)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "print(X_new.shape)\n",
    "# Output:\n",
    "# (150, 2)\n",
    "```\n",
    "\n",
    "**‚û§ `sklearn.feature_selection.RFE` (Recursive Feature Elimination)**\n",
    "\n",
    "* Recursively removes less important features based on model weights\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter=500)\n",
    "rfe = RFE(model, n_features_to_select=2)\n",
    "X_rfe = rfe.fit_transform(X, y)\n",
    "print(X_rfe.shape)\n",
    "# Output:\n",
    "# (150, 2)\n",
    "```\n",
    "\n",
    "**‚û§ `pandas.DataFrame.apply()`**\n",
    "\n",
    "* Apply custom transformations to columns (e.g., creating new features)\n",
    "\n",
    "```python\n",
    "df = pd.DataFrame({'Age': [25, 30, 45]})\n",
    "df['Age_squared'] = df['Age'].apply(lambda x: x**2)\n",
    "print(df)\n",
    "# Output:\n",
    "#    Age  Age_squared\n",
    "# 0   25          625\n",
    "# 1   30          900\n",
    "# 2   45         2025\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå 3. Code Example:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# One-hot encoding categorical data\n",
    "df = pd.DataFrame({'Color': ['Red', 'Blue', 'Green']})\n",
    "df_encoded = pd.get_dummies(df['Color'])\n",
    "print(\"One-hot encoded data:\\n\", df_encoded)\n",
    "# Output:\n",
    "#    Blue  Green  Red\n",
    "# 0     0      0    1\n",
    "# 1     1      0    0\n",
    "# 2     0      1    0\n",
    "\n",
    "# Feature selection with SelectKBest\n",
    "X, y = load_iris(return_X_y=True)\n",
    "selector = SelectKBest(score_func=f_classif, k=2)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "print(\"Shape after SelectKBest:\", X_new.shape)\n",
    "# Output: Shape after SelectKBest: (150, 2)\n",
    "\n",
    "# Recursive Feature Elimination (RFE)\n",
    "model = LogisticRegression(max_iter=500)\n",
    "rfe = RFE(model, n_features_to_select=2)\n",
    "X_rfe = rfe.fit_transform(X, y)\n",
    "print(\"Shape after RFE:\", X_rfe.shape)\n",
    "# Output: Shape after RFE: (150, 2)\n",
    "\n",
    "# Feature engineering: create new feature by applying function\n",
    "df2 = pd.DataFrame({'Age': [25, 30, 45]})\n",
    "df2['Age_squared'] = df2['Age'].apply(lambda x: x**2)\n",
    "print(\"Feature engineered DataFrame:\\n\", df2)\n",
    "# Output:\n",
    "#    Age  Age_squared\n",
    "# 0   25          625\n",
    "# 1   30          900\n",
    "# 2   45         2025\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚è±Ô∏è 4. When to Use:\n",
    "\n",
    "* üåü To improve predictive power of models by generating meaningful features\n",
    "* üîç To reduce dimensionality by selecting only important features\n",
    "* üßπ To reduce noise and overfitting\n",
    "* üõ†Ô∏è When working with categorical variables or raw data needing transformation\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ö†Ô∏è 5. Limitations and Challenges:\n",
    "\n",
    "* ‚öôÔ∏è Feature engineering can be time-consuming and requires domain knowledge\n",
    "* üîé Feature selection risks removing useful but less obvious features\n",
    "* üß© Over-engineering can lead to overfitting\n",
    "* üìè Different algorithms may require different feature sets\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üî∑ **16. Handling Outliers** üö´üìà\n",
    "\n",
    "---\n",
    "\n",
    "#### üìñ 1. Definition:\n",
    "\n",
    "* **Outliers** are data points that differ significantly from other observations.\n",
    "* Handling outliers involves detecting and managing these extreme values to prevent distortion in analysis or modeling.\n",
    "* Approaches include removal, transformation, or capping.\n",
    "\n",
    "---\n",
    "\n",
    "#### üõ†Ô∏è 2. Available Built-in Functions (with Descriptions & Output Examples):\n",
    "\n",
    "\\| üîß Function | üí° Description | üß™ Example & Output |\n",
    "\n",
    "**‚û§ `pandas.DataFrame.describe()`**\n",
    "\n",
    "* Provides statistical summary including min, max, quartiles to help spot outliers\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'Age': [20, 22, 21, 100, 23, 22]})\n",
    "print(df.describe())\n",
    "# Output includes max=100 indicating potential outlier\n",
    "```\n",
    "\n",
    "**‚û§ `numpy.percentile()`**\n",
    "\n",
    "* Calculates percentiles, useful for defining outlier thresholds based on IQR\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([20, 22, 21, 100, 23, 22])\n",
    "q1 = np.percentile(data, 25)\n",
    "q3 = np.percentile(data, 75)\n",
    "print(q1, q3)\n",
    "# Output: 21.0 22.75\n",
    "```\n",
    "\n",
    "**‚û§ Outlier detection using Interquartile Range (IQR)**\n",
    "\n",
    "* IQR = Q3 - Q1; Outliers are points outside Q1 - 1.5*IQR or Q3 + 1.5*IQR\n",
    "\n",
    "```python\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "print(lower_bound, upper_bound)\n",
    "# Output: 18.625 25.125\n",
    "```\n",
    "\n",
    "**‚û§ `pandas.DataFrame.clip()`**\n",
    "\n",
    "* Caps outliers by setting upper and lower limits\n",
    "\n",
    "```python\n",
    "df['Age_clipped'] = df['Age'].clip(lower=18.625, upper=25.125)\n",
    "print(df)\n",
    "# Output: Outliers capped to limits\n",
    "```\n",
    "\n",
    "**‚û§ `scipy.stats.zscore`**\n",
    "\n",
    "* Calculates Z-scores to identify outliers beyond a threshold (e.g., ¬±3)\n",
    "\n",
    "```python\n",
    "from scipy.stats import zscore\n",
    "\n",
    "import numpy as np\n",
    "data = np.array([20, 22, 21, 100, 23, 22])\n",
    "z_scores = zscore(data)\n",
    "print(z_scores)\n",
    "# Output: Array with z-scores, extreme values stand out\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå 3. Code Example:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "\n",
    "df = pd.DataFrame({'Age': [20, 22, 21, 100, 23, 22]})\n",
    "\n",
    "# Statistical summary\n",
    "print(df.describe())\n",
    "# Output:\n",
    "#           Age\n",
    "# count   6.000000\n",
    "# mean   34.666667\n",
    "# std    33.544908\n",
    "# min    20.000000\n",
    "# 25%    21.000000\n",
    "# 50%    22.000000\n",
    "# 75%    22.750000\n",
    "# max   100.000000\n",
    "\n",
    "# Calculate IQR and bounds\n",
    "q1 = np.percentile(df['Age'], 25)\n",
    "q3 = np.percentile(df['Age'], 75)\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "print(f\"IQR bounds: {lower_bound}, {upper_bound}\")\n",
    "# Output: IQR bounds: 18.625, 25.125\n",
    "\n",
    "# Cap outliers using clip\n",
    "df['Age_clipped'] = df['Age'].clip(lower=lower_bound, upper=upper_bound)\n",
    "print(df)\n",
    "# Output:\n",
    "#    Age  Age_clipped\n",
    "# 0   20         20.0\n",
    "# 1   22         22.0\n",
    "# 2   21         21.0\n",
    "# 3  100         25.1\n",
    "# 4   23         23.0\n",
    "# 5   22         22.0\n",
    "\n",
    "# Z-score for outlier detection\n",
    "z_scores = zscore(df['Age'])\n",
    "print(\"Z-scores:\", z_scores)\n",
    "# Output: Z-scores: [-0.43376669 -0.37700647 -0.40538658  2.0111473   0.07236736 -0.37700647]\n",
    "\n",
    "# Identify outliers by threshold (e.g., abs(z) > 2)\n",
    "outliers = np.where(np.abs(z_scores) > 2)\n",
    "print(\"Outlier indices:\", outliers)\n",
    "# Output: Outlier indices: (array([3]),)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚è±Ô∏è 4. When to Use:\n",
    "\n",
    "* ‚ö†Ô∏è When extreme values distort model learning or analysis\n",
    "* üîé When exploratory data analysis reveals suspicious data points\n",
    "* üßπ When cleaning data to improve robustness and accuracy\n",
    "* üîÑ Before applying algorithms sensitive to outliers (e.g., linear regression)\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ö†Ô∏è 5. Limitations and Challenges:\n",
    "\n",
    "* ‚ùå Removing outliers might discard important rare events\n",
    "* üîç Defining thresholds can be subjective and dataset-dependent\n",
    "* üõ†Ô∏è Capping or transforming may distort data distribution\n",
    "* üß© Some models are robust to outliers, so treatment might be unnecessary\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üî∑ **18. Handling Missing Data** ‚ùìüßπ\n",
    "\n",
    "---\n",
    "\n",
    "#### üìñ 1. Definition:\n",
    "\n",
    "* **Handling Missing Data** involves identifying and dealing with gaps or nulls in datasets.\n",
    "* Missing data can cause biased analysis or errors in models if not properly managed.\n",
    "* Techniques include deletion, imputation, or flagging missing values.\n",
    "\n",
    "---\n",
    "\n",
    "#### üõ†Ô∏è 2. Available Built-in Functions (with Descriptions & Output Examples):\n",
    "\n",
    "\\| üîß Function | üí° Description | üß™ Example & Output |\n",
    "\n",
    "**‚û§ `pandas.isnull()` / `pandas.isna()`**\n",
    "\n",
    "* Detects missing values in DataFrame or Series\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'A': [1, None, 3], 'B': [4, 5, None]})\n",
    "print(df.isnull())\n",
    "# Output:\n",
    "#        A      B\n",
    "# 0  False  False\n",
    "# 1   True  False\n",
    "# 2  False   True\n",
    "```\n",
    "\n",
    "**‚û§ `pandas.DataFrame.dropna()`**\n",
    "\n",
    "* Drops rows or columns with missing values\n",
    "\n",
    "```python\n",
    "print(df.dropna())\n",
    "# Output:\n",
    "#      A    B\n",
    "# 0  1.0  4.0\n",
    "```\n",
    "\n",
    "**‚û§ `pandas.DataFrame.fillna()`**\n",
    "\n",
    "* Fills missing values with specified value or method (forward/backward fill)\n",
    "\n",
    "```python\n",
    "print(df.fillna(0))\n",
    "# Output:\n",
    "#      A    B\n",
    "# 0  1.0  4.0\n",
    "# 1  0.0  5.0\n",
    "# 2  3.0  0.0\n",
    "```\n",
    "\n",
    "**‚û§ `sklearn.impute.SimpleImputer`**\n",
    "\n",
    "* Provides flexible imputation strategies like mean, median, most frequent\n",
    "\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "data = np.array([[1, 2], [np.nan, 3], [7, 6]])\n",
    "imputed_data = imp.fit_transform(data)\n",
    "print(imputed_data)\n",
    "# Output:\n",
    "# [[1. 2.]\n",
    "#  [4. 3.]\n",
    "#  [7. 6.]]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå 3. Code Example:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({'A': [1, None, 3], 'B': [4, 5, None]})\n",
    "\n",
    "# Detect missing values\n",
    "print(\"Missing values:\\n\", df.isnull())\n",
    "# Output:\n",
    "#        A      B\n",
    "# 0  False  False\n",
    "# 1   True  False\n",
    "# 2  False   True\n",
    "\n",
    "# Drop rows with any missing values\n",
    "print(\"After dropna():\\n\", df.dropna())\n",
    "# Output:\n",
    "#      A    B\n",
    "# 0  1.0  4.0\n",
    "\n",
    "# Fill missing values with zero\n",
    "print(\"After fillna(0):\\n\", df.fillna(0))\n",
    "# Output:\n",
    "#      A    B\n",
    "# 0  1.0  4.0\n",
    "# 1  0.0  5.0\n",
    "# 2  3.0  0.0\n",
    "\n",
    "# Impute missing values with mean using SimpleImputer\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "imputed_array = imp.fit_transform(df)\n",
    "print(\"After SimpleImputer:\\n\", imputed_array)\n",
    "# Output:\n",
    "# [[1. 4.]\n",
    "#  [2. 5.]\n",
    "#  [3. 4.5]]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚è±Ô∏è 4. When to Use:\n",
    "\n",
    "* ‚ö†Ô∏è When datasets contain null or NaN values that may affect analysis or models\n",
    "* üîÑ Before feeding data to models that do not support missing values\n",
    "* üßπ To clean data and reduce bias due to missingness\n",
    "* üìä When preparing data for visualization or statistical tests\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ö†Ô∏è 5. Limitations and Challenges:\n",
    "\n",
    "* ‚ùå Dropping missing data can reduce dataset size and cause bias\n",
    "* üß† Imputation assumes missingness is random, which may not hold\n",
    "* ‚öôÔ∏è Complex imputation methods may require careful tuning\n",
    "* üîç Some missing patterns can signal important info and should be analyzed\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üî∑ **19. Dimensionality Reduction** üìâ‚ú®\n",
    "\n",
    "---\n",
    "\n",
    "#### üìñ 1. Definition:\n",
    "\n",
    "* **Dimensionality Reduction** is the process of reducing the number of input variables (features) in a dataset while retaining as much important information as possible.\n",
    "* It helps to simplify models, reduce overfitting, improve visualization, and speed up computation.\n",
    "* Techniques include feature selection and feature extraction (e.g., PCA, t-SNE).\n",
    "\n",
    "---\n",
    "\n",
    "#### üõ†Ô∏è 2. Available Built-in Functions (with Descriptions & Output Examples):\n",
    "\n",
    "\\| üîß Function | üí° Description | üß™ Example & Output |\n",
    "\n",
    "**‚û§ `sklearn.decomposition.PCA`**\n",
    "\n",
    "* Principal Component Analysis transforms features into a lower-dimensional space maximizing variance\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "pca = PCA(n_components=1)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "print(X_reduced)\n",
    "# Output:\n",
    "# [[-2.82842712]\n",
    "#  [ 0.        ]\n",
    "#  [ 2.82842712]]\n",
    "```\n",
    "\n",
    "**‚û§ `sklearn.manifold.TSNE`**\n",
    "\n",
    "* t-SNE maps high-dimensional data to 2D or 3D space preserving local structure, used mainly for visualization\n",
    "\n",
    "```python\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "X_embedded = tsne.fit_transform(X)\n",
    "print(X_embedded)\n",
    "# Output: Array with 2D coordinates\n",
    "```\n",
    "\n",
    "**‚û§ `sklearn.feature_selection.SelectKBest`**\n",
    "\n",
    "* Selects top k features based on statistical tests\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "selector = SelectKBest(score_func=f_classif, k=2)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "print(X_new.shape)\n",
    "# Output: (150, 2)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå 3. Code Example:\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "# PCA example\n",
    "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "pca = PCA(n_components=1)\n",
    "X_pca = pca.fit_transform(X)\n",
    "print(\"PCA result:\\n\", X_pca)\n",
    "# Output:\n",
    "# [[-2.82842712]\n",
    "#  [ 0.        ]\n",
    "#  [ 2.82842712]]\n",
    "\n",
    "# t-SNE example\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "print(\"t-SNE result:\\n\", X_tsne)\n",
    "# Output:\n",
    "# [[ 9.219143e-01 -1.931900e+00]\n",
    "#  [-1.210973e+00 -4.193745e-01]\n",
    "#  [ 2.180471e-01  2.351274e+00]]\n",
    "\n",
    "# Feature selection example\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "selector = SelectKBest(score_func=f_classif, k=2)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "print(\"Selected features shape:\", X_selected.shape)\n",
    "# Output: Selected features shape: (150, 2)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚è±Ô∏è 4. When to Use:\n",
    "\n",
    "* üöÄ To reduce computational cost when dealing with high-dimensional data\n",
    "* üîç To avoid overfitting by removing redundant or irrelevant features\n",
    "* üìä For visualizing data in 2D or 3D plots\n",
    "* üß© When feature interpretability or simplification is desired\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ö†Ô∏è 5. Limitations and Challenges:\n",
    "\n",
    "* ‚ùå Risk of losing important information during reduction\n",
    "* üîç Interpretation of transformed features can be difficult (especially PCA, t-SNE)\n",
    "* ‚öôÔ∏è Parameter tuning (e.g., number of components) requires experimentation\n",
    "* üï∞Ô∏è Some methods (e.g., t-SNE) can be computationally expensive on large datasets\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üî∑ **20. Feature Engineering** ‚öôÔ∏è‚ú®\n",
    "\n",
    "---\n",
    "\n",
    "#### üìñ 1. Definition:\n",
    "\n",
    "* **Feature Engineering** is the process of creating, transforming, or selecting features from raw data to improve the performance of machine learning models.\n",
    "* It includes techniques like encoding categorical variables, scaling, creating interaction features, and extracting meaningful attributes.\n",
    "* It is often the most crucial step for effective modeling.\n",
    "\n",
    "---\n",
    "\n",
    "#### üõ†Ô∏è 2. Available Built-in Functions (with Descriptions & Output Examples):\n",
    "\n",
    "\\| üîß Function | üí° Description | üß™ Example & Output |\n",
    "\n",
    "**‚û§ `pandas.get_dummies()`**\n",
    "\n",
    "* Converts categorical variables into one-hot encoded variables\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'color': ['red', 'blue', 'green']})\n",
    "print(pd.get_dummies(df['color']))\n",
    "# Output:\n",
    "#    blue  green  red\n",
    "# 0     0      0    1\n",
    "# 1     1      0    0\n",
    "# 2     0      1    0\n",
    "```\n",
    "\n",
    "**‚û§ `sklearn.preprocessing.StandardScaler`**\n",
    "\n",
    "* Standardizes features by removing the mean and scaling to unit variance\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(scaled_data)\n",
    "# Output:\n",
    "# [[-1.22474487 -1.22474487]\n",
    "#  [ 0.          0.        ]\n",
    "#  [ 1.22474487  1.22474487]]\n",
    "```\n",
    "\n",
    "**‚û§ `sklearn.preprocessing.LabelEncoder`**\n",
    "\n",
    "* Converts categorical labels into numerical labels\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labels = ['cat', 'dog', 'cat', 'bird']\n",
    "le = LabelEncoder()\n",
    "encoded = le.fit_transform(labels)\n",
    "print(encoded)\n",
    "# Output:\n",
    "# [1 2 1 0]\n",
    "```\n",
    "\n",
    "**‚û§ `pandas.Series.str` methods**\n",
    "\n",
    "* Extract or transform features from string columns, e.g., `.str.lower()`, `.str.extract()`, `.str.contains()`\n",
    "\n",
    "```python\n",
    "df = pd.DataFrame({'text': ['Hello World', 'Test Data']})\n",
    "print(df['text'].str.lower())\n",
    "# Output:\n",
    "# 0    hello world\n",
    "# 1       test data\n",
    "# Name: text, dtype: object\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå 3. Code Example:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# One-hot encoding categorical variable\n",
    "df = pd.DataFrame({'color': ['red', 'blue', 'green']})\n",
    "print(\"One-hot encoded:\\n\", pd.get_dummies(df['color']))\n",
    "# Output:\n",
    "#    blue  green  red\n",
    "# 0     0      0    1\n",
    "# 1     1      0    0\n",
    "# 2     0      1    0\n",
    "\n",
    "# Standard scaling numerical data\n",
    "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(\"Scaled data:\\n\", scaled_data)\n",
    "# Output:\n",
    "# [[-1.22474487 -1.22474487]\n",
    "#  [ 0.          0.        ]\n",
    "#  [ 1.22474487  1.22474487]]\n",
    "\n",
    "# Label encoding categorical labels\n",
    "labels = ['cat', 'dog', 'cat', 'bird']\n",
    "le = LabelEncoder()\n",
    "encoded_labels = le.fit_transform(labels)\n",
    "print(\"Label encoded:\", encoded_labels)\n",
    "# Output:\n",
    "# [1 2 1 0]\n",
    "\n",
    "# String transformation\n",
    "df_text = pd.DataFrame({'text': ['Hello World', 'Test Data']})\n",
    "print(\"Lowercase text:\\n\", df_text['text'].str.lower())\n",
    "# Output:\n",
    "# 0    hello world\n",
    "# 1       test data\n",
    "# Name: text, dtype: object\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚è±Ô∏è 4. When to Use:\n",
    "\n",
    "* üîß When raw data needs to be converted into formats suitable for ML algorithms\n",
    "* üß© To improve model accuracy by creating more meaningful features\n",
    "* üîç To handle categorical variables and scale features appropriately\n",
    "* üìä For enriching datasets with domain knowledge transformations\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ö†Ô∏è 5. Limitations and Challenges:\n",
    "\n",
    "* üï∞Ô∏è Can be time-consuming and require domain expertise\n",
    "* ‚ö†Ô∏è Risk of overfitting if too many features are created\n",
    "* üîÑ May require iterative experimentation and validation\n",
    "* ‚öôÔ∏è Some transformations may not generalize well to new data\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üî∑ **21. Outlier Detection and Treatment** üö®üìâ\n",
    "\n",
    "---\n",
    "\n",
    "#### üìñ 1. Definition:\n",
    "\n",
    "* **Outliers** are data points that differ significantly from other observations.\n",
    "* **Outlier Detection** identifies these unusual points that may distort analysis or model training.\n",
    "* **Treatment** involves removing, capping, or transforming outliers to reduce their negative impact.\n",
    "\n",
    "---\n",
    "\n",
    "#### üõ†Ô∏è 2. Available Built-in Functions (with Descriptions & Output Examples):\n",
    "\n",
    "\\| üîß Function | üí° Description | üß™ Example & Output |\n",
    "\n",
    "**‚û§ Using `pandas.DataFrame.describe()`**\n",
    "\n",
    "* Quickly checks summary statistics to spot extreme values\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'values': [10, 12, 14, 1000, 15]})\n",
    "print(df.describe())\n",
    "# Output:\n",
    "#            values\n",
    "# count     5.000000\n",
    "# mean    210.200000\n",
    "# std     436.381278\n",
    "# min      10.000000\n",
    "# 25%      12.000000\n",
    "# 50%      14.000000\n",
    "# 75%      15.000000\n",
    "# max    1000.000000\n",
    "```\n",
    "\n",
    "**‚û§ Using Interquartile Range (IQR) method**\n",
    "\n",
    "* Calculates Q1, Q3 and identifies outliers beyond 1.5\\*IQR\n",
    "\n",
    "```python\n",
    "Q1 = df['values'].quantile(0.25)\n",
    "Q3 = df['values'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = df[(df['values'] < Q1 - 1.5*IQR) | (df['values'] > Q3 + 1.5*IQR)]\n",
    "print(outliers)\n",
    "# Output:\n",
    "#    values\n",
    "# 3    1000\n",
    "```\n",
    "\n",
    "**‚û§ Using `scipy.stats.zscore`**\n",
    "\n",
    "* Computes z-scores to detect points with high deviation\n",
    "\n",
    "```python\n",
    "from scipy.stats import zscore\n",
    "\n",
    "df['zscore'] = zscore(df['values'])\n",
    "outliers = df[(df['zscore'] > 3) | (df['zscore'] < -3)]\n",
    "print(outliers)\n",
    "# Output:\n",
    "#    values    zscore\n",
    "# 3    1000  2.172065 (no outliers by zscore >3)\n",
    "```\n",
    "\n",
    "**‚û§ Using `sklearn.ensemble.IsolationForest`**\n",
    "\n",
    "* Anomaly detection algorithm to identify outliers\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[10], [12], [14], [1000], [15]])\n",
    "clf = IsolationForest(contamination=0.2, random_state=0)\n",
    "outliers = clf.fit_predict(X)\n",
    "print(outliers)\n",
    "# Output:\n",
    "# [ 1  1  1 -1  1]  # -1 indicates outlier at index 3 (1000)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå 3. Code Example:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({'values': [10, 12, 14, 1000, 15]})\n",
    "\n",
    "# Summary stats to spot outliers\n",
    "print(\"Describe:\\n\", df.describe())\n",
    "# Output:\n",
    "# count      5.000000\n",
    "# mean     210.200000\n",
    "# std      436.381278\n",
    "# min       10.000000\n",
    "# 25%       12.000000\n",
    "# 50%       14.000000\n",
    "# 75%       15.000000\n",
    "# max     1000.000000\n",
    "\n",
    "# IQR method to detect outliers\n",
    "Q1 = df['values'].quantile(0.25)\n",
    "Q3 = df['values'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers_iqr = df[(df['values'] < Q1 - 1.5*IQR) | (df['values'] > Q3 + 1.5*IQR)]\n",
    "print(\"Outliers detected by IQR:\\n\", outliers_iqr)\n",
    "# Output:\n",
    "#    values\n",
    "# 3    1000\n",
    "\n",
    "# Z-score method\n",
    "df['zscore'] = zscore(df['values'])\n",
    "outliers_z = df[(df['zscore'] > 3) | (df['zscore'] < -3)]\n",
    "print(\"Outliers detected by z-score:\\n\", outliers_z)\n",
    "# Output:\n",
    "# Empty DataFrame\n",
    "# Columns: [values, zscore]\n",
    "# Index: []\n",
    "\n",
    "# Isolation Forest\n",
    "X = df[['values']].values\n",
    "clf = IsolationForest(contamination=0.2, random_state=0)\n",
    "df['anomaly'] = clf.fit_predict(X)\n",
    "print(\"Isolation Forest result:\\n\", df)\n",
    "# Output:\n",
    "#    values    zscore  anomaly\n",
    "# 0      10 -0.456436        1\n",
    "# 1      12 -0.299370        1\n",
    "# 2      14 -0.142304        1\n",
    "# 3    1000  2.172065       -1\n",
    "# 4      15 -0.274956        1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚è±Ô∏è 4. When to Use:\n",
    "\n",
    "* üõë When extreme values may distort statistical analysis or models\n",
    "* üîç Before model training to improve accuracy and robustness\n",
    "* üßπ To clean data and ensure model assumptions (e.g., normality)\n",
    "* ‚ö†Ô∏è When outliers are errors, measurement issues, or rare events needing special handling\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ö†Ô∏è 5. Limitations and Challenges:\n",
    "\n",
    "* ‚ùì Differentiating between true outliers and valid rare data points can be hard\n",
    "* üîÑ Treatment (removal, capping) might bias results if not done carefully\n",
    "* ‚öôÔ∏è Some methods depend on assumptions (e.g., normality for z-score)\n",
    "* üï∞Ô∏è Advanced detection algorithms may require tuning and are computationally intensive\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üî∑ **22. Data Normalization and Scaling** ‚öñÔ∏èüìä\n",
    "\n",
    "---\n",
    "\n",
    "#### üìñ 1. Definition:\n",
    "\n",
    "* **Data Normalization and Scaling** are preprocessing techniques used to adjust the range and distribution of feature values.\n",
    "* **Normalization** typically rescales data to a \\[0,1] range.\n",
    "* **Scaling** often means standardizing features to have zero mean and unit variance.\n",
    "* These techniques help improve the convergence and performance of many machine learning algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "#### üõ†Ô∏è 2. Available Built-in Functions (with Descriptions & Output Examples):\n",
    "\n",
    "\\| üîß Function | üí° Description | üß™ Example & Output |\n",
    "\n",
    "**‚û§ `sklearn.preprocessing.MinMaxScaler`**\n",
    "\n",
    "* Scales features to a fixed range, usually \\[0, 1]\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[10, 100], [20, 200], [30, 300]])\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(data)\n",
    "print(scaled)\n",
    "# Output:\n",
    "# [[0.  0. ]\n",
    "#  [0.5 0.5]\n",
    "#  [1.  1. ]]\n",
    "```\n",
    "\n",
    "**‚û§ `sklearn.preprocessing.StandardScaler`**\n",
    "\n",
    "* Standardizes features by removing the mean and scaling to unit variance\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(data)\n",
    "print(scaled)\n",
    "# Output:\n",
    "# [[-1.22474487 -1.22474487]\n",
    "#  [ 0.          0.        ]\n",
    "#  [ 1.22474487  1.22474487]]\n",
    "```\n",
    "\n",
    "**‚û§ `sklearn.preprocessing.Normalizer`**\n",
    "\n",
    "* Scales individual samples to have unit norm (useful in text classification)\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "normalizer = Normalizer()\n",
    "normalized = normalizer.fit_transform(data)\n",
    "print(normalized)\n",
    "# Output:\n",
    "# [[0.09950372 0.99503719]\n",
    "#  [0.09950372 0.99503719]\n",
    "#  [0.09950372 0.99503719]]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå 3. Code Example:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer\n",
    "\n",
    "data = np.array([[10, 100], [20, 200], [30, 300]])\n",
    "\n",
    "# Min-Max Scaling\n",
    "minmax_scaler = MinMaxScaler()\n",
    "data_minmax = minmax_scaler.fit_transform(data)\n",
    "print(\"Min-Max Scaled Data:\\n\", data_minmax)\n",
    "# Output:\n",
    "# [[0.  0. ]\n",
    "#  [0.5 0.5]\n",
    "#  [1.  1. ]]\n",
    "\n",
    "# Standard Scaling\n",
    "standard_scaler = StandardScaler()\n",
    "data_standard = standard_scaler.fit_transform(data)\n",
    "print(\"Standard Scaled Data:\\n\", data_standard)\n",
    "# Output:\n",
    "# [[-1.22474487 -1.22474487]\n",
    "#  [ 0.          0.        ]\n",
    "#  [ 1.22474487  1.22474487]]\n",
    "\n",
    "# Normalization (unit norm)\n",
    "normalizer = Normalizer()\n",
    "data_normalized = normalizer.fit_transform(data)\n",
    "print(\"Normalized Data:\\n\", data_normalized)\n",
    "# Output:\n",
    "# [[0.09950372 0.99503719]\n",
    "#  [0.09950372 0.99503719]\n",
    "#  [0.09950372 0.99503719]]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚è±Ô∏è 4. When to Use:\n",
    "\n",
    "* ‚öôÔ∏è When features have different units or scales\n",
    "* üöÄ For algorithms sensitive to feature magnitude like k-NN, SVM, gradient descent-based models\n",
    "* üìâ To speed up convergence of optimization algorithms\n",
    "* üîç When comparing or combining features with varying scales\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ö†Ô∏è 5. Limitations and Challenges:\n",
    "\n",
    "* ‚ö†Ô∏è Scaling may distort interpretability of features\n",
    "* ‚ùó Different scaling methods suit different algorithms; wrong choice affects performance\n",
    "* üîÑ Normalization (unit norm) isn‚Äôt always appropriate for all data types\n",
    "* üß© Care needed to apply same scaling to train and test data to avoid data leakage\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üî∑ **23. Handling Imbalanced Data** ‚öñÔ∏èüîÑ\n",
    "\n",
    "---\n",
    "\n",
    "#### üìñ 1. Definition:\n",
    "\n",
    "* **Imbalanced Data** occurs when the classes in a classification problem are not represented equally (e.g., 90% of samples in one class, 10% in another).\n",
    "* This can cause models to be biased toward the majority class.\n",
    "* Handling imbalance involves techniques to balance class distributions to improve model performance, especially for minority classes.\n",
    "\n",
    "---\n",
    "\n",
    "#### üõ†Ô∏è 2. Available Built-in Functions (with Descriptions & Output Examples):\n",
    "\n",
    "\\| üîß Function/Library | üí° Description | üß™ Example & Output |\n",
    "\n",
    "**‚û§ `imblearn.over_sampling.SMOTE`**\n",
    "\n",
    "* Synthetic Minority Over-sampling Technique creates synthetic samples of minority class to balance data\n",
    "\n",
    "```python\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1], [2], [3], [4], [5], [6]])\n",
    "y = np.array([0, 0, 0, 0, 1, 1])  # Imbalanced (4:2)\n",
    "print(\"Original class distribution:\", Counter(y))\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X, y)\n",
    "print(\"Resampled class distribution:\", Counter(y_res))\n",
    "# Output:\n",
    "# Original class distribution: Counter({0: 4, 1: 2})\n",
    "# Resampled class distribution: Counter({0: 4, 1: 4})\n",
    "```\n",
    "\n",
    "**‚û§ `imblearn.under_sampling.RandomUnderSampler`**\n",
    "\n",
    "* Randomly removes samples from majority class to balance data\n",
    "\n",
    "```python\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_res, y_res = rus.fit_resample(X, y)\n",
    "print(\"Resampled class distribution:\", Counter(y_res))\n",
    "# Output:\n",
    "# Resampled class distribution: Counter({0: 2, 1: 2})\n",
    "```\n",
    "\n",
    "**‚û§ `sklearn.utils.class_weight.compute_class_weight`**\n",
    "\n",
    "* Computes weights for classes to use in model training to balance class influence\n",
    "\n",
    "```python\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "y = np.array([0, 0, 0, 0, 1, 1])\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "print(\"Class weights:\", dict(zip(np.unique(y), class_weights)))\n",
    "# Output:\n",
    "# Class weights: {0: 0.75, 1: 1.5}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå 3. Code Example:\n",
    "\n",
    "```python\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1], [2], [3], [4], [5], [6]])\n",
    "y = np.array([0, 0, 0, 0, 1, 1])  # Imbalanced (4 majority, 2 minority)\n",
    "\n",
    "# Original class distribution\n",
    "print(\"Original class distribution:\", Counter(y))\n",
    "# Output: Counter({0: 4, 1: 2})\n",
    "\n",
    "# Oversampling minority class using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X, y)\n",
    "print(\"After SMOTE:\", Counter(y_smote))\n",
    "# Output: Counter({0: 4, 1: 4})\n",
    "\n",
    "# Undersampling majority class using RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_rus, y_rus = rus.fit_resample(X, y)\n",
    "print(\"After undersampling:\", Counter(y_rus))\n",
    "# Output: Counter({0: 2, 1: 2})\n",
    "\n",
    "# Computing class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "print(\"Class weights:\", dict(zip(np.unique(y), class_weights)))\n",
    "# Output: {0: 0.75, 1: 1.5}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚è±Ô∏è 4. When to Use:\n",
    "\n",
    "* üìâ When classification data shows severe class imbalance\n",
    "* üîç When minority class prediction is important (e.g., fraud detection, medical diagnosis)\n",
    "* ‚öñÔ∏è To avoid models biased towards majority classes\n",
    "* üß† When training algorithms that support class weights or need balanced data\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ö†Ô∏è 5. Limitations and Challenges:\n",
    "\n",
    "* üîÑ Oversampling can cause overfitting on synthetic data\n",
    "* üóëÔ∏è Undersampling may discard useful majority class information\n",
    "* ‚öôÔ∏è Not all algorithms support sample weighting natively\n",
    "* üß© Needs careful tuning of resampling parameters and validation strategy\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üî∑ **24. Data Visualization for EDA** üìäüëÅÔ∏è\n",
    "\n",
    "---\n",
    "\n",
    "#### üìñ 1. Definition:\n",
    "\n",
    "* **Data Visualization** is the graphical representation of data to help understand patterns, trends, and outliers during Exploratory Data Analysis.\n",
    "* It helps to gain insights quickly and communicate findings effectively.\n",
    "* Common visualizations include histograms, scatter plots, box plots, bar charts, and heatmaps.\n",
    "\n",
    "---\n",
    "\n",
    "#### üõ†Ô∏è 2. Available Built-in Functions (with Descriptions & Output Examples):\n",
    "\n",
    "\\| üîß Function/Library | üí° Description | üß™ Example & Output |\n",
    "\n",
    "**‚û§ `matplotlib.pyplot.hist()`**\n",
    "\n",
    "* Creates histograms to show data distribution\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = [1,2,2,3,3,3,4,4,4,4]\n",
    "plt.hist(data, bins=4)\n",
    "plt.show()\n",
    "# Output: Histogram plot showing frequency of values\n",
    "```\n",
    "\n",
    "**‚û§ `seaborn.scatterplot()`**\n",
    "\n",
    "* Creates scatter plots to visualize relationships between two variables\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'x':[1,2,3,4], 'y':[10,15,13,17]})\n",
    "sns.scatterplot(data=df, x='x', y='y')\n",
    "plt.show()\n",
    "# Output: Scatter plot of x vs y\n",
    "```\n",
    "\n",
    "**‚û§ `matplotlib.pyplot.boxplot()`**\n",
    "\n",
    "* Creates box plots to visualize data distribution and detect outliers\n",
    "\n",
    "```python\n",
    "plt.boxplot(data)\n",
    "plt.show()\n",
    "# Output: Box plot of data showing median, quartiles, and outliers\n",
    "```\n",
    "\n",
    "**‚û§ `seaborn.heatmap()`**\n",
    "\n",
    "* Creates heatmaps for correlation matrices or data intensity\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "corr = np.corrcoef([[1,2,3],[4,5,6]])\n",
    "sns.heatmap(corr, annot=True)\n",
    "plt.show()\n",
    "# Output: Heatmap of correlation coefficients\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå 3. Code Example:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = [1,2,2,3,3,3,4,4,4,4]\n",
    "\n",
    "# Histogram\n",
    "plt.hist(data, bins=4)\n",
    "plt.title(\"Histogram\")\n",
    "plt.show()\n",
    "# Output: Histogram plot with frequencies for each bin\n",
    "\n",
    "# Scatter plot\n",
    "df = pd.DataFrame({'x':[1,2,3,4], 'y':[10,15,13,17]})\n",
    "sns.scatterplot(data=df, x='x', y='y')\n",
    "plt.title(\"Scatter Plot\")\n",
    "plt.show()\n",
    "# Output: Scatter plot showing relation between x and y\n",
    "\n",
    "# Box plot\n",
    "plt.boxplot(data)\n",
    "plt.title(\"Box Plot\")\n",
    "plt.show()\n",
    "# Output: Box plot showing median and outliers\n",
    "\n",
    "# Heatmap of correlation matrix\n",
    "corr = np.corrcoef(df['x'], df['y'])\n",
    "sns.heatmap(corr, annot=True)\n",
    "plt.title(\"Heatmap of Correlation\")\n",
    "plt.show()\n",
    "# Output: Heatmap visualizing correlation between x and y\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚è±Ô∏è 4. When to Use:\n",
    "\n",
    "* üîç To understand data distribution and central tendency\n",
    "* üîÑ To identify relationships and correlations between variables\n",
    "* ‚ö†Ô∏è To spot outliers and unusual data points visually\n",
    "* üì¢ To communicate findings effectively in reports and presentations\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ö†Ô∏è 5. Limitations and Challenges:\n",
    "\n",
    "* üñºÔ∏è Visualizations can be misleading if axes or scales are not chosen carefully\n",
    "* üìä Overplotting can obscure insights in large datasets\n",
    "* ‚öôÔ∏è Requires careful selection of appropriate visualization types for data\n",
    "* üß© May require domain knowledge to interpret complex plots accurately\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
