{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### âœ… **Complete List of EDA Topics for Machine Learning**\n",
    "\n",
    "*(Organized step-by-step, from raw data to insights)*\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ§¹ 1. **Data Cleaning**\n",
    "\n",
    "* Handling missing values (imputation, removal)\n",
    "* Handling duplicates\n",
    "* Handling outliers\n",
    "* Type conversions (e.g., object to float)\n",
    "* Dealing with inconsistent formats (dates, currencies, etc.)\n",
    "* String trimming and standardization\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ“ 2. **Data Type Identification & Conversion**\n",
    "\n",
    "* Numerical vs Categorical\n",
    "* Ordinal vs Nominal\n",
    "* Datetime parsing\n",
    "* Encoding (Label, One-hot, Ordinal)\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ“Š 3. **Univariate Analysis**\n",
    "\n",
    "* Summary statistics (mean, median, mode, std, IQR)\n",
    "* Frequency distribution\n",
    "* Value counts\n",
    "* Distribution plots (histogram, KDE, boxplot)\n",
    "* Detecting skewness and kurtosis\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ“ˆ 4. **Bivariate Analysis**\n",
    "\n",
    "* Correlation matrix (Pearson, Spearman)\n",
    "* Scatter plots\n",
    "* Heatmaps\n",
    "* Pair plots\n",
    "* Groupby analysis\n",
    "* Cross-tabulation\n",
    "* Boxplots/grouped boxplots\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ” 5. **Multivariate Analysis**\n",
    "\n",
    "* Multivariate correlation\n",
    "* Pairplots (Seaborn)\n",
    "* FacetGrid analysis\n",
    "* PCA for visualization\n",
    "* Bubble charts\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ“‰ 6. **Outlier Detection**\n",
    "\n",
    "* Z-score\n",
    "* IQR method\n",
    "* Boxplot visual method\n",
    "* Isolation Forest (optional ML method)\n",
    "* Mahalanobis distance\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ§¬ 7. **Feature Distribution Analysis**\n",
    "\n",
    "* Normal vs non-normal distribution\n",
    "* Skewness correction (log, sqrt, Box-Cox)\n",
    "* Visualization: histogram, distplot, violin plot\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ“† 8. **Time Series EDA**\n",
    "\n",
    "* Time-based decomposition\n",
    "* Rolling statistics\n",
    "* Seasonal trends\n",
    "* Lag analysis\n",
    "* Autocorrelation/Partial Autocorrelation\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ“‚ 9. **Categorical Variable Analysis**\n",
    "\n",
    "* Frequency tables\n",
    "* Bar plots / Count plots\n",
    "* Pie charts (use sparingly)\n",
    "* Stacked bar charts\n",
    "* Chi-square test (for association)\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ“Š 10. **Numerical Variable Analysis**\n",
    "\n",
    "* Distribution shape\n",
    "* Mean/median comparison\n",
    "* Boxplots by category\n",
    "* Violin plots\n",
    "* ANOVA or t-tests\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ”€ 11. **Encoding Categorical Data**\n",
    "\n",
    "* Label Encoding\n",
    "* One-Hot Encoding\n",
    "* Target/Mean Encoding\n",
    "* Frequency Encoding\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ“‰ 12. **Correlation Analysis**\n",
    "\n",
    "* Pearson, Spearman, Kendall coefficients\n",
    "* Heatmaps\n",
    "* Variance Inflation Factor (VIF) for multicollinearity\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ§ª 13. **Missing Value Treatment**\n",
    "\n",
    "* Count and percentage of missing values\n",
    "* Missingness pattern visualization\n",
    "* Imputation techniques:\n",
    "\n",
    "  * Mean/median/mode\n",
    "  * Forward/backward fill\n",
    "  * KNN imputation\n",
    "  * Regression imputation\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ§® 14. **Feature Engineering (EDA-Aided)**\n",
    "\n",
    "* Polynomial features\n",
    "* Interaction terms\n",
    "* Date parts (year, month, weekday, etc.)\n",
    "* Binning (equal-width, equal-frequency, quantile-based)\n",
    "\n",
    "---\n",
    "\n",
    "#### âš–ï¸ 15. **Target Variable Analysis**\n",
    "\n",
    "* Class imbalance (binary/multiclass)\n",
    "* Distribution of target vs features\n",
    "* Use of stratification for classification\n",
    "* SMOTE or undersampling techniques (if applied)\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ“ˆ 16. **Visualization Techniques**\n",
    "\n",
    "* Seaborn, Matplotlib, Plotly\n",
    "* Histograms, KDE, Boxplots\n",
    "* Count plots, Pie charts, Bar charts\n",
    "* Heatmaps, Correlation plots\n",
    "* Joint plots, Pair plots, Violin plots\n",
    "* Time series plots\n",
    "* Missing value matrix (e.g., `msno.matrix()`)\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ“‹ 17. **EDA Reporting**\n",
    "\n",
    "* Pandas Profiling\n",
    "* Sweetviz\n",
    "* D-Tale\n",
    "* Autoviz\n",
    "* Lux\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ›‘ 18. **EDA Red Flags**\n",
    "\n",
    "* Data leakage detection\n",
    "* Target leakage in features\n",
    "* High multicollinearity\n",
    "* Dominant class in target\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… Complete EDA Guide for Machine Learning (All-in-One)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§¹ 1. Data Cleaning\n",
    "\n",
    "### ğŸ“˜ Definition:\n",
    "\n",
    "Data cleaning is the process of detecting and correcting (or removing) corrupt or inaccurate records from a dataset.\n",
    "\n",
    "### ğŸ”§ Built-in Functions:\n",
    "\n",
    "* `df.isnull()` â€“ Detects missing values\n",
    "* `df.dropna()` â€“ Removes missing values\n",
    "* `df.fillna()` â€“ Fills missing values\n",
    "* `df.duplicated()` â€“ Detects duplicate rows\n",
    "* `df.drop_duplicates()` â€“ Removes duplicate rows\n",
    "* `pd.to_numeric()` â€“ Converts data types\n",
    "* `df.replace()` â€“ Replaces specific values\n",
    "* `df.astype()` â€“ Type conversion\n",
    "\n",
    "### ğŸ§ª Example:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, np.nan, 4],\n",
    "    'B': ['  text', 'text', 'Text', 'text'],\n",
    "    'C': [1, 1, 1, 1]\n",
    "})\n",
    "\n",
    "# Remove missing values\n",
    "df_cleaned = df.dropna()\n",
    "print(df_cleaned)\n",
    "# Output:\n",
    "#      A     B  C\n",
    "# 0  1.0   text  1\n",
    "# 1  2.0   text  1\n",
    "# 3  4.0   text  1\n",
    "```\n",
    "\n",
    "### âœ… When to Use:\n",
    "\n",
    "* At the beginning of any ML project\n",
    "\n",
    "### âŒ When Not to Use:\n",
    "\n",
    "* When you need to preserve raw data for auditing\n",
    "\n",
    "### âš ï¸ Limitations:\n",
    "\n",
    "* Over-cleaning may lead to loss of important information\n",
    "\n",
    "### ğŸ¯ Interview Questions:\n",
    "\n",
    "1. How do you handle missing values in a dataset?\n",
    "2. What is the difference between `dropna()` and `fillna()`?\n",
    "3. How can you detect outliers during data cleaning?\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ 2. Data Type Identification & Conversion\n",
    "\n",
    "### ğŸ“˜ Definition:\n",
    "\n",
    "Identifying and converting data into appropriate formats such as numerical, categorical, or datetime for proper analysis.\n",
    "\n",
    "### ğŸ”§ Built-in Functions:\n",
    "\n",
    "* `df.dtypes` â€“ Shows data types\n",
    "* `df.astype()` â€“ Converts type\n",
    "* `pd.to_datetime()` â€“ Parses datetime\n",
    "* `pd.get_dummies()` â€“ One-hot encoding\n",
    "* `LabelEncoder()` â€“ Label encoding\n",
    "* `OrdinalEncoder()` â€“ Ordinal encoding\n",
    "\n",
    "### ğŸ§ª Example:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.DataFrame({'Gender': ['Male', 'Female', 'Female']})\n",
    "le = LabelEncoder()\n",
    "df['Gender_encoded'] = le.fit_transform(df['Gender'])\n",
    "print(df)\n",
    "# Output:\n",
    "#   Gender  Gender_encoded\n",
    "# 0   Male               1\n",
    "# 1 Female               0\n",
    "# 2 Female               0\n",
    "```\n",
    "\n",
    "### âœ… When to Use:\n",
    "\n",
    "* Before applying ML models\n",
    "\n",
    "### âŒ When Not to Use:\n",
    "\n",
    "* When working on raw EDA before preprocessing\n",
    "\n",
    "### âš ï¸ Limitations:\n",
    "\n",
    "* Incorrect encoding can mislead models\n",
    "\n",
    "### ğŸ¯ Interview Questions:\n",
    "\n",
    "1. Difference between One-hot and Label encoding?\n",
    "2. What are ordinal variables and how do you handle them?\n",
    "3. How do you convert a column to datetime format?\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š 3. Univariate Analysis\n",
    "\n",
    "### ğŸ“˜ Definition:\n",
    "\n",
    "Analyzing one variable at a time to understand its distribution and characteristics.\n",
    "\n",
    "### ğŸ”§ Built-in Functions:\n",
    "\n",
    "* `df.describe()` â€“ Summary statistics\n",
    "* `df.value_counts()` â€“ Frequency of values\n",
    "* `df['col'].plot(kind='hist')` â€“ Histogram\n",
    "* `sns.boxplot()` â€“ Boxplot\n",
    "* `sns.kdeplot()` â€“ Kernel Density Estimate\n",
    "\n",
    "### ğŸ§ª Example:\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.boxplot(data=df, x='A')\n",
    "plt.show()\n",
    "# Output: Boxplot visualization\n",
    "```\n",
    "\n",
    "### âœ… When to Use:\n",
    "\n",
    "* To understand distribution, outliers, central tendency\n",
    "\n",
    "### âŒ When Not to Use:\n",
    "\n",
    "* When analyzing interaction between features\n",
    "\n",
    "### âš ï¸ Limitations:\n",
    "\n",
    "* Canâ€™t show relationships with other variables\n",
    "\n",
    "### ğŸ¯ Interview Questions:\n",
    "\n",
    "1. What are summary statistics?\n",
    "2. How do you detect skewness and kurtosis?\n",
    "3. Why is univariate analysis important?\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ˆ 4. Bivariate Analysis\n",
    "\n",
    "### ğŸ“˜ Definition:\n",
    "\n",
    "Analyzing the relationship between two variables.\n",
    "\n",
    "### ğŸ”§ Built-in Functions:\n",
    "\n",
    "* `df.corr()` â€“ Correlation\n",
    "* `pd.crosstab()` â€“ Cross tabulation\n",
    "* `df.groupby()` â€“ Group-wise analysis\n",
    "* `sns.scatterplot()` â€“ Scatter plot\n",
    "* `sns.heatmap()` â€“ Heatmap\n",
    "* `sns.boxplot(x, y)` â€“ Grouped boxplot\n",
    "\n",
    "### ğŸ§ª Example:\n",
    "\n",
    "```python\n",
    "sns.scatterplot(data=df, x='A', y='C')\n",
    "plt.show()\n",
    "# Output: Scatter plot\n",
    "```\n",
    "\n",
    "### âœ… When to Use:\n",
    "\n",
    "* To identify linear/non-linear relationships\n",
    "\n",
    "### âŒ When Not to Use:\n",
    "\n",
    "* When one or both variables are not meaningful together\n",
    "\n",
    "### âš ï¸ Limitations:\n",
    "\n",
    "* Only works on two variables at a time\n",
    "\n",
    "### ğŸ¯ Interview Questions:\n",
    "\n",
    "1. What is the use of scatter plots?\n",
    "2. What is a heatmap and when is it used?\n",
    "3. How can correlation mislead in non-linear cases?\n",
    "\n",
    "---\n",
    "\n",
    "(## ğŸ” 5. Multivariate Analysis\n",
    "\n",
    "### ğŸ“˜ Definition:\n",
    "\n",
    "Analyzing relationships among more than two variables simultaneously.\n",
    "\n",
    "### ğŸ”§ Built-in Functions:\n",
    "\n",
    "* `sns.pairplot()` â€“ Pairwise plots\n",
    "* `sns.FacetGrid()` â€“ Multi-variable faceted plots\n",
    "* `PCA()` â€“ Dimensionality reduction\n",
    "* `sns.scatterplot()` with `hue` â€“ Colored multivariable scatter\n",
    "\n",
    "### ğŸ§ª Example:\n",
    "\n",
    "```python\n",
    "sns.pairplot(df, hue='Gender')\n",
    "plt.show()\n",
    "# Output: Multiple scatter plots based on each pair of variables\n",
    "```\n",
    "\n",
    "### âœ… When to Use:\n",
    "\n",
    "* When studying combined effects of features\n",
    "\n",
    "### âŒ When Not to Use:\n",
    "\n",
    "* With too many variables (can be noisy)\n",
    "\n",
    "### âš ï¸ Limitations:\n",
    "\n",
    "* Hard to visualize beyond 3 dimensions\n",
    "\n",
    "### ğŸ¯ Interview Questions:\n",
    "\n",
    "1. What is multivariate analysis?\n",
    "2. When would you use PCA in EDA?\n",
    "3. Difference between pairplot and FacetGrid?\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‰ 6. Outlier Detection\n",
    "\n",
    "### ğŸ“˜ Definition:\n",
    "\n",
    "Finding data points that are significantly different from others.\n",
    "\n",
    "### ğŸ”§ Built-in Functions:\n",
    "\n",
    "* `zscore()` â€“ Z-score method\n",
    "* `IQR` logic with `quantile()`\n",
    "* `sns.boxplot()` â€“ Boxplot for visual detection\n",
    "* `IsolationForest()` â€“ Tree-based outlier detection\n",
    "\n",
    "### ğŸ§ª Example:\n",
    "\n",
    "```python\n",
    "Q1 = df['A'].quantile(0.25)\n",
    "Q3 = df['A'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = df[(df['A'] < Q1 - 1.5*IQR) | (df['A'] > Q3 + 1.5*IQR)]\n",
    "print(outliers)\n",
    "# Output: Rows considered as outliers\n",
    "```\n",
    "\n",
    "### âœ… When to Use:\n",
    "\n",
    "* Before modeling, to improve accuracy\n",
    "\n",
    "### âŒ When Not to Use:\n",
    "\n",
    "* In naturally long-tailed distributions\n",
    "\n",
    "### âš ï¸ Limitations:\n",
    "\n",
    "* May remove valid rare events\n",
    "\n",
    "### ğŸ¯ Interview Questions:\n",
    "\n",
    "1. What is IQR and how is it used?\n",
    "2. How does Isolation Forest detect outliers?\n",
    "3. Why are outliers important in ML?\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§¬ 7. Feature Distribution Analysis\n",
    "\n",
    "### ğŸ“˜ Definition:\n",
    "\n",
    "Examining the distribution shape of individual features.\n",
    "\n",
    "### ğŸ”§ Built-in Functions:\n",
    "\n",
    "* `sns.histplot()` â€“ Histogram\n",
    "* `sns.kdeplot()` â€“ KDE plot\n",
    "* `sns.violinplot()` â€“ Violin plot\n",
    "* `np.log()`, `np.sqrt()` â€“ Skewness correction\n",
    "\n",
    "### ğŸ§ª Example:\n",
    "\n",
    "```python\n",
    "sns.violinplot(x='Gender', y='A', data=df)\n",
    "plt.show()\n",
    "# Output: Violin plot showing distribution by Gender\n",
    "```\n",
    "\n",
    "### âœ… When to Use:\n",
    "\n",
    "* When verifying data assumptions like normality\n",
    "\n",
    "### âŒ When Not to Use:\n",
    "\n",
    "* When shape is irrelevant to analysis/model\n",
    "\n",
    "### âš ï¸ Limitations:\n",
    "\n",
    "* Misleading if outliers not handled\n",
    "\n",
    "### ğŸ¯ Interview Questions:\n",
    "\n",
    "1. Whatâ€™s the use of KDE plot?\n",
    "2. How do you handle skewed features?\n",
    "3. When do you apply log transformation?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hereâ€™s the continuation in your specified format for the next EDA topics:\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© 8. Feature Engineering\n",
    "\n",
    "### ğŸ“˜ Definition:\n",
    "\n",
    "Feature engineering is the process of creating new features or modifying existing ones to improve model performance.\n",
    "\n",
    "### ğŸ”§ Built-in Functions:\n",
    "\n",
    "* `df['new'] = df['col1'] + df['col2']` â€“ Create new features\n",
    "* `df['col'].apply()` â€“ Apply custom transformations\n",
    "* `pd.cut()` â€“ Bin numerical values\n",
    "* `pd.qcut()` â€“ Quantile binning\n",
    "* `np.log()`, `np.sqrt()` â€“ Transform features\n",
    "* `PolynomialFeatures()` â€“ Generate polynomial terms\n",
    "\n",
    "### ğŸ§ª Example:\n",
    "\n",
    "```python\n",
    "df['log_A'] = np.log(df['A'] + 1)\n",
    "print(df[['A', 'log_A']])\n",
    "# Output: Original and log-transformed column\n",
    "```\n",
    "\n",
    "### âœ… When to Use:\n",
    "\n",
    "* To expose hidden patterns to ML algorithms\n",
    "\n",
    "### âŒ When Not to Use:\n",
    "\n",
    "* When raw features are already well-optimized\n",
    "\n",
    "### âš ï¸ Limitations:\n",
    "\n",
    "* Over-engineering can cause overfitting\n",
    "\n",
    "### ğŸ¯ Interview Questions:\n",
    "\n",
    "1. What is feature engineering?\n",
    "2. How does feature transformation affect ML models?\n",
    "3. Difference between `pd.cut()` and `pd.qcut()`?\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” 9. Feature Selection\n",
    "\n",
    "### ğŸ“˜ Definition:\n",
    "\n",
    "Selecting the most relevant features that contribute to the model and removing irrelevant ones.\n",
    "\n",
    "### ğŸ”§ Built-in Functions:\n",
    "\n",
    "* `SelectKBest()` â€“ Top K features\n",
    "* `RFE()` â€“ Recursive Feature Elimination\n",
    "* `VarianceThreshold()` â€“ Low variance filter\n",
    "* `df.corr()` â€“ Correlation-based filtering\n",
    "* `model.feature_importances_` â€“ From tree models\n",
    "\n",
    "### ğŸ§ª Example:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "df_selected = selector.fit_transform(df[['A', 'C']])\n",
    "print(df_selected)\n",
    "# Output: Selected features array\n",
    "```\n",
    "\n",
    "### âœ… When to Use:\n",
    "\n",
    "* To reduce overfitting and improve model performance\n",
    "\n",
    "### âŒ When Not to Use:\n",
    "\n",
    "* On small datasets where all features are essential\n",
    "\n",
    "### âš ï¸ Limitations:\n",
    "\n",
    "* Risk of removing informative features\n",
    "\n",
    "### ğŸ¯ Interview Questions:\n",
    "\n",
    "1. Why is feature selection important?\n",
    "2. Whatâ€™s the difference between filter and wrapper methods?\n",
    "3. How do tree models help in feature selection?\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§ª 10. Handling Imbalanced Data\n",
    "\n",
    "### ğŸ“˜ Definition:\n",
    "\n",
    "Techniques used to address datasets where the target class distribution is skewed.\n",
    "\n",
    "### ğŸ”§ Built-in Functions:\n",
    "\n",
    "* `value_counts()` â€“ View imbalance\n",
    "* `resample()` â€“ Over/undersampling\n",
    "* `SMOTE()` â€“ Synthetic Minority Over-sampling\n",
    "* `class_weight='balanced'` â€“ Adjust model training\n",
    "* `confusion_matrix()` â€“ Evaluate imbalance impact\n",
    "\n",
    "### ğŸ§ª Example:\n",
    "\n",
    "```python\n",
    "from sklearn.utils import resample\n",
    "\n",
    "df_majority = df[df['target'] == 0]\n",
    "df_minority = df[df['target'] == 1]\n",
    "df_minority_upsampled = resample(df_minority, replace=True, n_samples=len(df_majority), random_state=42)\n",
    "df_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
    "print(df_balanced['target'].value_counts())\n",
    "# Output: Balanced class counts\n",
    "```\n",
    "\n",
    "### âœ… When to Use:\n",
    "\n",
    "* When classification accuracy is biased toward majority class\n",
    "\n",
    "### âŒ When Not to Use:\n",
    "\n",
    "* On already balanced data\n",
    "\n",
    "### âš ï¸ Limitations:\n",
    "\n",
    "* Over/undersampling may introduce noise or remove useful data\n",
    "\n",
    "### ğŸ¯ Interview Questions:\n",
    "\n",
    "1. What is SMOTE and how does it work?\n",
    "2. What are some metrics better than accuracy in imbalanced datasets?\n",
    "3. How do you detect and fix imbalanced datasets?\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
